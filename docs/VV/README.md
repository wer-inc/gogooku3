# ğŸš€ APEX-Ranker: å®Œå…¨è¨­è¨ˆæ›¸ v1.0
## PatchTST Ã— Advanced Ranking System with Modular Architecture

---

## ğŸ“‹ ç›®æ¬¡

1. [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦](#ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦)
2. [GPUæ´»ç”¨æˆ¦ç•¥ã¨æ•°å€¤ç›®æ¨™](#gpuæ´»ç”¨æˆ¦ç•¥ã¨æ•°å€¤ç›®æ¨™)
3. [ãƒ‡ãƒ¼ã‚¿ä»•æ§˜ã¨åˆ¶ç´„](#ãƒ‡ãƒ¼ã‚¿ä»•æ§˜ã¨åˆ¶ç´„)
   - [VVM ç‰¹å¾´ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#vvm-ç‰¹å¾´ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
4. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ](#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ)
5. [æ®µéšçš„å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—](#æ®µéšçš„å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—)
6. [è©³ç´°å®Ÿè£…ä»•æ§˜](#è©³ç´°å®Ÿè£…ä»•æ§˜)
7. [æ¤œè¨¼ãƒ—ãƒ­ãƒˆã‚³ãƒ«](#æ¤œè¨¼ãƒ—ãƒ­ãƒˆã‚³ãƒ«)
8. [é‹ç”¨ãƒ»æœ€é©åŒ–](#é‹ç”¨æœ€é©åŒ–)

---

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

## ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåå€™è£œ

### 1. **APEX-Ranker** (Adaptive Predictive EXchange Ranker) âœ¨ æ¨å¥¨
- **æ„å‘³**: é©å¿œçš„å¸‚å ´äºˆæ¸¬ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  / é ‚ç‚¹ã‚’ç›®æŒ‡ã™ãƒ©ãƒ³ã‚«ãƒ¼
- **ç‰¹å¾´**: ç°¡æ½”ã§è¦šãˆã‚„ã™ãã€æŠ€è¡“çš„ã‹ã¤ãƒ“ã‚¸ãƒã‚¹çš„ã«ã‚‚é€šç”¨
- **ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå**: `apex-ranker/`, ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å: `apex_ranker`

### 2. **QuantumFlow** (Quantum-inspired Forecasting & Learning Optimizer Workflow)
- **æ„å‘³**: é‡å­è¨ˆç®—ã«ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ã•ã‚ŒãŸäºˆæ¸¬æœ€é©åŒ–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
- **ç‰¹å¾´**: å…ˆé€²çš„ãƒ»å­¦è¡“çš„ãªå°è±¡ã€è¤‡é›‘ãªç›¸äº’ä½œç”¨ã‚’å¤šæ¬¡å…ƒã§æ‰ãˆã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸

### 3. **NEXUS-Alpha** (Neural EXtension for Universal Stock Alpha)
- **æ„å‘³**: æ™®éçš„ãªè¶…éåç›Šã‚’ç›®æŒ‡ã™ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ 
- **ç‰¹å¾´**: æƒ…å ±çµ±åˆã®æ ¸ã¨ãªã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¨ã—ã¦ã®ä½ç½®ã¥ã‘

---

## ğŸ¯ åŸºæœ¬æ–¹é‡

### ã‚³ã‚¢è¨­è¨ˆæ€æƒ³

1. **æ®µéšçš„é€²åŒ–**: v0ï¼ˆæœ€å°å¯å‹•ï¼‰â†’ v5ï¼ˆæœ€çµ‚å½¢ï¼‰ã¾ã§ã€å„æ®µéšã§æ¤œè¨¼å¯èƒ½
2. **ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†é›¢**: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã§ã‚‚å‹•ä½œã€å¾Œã‹ã‚‰ç„¡åœæ­¢ã§æ©Ÿèƒ½è¿½åŠ å¯èƒ½
3. **GPUæœ€é©åŒ–**: Mixed Precision + DDPå¯¾å¿œã§å®Ÿç”¨çš„ãªè¨ˆç®—æ™‚é–“ã‚’å®Ÿç¾
4. **å†ç¾æ€§é‡è¦–**: å®Œå…¨ãªå†ç¾æ€§ã¨ablation studyã«ã‚ˆã‚‹åŠ¹æœæ¤œè¨¼

### ç¾çŠ¶ãƒ‡ãƒ¼ã‚¿åˆ¶ç´„ã¸ã®å¯¾å¿œ

#### âœ… **åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿**
- ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆOHLCVï¼‰
- ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ï¼ˆ395ç‰¹å¾´ã®ä¸€éƒ¨ï¼‰
- ãƒ•ãƒ­ãƒ¼æŒ‡æ¨™ï¼ˆå‡ºæ¥é«˜ã€æ¿ã€å£²è²·ä»£é‡‘ç­‰ï¼‰
- ã‚»ã‚¯ã‚¿ãƒ¼æ¨ªæ–­ç‰¹å¾´ï¼ˆæ¥­ç¨®ã€è¦æ¨¡ç­‰ï¼‰
- ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆYang-Zhang, VoVç­‰ã®å®Ÿç¾ãƒœãƒ©ï¼‰
- ã‚°ãƒ©ãƒ•ç‰¹å¾´ï¼ˆKè¿‘å‚ã€æ¬¡æ•°ç­‰ï¼‰

#### âŒ **ç¾æ™‚ç‚¹ã§ä¸è¶³ãƒ»ä»Šå¾Œè¿½åŠ äºˆå®š**
- ã‚ªãƒ—ã‚·ãƒ§ãƒ³IV/VIXç³»æŒ‡æ¨™
- å¤–ç”Ÿãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ï¼ˆç‚ºæ›¿ã€é‡‘åˆ©ã€ã‚³ãƒ¢ãƒ‡ã‚£ãƒ†ã‚£ï¼‰
- ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã€é–‹ç¤ºè³‡æ–™ï¼‰
- å–å¼•ã‚³ã‚¹ãƒˆ/ç´„å®šãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã€ãƒãƒ¼ã‚±ãƒƒãƒˆã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆï¼‰

#### ğŸ”§ **å¯¾å¿œæˆ¦ç•¥**
- **ä»£æ›¿ç‰¹å¾´ã§è£œå®Œ**: å®Ÿç¾ãƒœãƒ©ã€breadthæŒ‡æ¨™ã€å¸‚å ´å›å¸°æ®‹å·®ã§ä»£ç”¨
- **æ‹¡å¼µã‚¹ãƒ­ãƒƒãƒˆè¨­è¨ˆ**: å¾Œã‹ã‚‰å·®ã—è¾¼ã‚ã‚‹ IF ã‚’ç”¨æ„
- **ã‚·ãƒŠãƒªã‚ªåˆ†æ**: ã‚³ã‚¹ãƒˆã¯è¤‡æ•°ã‚·ãƒŠãƒªã‚ªï¼ˆ0/10/20/30bpsï¼‰ã§è©•ä¾¡

---

# GPUæ´»ç”¨æˆ¦ç•¥ã¨æ•°å€¤ç›®æ¨™

## ğŸ’» GPUæ´»ç”¨æˆ¦ç•¥

### ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶

#### **æœ€å°æ§‹æˆ**
- GPU: NVIDIA GPU 16GB VRAMä»¥ä¸Šï¼ˆTesla T4, RTX 4060Tiç­‰ï¼‰
- RAM: 32GBä»¥ä¸Š
- ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸: SSD 500GBä»¥ä¸Šï¼ˆãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰

#### **æ¨å¥¨æ§‹æˆ**
- GPU: NVIDIA V100 / A100 32-48GB VRAM Ã— 1-4æš
- RAM: 64GBä»¥ä¸Š
- ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸: NVMe SSD 1TBä»¥ä¸Š

#### **ã‚¯ãƒ©ã‚¦ãƒ‰æ¨å¥¨**
- AWS: p3.2xlarge (V100 16GB) / p4d.24xlarge (A100 40GBÃ—8)
- GCP: n1-standard-8 + Tesla V100
- Azure: NCv3-series

### è¨ˆç®—é‡è¨­è¨ˆ

#### **ãƒ¡ãƒ¢ãƒªè¦‹ç©**

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | ã‚µã‚¤ã‚º | å‚™è€ƒ |
|--------------|--------|------|
| **å…¥åŠ›ãƒ‡ãƒ¼ã‚¿** | 284MB | [2000éŠ˜æŸ„, 180æ—¥, 395ç‰¹å¾´] @ FP16 |
| **ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿å¾Œ** | 15MB | [2000, 20ãƒ‘ãƒƒãƒ, 192dim] @ FP16 |
| **ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿** | 50-100MB | depth=3-6ã®å ´åˆ |
| **å‹¾é…ãƒ»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹** | 150-300MB | AdamWä½¿ç”¨æ™‚ |
| **ç·ãƒ¡ãƒ¢ãƒªï¼ˆãƒ”ãƒ¼ã‚¯ï¼‰** | 8-12GB | 1ãƒãƒƒãƒï¼ˆ1æ—¥=2000éŠ˜æŸ„ï¼‰å‡¦ç†æ™‚ |

#### **å­¦ç¿’é€Ÿåº¦è¦‹ç©**

| è¨­å®š | GPU | æ™‚é–“/ã‚¨ãƒãƒƒã‚¯ | æ™‚é–“/fold |
|------|-----|--------------|----------|
| v0 (åŸºæœ¬) | V100 16GB | 18åˆ† | 12åˆ† (40epoch) |
| v2 (KNN) | V100 16GB | 25åˆ† | 18åˆ† |
| v5 (ãƒ•ãƒ«) | V100 16GB | 45åˆ† | 35åˆ† |
| v5 (4Ã—V100) | V100 16GBÃ—4 | 12åˆ† | 10åˆ† (foldä¸¦åˆ—) |
| v5 (A100) | A100 40GB | 20åˆ† | 15åˆ† |

### æœ€é©åŒ–æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

#### **Mixed Precision (AMP)**
```python
# PyTorch Lightningæ¨å¥¨è¨­å®š
trainer = pl.Trainer(
    precision="16-mixed",  # FP16 + FP32è‡ªå‹•æ··åœ¨
    devices=[0],           # GPU ID
    accelerator="gpu",
)
```

#### **åˆ†æ•£ä¸¦åˆ—ï¼ˆDDPï¼‰**
```python
# ãƒãƒ«ãƒGPUè¨­å®š
trainer = pl.Trainer(
    precision="16-mixed",
    devices=4,  # 4GPUä½¿ç”¨
    strategy="ddp_find_unused_parameters_false",  # é«˜é€ŸåŒ–
    sync_batchnorm=True,  # BatchNormåŒæœŸ
)
```

#### **Gradient Checkpointingï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰**
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç´„50%å‰Šæ¸›
- å­¦ç¿’é€Ÿåº¦: ç´„20%ä½ä¸‹
- é©ç”¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°: VRAMä¸è¶³æ™‚ã®ã¿

#### **Flash Attention 2.0ï¼ˆé€Ÿåº¦å‘ä¸Šï¼‰**
- è¨ˆç®—é€Ÿåº¦: 2-3å€é«˜é€ŸåŒ–
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç´„50%å‰Šæ¸›
- é©ç”¨: depth>4, n_heads>8ã®å ´åˆã«åŠ¹æœå¤§

---

## ğŸ¯ æ•°å€¤ç›®æ¨™

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ï¼ˆ10å¹´ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼‰

#### **v0: æœ€å°å¯å‹•ç‰ˆ**ï¼ˆå®Ÿè£…1-2é€±é–“ï¼‰
| æŒ‡æ¨™ | ç›®æ¨™å€¤ | è¨±å®¹ç¯„å›² |
|------|--------|----------|
| **RankIC (5d)** | 0.055 | 0.05-0.06 |
| **ICIR (5d)** | 1.1 | 1.0-1.2 |
| **å¹´ç‡Sharpe** | 1.35 | 1.2-1.5 |
| **æœ€å¤§DD** | -22.5% | -20% ~ -25% |
| **Precision@50** | 53.5% | 52-55% |
| **å¹´é–“åç›Šç‡** | 13.5% | 12-15% |
| **å‹ç‡ï¼ˆæ—¥æ¬¡ï¼‰** | 54% | 53-55% |

#### **v2: å®Ÿç”¨ç‰ˆ**ï¼ˆå®Ÿè£…3-4é€±é–“ï¼‰
| æŒ‡æ¨™ | ç›®æ¨™å€¤ | è¨±å®¹ç¯„å›² |
|------|--------|----------|
| **RankIC (5d)** | 0.09 | 0.08-0.10 |
| **ICIR (5d)** | 1.75 | 1.5-2.0 |
| **å¹´ç‡Sharpe** | 2.0 | 1.8-2.2 |
| **æœ€å¤§DD** | -16.5% | -15% ~ -18% |
| **Precision@50** | 60% | 58-62% |
| **å¹´é–“åç›Šç‡** | 22.5% | 20-25% |
| **å‹ç‡ï¼ˆæ—¥æ¬¡ï¼‰** | 57% | 56-58% |

#### **v5: æœ€çµ‚å½¢**ï¼ˆå®Ÿè£…3-6ãƒ¶æœˆï¼‰
| æŒ‡æ¨™ | ç›®æ¨™å€¤ | è¨±å®¹ç¯„å›² |
|------|--------|----------|
| **RankIC (5d)** | 0.11 | 0.10-0.12 |
| **ICIR (5d)** | 2.25 | 2.0-2.5 |
| **å¹´ç‡Sharpe** | 2.75 | 2.5-3.0 |
| **æœ€å¤§DD** | -11% | -10% ~ -12% |
| **Precision@50** | 67.5% | 65-70% |
| **å¹´é–“åç›Šç‡** | 35% | 30-40% |
| **å‹ç‡ï¼ˆæ—¥æ¬¡ï¼‰** | 60% | 58-62% |

### è¨ˆç®—åŠ¹ç‡ç›®æ¨™

| æŒ‡æ¨™ | ç›®æ¨™å€¤ | æ¸¬å®šæ–¹æ³• |
|------|--------|----------|
| **å­¦ç¿’æ™‚é–“/fold** | < 15åˆ† | å˜ä¸€V100ã€400æ—¥åˆ† |
| **æ¨è«–é€Ÿåº¦** | < 1ç§’ | 2000éŠ˜æŸ„ãƒãƒƒãƒæ¨è«– |
| **GPUåˆ©ç”¨ç‡** | > 85% | `nvidia-smi dmon` |
| **VRAMä½¿ç”¨ç‡** | < 80% | OOMå›é¿ |
| **ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰æ™‚é–“** | < 5% | ç·å­¦ç¿’æ™‚é–“ã®5%æœªæº€ |

### å®‰å®šæ€§ãƒ»å …ç‰¢æ€§ç›®æ¨™

| æŒ‡æ¨™ | ç›®æ¨™å€¤ | æ¤œè¨¼æ–¹æ³• |
|------|--------|----------|
| **éå­¦ç¿’åˆ¶å¾¡** | val-train ICå·® < 0.02 | å…¨foldå¹³å‡ |
| **ãƒ¬ã‚¸ãƒ¼ãƒ é©å¿œ** | å±æ©Ÿæ™‚IC > 0.03 | 2020å¹´3æœˆç­‰ |
| **åˆ†ä½æ ¡æ­£** | P10è¶…éç‡ 9-11% | v1ä»¥é™ |
| **å†ç¾æ€§** | å®Œå…¨ä¸€è‡´ | seedå›ºå®šæ™‚ |
| **ã‚¿ãƒ¼ãƒ³ã‚ªãƒ¼ãƒãƒ¼** | < 50%/æœˆ | v2ä»¥é™ |

---

# ãƒ‡ãƒ¼ã‚¿ä»•æ§˜ã¨åˆ¶ç´„

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

### å…¥åŠ›å½¢å¼

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ features.parquet       # é•·è¡¨: (Date, Code, ç‰¹å¾´ç¾¤)
â”‚   â””â”€â”€ targets.parquet         # é•·è¡¨: (Date, Code, ret_1d, ret_5d, ret_10d, ret_20d)
â”œâ”€â”€ metadata/
â”‚   â”œâ”€â”€ dataset_features_detail.json  # ç‰¹å¾´ãƒ¡ã‚¿æƒ…å ±
â”‚   â”œâ”€â”€ sector_mapping.csv           # æ¥­ç¨®ãƒãƒƒãƒ”ãƒ³ã‚°
â”‚   â””â”€â”€ trading_calendar.csv         # å–¶æ¥­æ—¥ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼
â””â”€â”€ config/
    â””â”€â”€ data_config.yaml              # ãƒ‡ãƒ¼ã‚¿è¨­å®š
```

### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆdata_config.yamlï¼‰

```yaml
data:
  # åŸºæœ¬è¨­å®š
  lookback: 180          # éå»ä½•æ—¥åˆ†ã‚’å…¥åŠ›ã¨ã™ã‚‹ã‹
  horizons: [1, 5, 10, 20]  # äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼ˆæ—¥æ•°ï¼‰
  
  # ç‰¹å¾´é¸æŠï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ¶å¾¡ï¼‰
  features:
    # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´ã‚«ãƒ†ã‚´ãƒªï¼ˆç¾çŠ¶åˆ©ç”¨å¯èƒ½ï¼‰
    include:
      - price              # OHLCV
      - technical          # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
      - flow              # å‡ºæ¥é«˜ãƒ»æ¿ãƒ»å£²è²·ä»£é‡‘
      - sector_cross      # ã‚»ã‚¯ã‚¿ãƒ¼æ¨ªæ–­ç‰¹å¾´
      - volatility        # Yang-Zhang, VoVç­‰
      - graph             # Kè¿‘å‚ã€æ¬¡æ•°ç­‰
    
    # ä»Šã¯ä½¿ã‚ãªã„ï¼ˆå°†æ¥è¿½åŠ äºˆå®šï¼‰
    exclude:
      - options_iv        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³IV/VIX
      - macro             # å¤–ç”Ÿãƒã‚¯ãƒ­ï¼ˆç‚ºæ›¿ãƒ»é‡‘åˆ©ç­‰ï¼‰
      - text              # ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
      - execution         # å–å¼•ã‚³ã‚¹ãƒˆãƒ»ç´„å®šãƒ‡ãƒ¼ã‚¿
  
  # æ­£è¦åŒ–æˆ¦ç•¥
  normalization:
    method: cross_section_z  # å½“æ—¥å†…Z-score
    train_mode: fit          # å­¦ç¿’æ™‚ã¯çµ±è¨ˆã‚’fit
    infer_mode: transform    # æ¨è«–æ™‚ã¯å½“æ—¥çµ±è¨ˆã§å¤‰æ›
    clip_sigma: 5.0          # å¤–ã‚Œå€¤ã‚¯ãƒªãƒƒãƒ—ï¼ˆÂ±5Ïƒï¼‰
  
  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
  sampling:
    strategy: day_batch      # 1ãƒãƒƒãƒ=1æ—¥å…¨éŠ˜æŸ„
    min_stocks_per_day: 500  # æœ€ä½éŠ˜æŸ„æ•°ï¼ˆã“ã‚Œæœªæº€ã®æ—¥ã¯é™¤å¤–ï¼‰
    skip_same_return_days: true  # å…¨éŠ˜æŸ„ãƒªã‚¿ãƒ¼ãƒ³ãŒåŒå€¤ã®æ—¥ã‚’ã‚¹ã‚­ãƒƒãƒ—

# æ‹¡å¼µã‚¹ãƒ­ãƒƒãƒˆï¼ˆå°†æ¥ã®å·®ã—è¾¼ã¿ç”¨ï¼‰
extensions:
  iv_slot:
    enabled: false
    dim: 0
  macro_slot:
    enabled: false
    dim: 0
  text_slot:
    enabled: false
    dim: 0
  execution_slot:
    enabled: false
    dim: 0
```

### ç‰¹å¾´ãƒ¡ã‚¿æƒ…å ±ï¼ˆdataset_features_detail.jsonï¼‰

```json
{
  "feature_groups": {
    "price": {
      "columns": ["open", "high", "low", "close", "volume", "vwap"],
      "count": 6,
      "enabled": true
    },
    "technical": {
      "columns": ["rsi_14", "macd", "bb_upper", "bb_lower", "..."],
      "count": 50,
      "enabled": true
    },
    "flow": {
      "columns": ["bid_volume", "ask_volume", "trade_imbalance", "..."],
      "count": 20,
      "enabled": true
    },
    "sector_cross": {
      "columns": ["sector_id", "market_cap_rank", "relative_strength", "..."],
      "count": 15,
      "enabled": true
    },
    "volatility": {
      "columns": ["yang_zhang", "vov", "realized_vol_20d", "..."],
      "count": 10,
      "enabled": true
    },
    "graph": {
      "columns": ["knn_degree", "correlation_centrality", "..."],
      "count": 8,
      "enabled": true
    },
    "options_iv": {
      "columns": [],
      "count": 0,
      "enabled": false,
      "note": "å°†æ¥è¿½åŠ äºˆå®š"
    },
    "macro": {
      "columns": [],
      "count": 0,
      "enabled": false,
      "note": "å°†æ¥è¿½åŠ äºˆå®š"
    },
    "text": {
      "columns": [],
      "count": 0,
      "enabled": false,
      "note": "å°†æ¥è¿½åŠ äºˆå®š"
    },
    "execution": {
      "columns": [],
      "count": 0,
      "enabled": false,
      "note": "å°†æ¥è¿½åŠ äºˆå®š"
    }
  },
  "total_features_enabled": 109,
  "total_features_planned": 150
}
```

### ã‚³ã‚¢ç‰¹å¾´é‡ãƒãƒ³ãƒ‰ãƒ«ï¼ˆcore50 / plus30ï¼‰

2025-10 æ™‚ç‚¹ã®å®Ÿé‹ç”¨ã§ã¯ã€ç´„ 190 åˆ—ã®ä¸­ã‹ã‚‰ **çŸ­æœŸã§åŠ¹ãã‚„ã™ããƒªãƒ¼ã‚¯ã‚’é¿ã‘ãŸåˆ—**ã‚’
æ˜ç¤ºçš„ã«ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã—ã€`configs/atft/feature_groups.yaml` ã§ç®¡ç†ã—ã¦ã„ã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š (`configs/atft/data/jpx_large_scale.yaml`) ã§ã¯ `core50` ã‚’å¿…é ˆã¨ã—ã€
å¿…è¦ã«å¿œã˜ã¦ `plus30` ã‚’è¿½åŠ ã™ã‚‹æ§‹æˆã«ãªã‚Šã¾ã—ãŸã€‚

- **core50:** çŸ­æœŸãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  + å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ  + ã‚»ã‚¯ã‚¿ãƒ¼ç›¸å¯¾ + ãƒ•ãƒ­ãƒ¼ + è²¡å‹™åŸºç¤  
  (`returns_1d`, `volatility_20d`, `ema_5/20/200`, `mkt_ret_1d`, `alpha_1d`,  
  `flow_foreign_net_z`, `stmt_progress_op` ãªã© 50 åˆ—å‰å¾Œ)
- **plus30:** è¿½åŠ ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»å¸‚å ´çŠ¶æ…‹ãƒ»ãƒ•ãƒ­ãƒ¼æ¯”ç‡ãƒ»è²¡å‹™è©³ç´°ã§ã®ä¸Šä¹—ã›  
  (`returns_10d`, `volatility_60d`, `rsi_14`, `mkt_ret_20d`, `flow_activity_ratio`,  
  `stmt_roe` ãªã© 30 åˆ—å‰å¾Œ)

ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ä»£è¡¨çš„ãªå†…è¨³ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ï¼ˆé‡è¤‡ã‚’é™¤å¤–ã—ãŸå®Ÿåœ¨åˆ—ã‚’æƒ³å®šï¼‰ï¼š

| åŒºåˆ†                 | ä¸»ãªåˆ—ä¾‹                                                                                      |
|----------------------|------------------------------------------------------------------------------------------------|
| ä¾¡æ ¼ãƒ»ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«      | `returns_1d`, `returns_5d`, `volatility_5d`, `ema_5`, `ma_gap_5_20`, `bb_position`             |
| å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ           | `mkt_ret_1d`, `mkt_gap_5_20`, `mkt_bull_200`, `mkt_vol_20d`, `mkt_ret_1d_z`                    |
| éŠ˜æŸ„Ã—å¸‚å ´ã‚¯ãƒ­ã‚¹       | `beta_60d`, `alpha_5d`, `rel_strength_5d`, `idio_vol_ratio`, `trend_align_mkt`                 |
| ã‚»ã‚¯ã‚¿ãƒ¼ç›¸å¯¾          | `sec_mom_20`, `sec_ret_5d_eq`, `sec_vol_20_z`, `sec_gap_5_20`, `sec_member_cnt`                |
| ãƒ•ãƒ­ãƒ¼ï¼éœ€çµ¦          | `flow_foreign_net_z`, `flow_smart_idx`, `flow_impulse`, `flow_activity_ratio`, `flow_breadth_pos` |
| è²¡å‹™ã‚¤ãƒ™ãƒ³ãƒˆ          | `stmt_yoy_sales`, `stmt_opm`, `stmt_progress_op`, `stmt_rev_fore_np`, `stmt_imp_statement`      |
| ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ è£œåŠ© (plus) | `returns_10d`, `returns_20d`, `volatility_60d`, `rsi_2`, `macd_histogram`, `stoch_k`           |

ãƒã‚¹ã‚¯åˆ—ã¯ `is_flow_valid`, `is_stmt_valid`, `is_valid_ma`, `is_sec_cs_valid` ã‚’ä½µç”¨ã™ã‚‹æƒ³å®šã§ã€
å­¦ç¿’æ™‚ã¯ **Fold å†…ã§ã®ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ Z-score** ã‚’å‰å‡¦ç†ã¨ã—ã¦é©ç”¨ã—ã¾ã™ã€‚
ã“ã®æ•´ç†ã«ã‚ˆã‚Šã€APEX-Ranker v0 / ATFT-GAT-FAN ã„ãšã‚Œã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã‚‚åŒã˜æŸã‚’å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

### VVM ç‰¹å¾´ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**çµè«–:** ã¯ã„ã€‚ç¾åœ¨ã® dataset ã¯ Volatility / Volume / Momentumï¼ˆVVMï¼‰ã‚’ä¸­å¿ƒã«æ®ãˆãŸ 4 éšå±¤æ§‹æˆã§ã€çŸ­æœŸï¼ˆ1â€“5dï¼‰ï½ä¸­æœŸï¼ˆ10â€“20dï¼‰äºˆæ¸¬ã«ãã®ã¾ã¾æŠ•å…¥ã§ãã¾ã™ã€‚

#### ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹é€ ã¨ãƒã‚¹ã‚¯é‹ç”¨
- 4 éšå±¤ï¼ˆéŠ˜æŸ„ãƒ»å¸‚å ´ TOPIXãƒ»ã‚»ã‚¯ã‚¿ãƒ¼é›†ç´„ãƒ»éŠ˜æŸ„Ã—å¸‚å ´ã‚¯ãƒ­ã‚¹ï¼‰ã®åŒç³»çµ±æŒ‡æ¨™ã‚’ç”¨æ„
- `is_*_valid` ç³»ãƒ•ãƒ©ã‚°ã§ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æœªæº€ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•ãƒã‚¹ã‚¯ã—ã€ãƒªãƒ¼ã‚¯å¯¾ç­–ã‚’å¾¹åº•
- ã™ã¹ã¦å½“æ—¥æ™‚ç‚¹ã¾ã§ã®å±¥æ­´ã®ã¿ã§ç®—å‡ºã—ã€15:00/T+1 ãƒ«ãƒ¼ãƒ«ã‚’éµå®ˆ

#### Volatilityï¼ˆä¾¡æ ¼å¤‰å‹•ï¼‰
- **éŠ˜æŸ„ãƒ¬ãƒ™ãƒ«:** `volatility_5d`, `volatility_10d`, `volatility_20d`, `volatility_60d`, `realized_volatility`, `bb_width`
- **å¸‚å ´ãƒ¬ãƒ™ãƒ«ï¼ˆTOPIXï¼‰:** `mkt_vol_20d`, `mkt_vol_20d_z`, `mkt_bb_bw`, `mkt_bb_bw_z`, `mkt_high_vol`
- **ã‚»ã‚¯ã‚¿ãƒ¼é›†ç´„:** `sec_vol_20`, `sec_vol_20_z`
- **éŠ˜æŸ„Ã—å¸‚å ´ã‚¯ãƒ­ã‚¹:** `idio_vol_ratio`ï¼ˆ= `volatility_20d / (mkt_vol_20d + Îµ)`ï¼‰

#### Volumeï¼ˆå‡ºæ¥é«˜ãƒ»ãƒ•ãƒ­ãƒ¼ï¼‰
- **éŠ˜æŸ„ãƒ¬ãƒ™ãƒ«:** `volume_ratio_5`, `volume_ratio_20`, `volume_ma_5`, `volume_ma_20`, `turnover_rate`, `dollar_volume`
- **å¸‚å ´/ãƒ•ãƒ­ãƒ¼è£œåŠ©:** `flow_activity_z`, `flow_activity_ratio`
- **Premium æ©Ÿèƒ½:** `am_vol_ratio_20`, `bd_activity_ratio`, `bd_net_z_52`

#### Momentumï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ / åè»¢ï¼‰
- **éŠ˜æŸ„ãƒ¬ãƒ™ãƒ«ï¼ˆä¾¡æ ¼ï¼‰:** `returns_1d/5d/10d/20d`, `log_returns_1d`, `ema_5/20/200`, `ma_gap_5_20`, `ema5_slope`, `dist_to_200ema`, `rsi_2`, `rsi_14`, `bb_position`, `close_to_high`, `close_to_low`, `macd_histogram`, `stoch_k`
- **å¸‚å ´ãƒ¬ãƒ™ãƒ«:** `mkt_ret_1d/5d/10d`, `mkt_gap_5_20`, `mkt_bull_200`, `mkt_trend_up`
- **ã‚»ã‚¯ã‚¿ãƒ¼ç›¸å¯¾:** `sec_mom_20`, `sec_ret_5d_eq`, `rel_to_sec_5d`
- **éŠ˜æŸ„Ã—å¸‚å ´ã‚¯ãƒ­ã‚¹:** `rel_strength_5d`, `alpha_1d`, `alpha_5d`, `trend_align_mkt`

#### VVM å¼·åº¦ã®è©•ä¾¡
- **Volatility:** éŠ˜æŸ„ãƒ»å¸‚å ´ãƒ»ã‚»ã‚¯ã‚¿ãƒ¼ã® 3 éšå±¤ã‚’ç¶²ç¾…
- **Volume:** ç”Ÿå‡ºæ¥é«˜ãƒ»ç§»å‹•å¹³å‡æ¯”ãƒ»å›è»¢ç‡ã§æ°´æº–ã¨å¤‰åŒ–ã‚’ä¸¡ç«‹
- **Momentum:** 1â€“20 æ—¥ã®ãƒªã‚¿ãƒ¼ãƒ³ï¼EMA ã‚®ãƒ£ãƒƒãƒ—ï¼ä½ç½®ç³»ï¼ã‚ªã‚·ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’æƒãˆã€ç›¸å¯¾è»¸ã‚‚ç¢ºä¿
- **é‹ç”¨:** `is_*_valid` ãƒã‚¹ã‚¯ã¨ T+1 éµå®ˆã§å®Ÿå‹™çš„ãªãƒªãƒ¼ã‚¯é˜²æ­¢

#### VVM ã‚’ã•ã‚‰ã«åšãã™ã‚‹æœ€å°å·®åˆ†
1. **Volatility ã®è³ªæ„Ÿå‘ä¸Š**
   - `downside_vol_20d = std(min(returns_1d, 0), 20) * sqrt(252)`ï¼ˆä¸‹æ–¹ãƒœãƒ©ã®é¡•åœ¨åŒ–ï¼‰
   - `range_pct = (High - Low) / (Close + Îµ)`ï¼ˆä¾¡æ ¼æ°´æº–ã«ä¾å­˜ã—ãªã„ãƒ¬ãƒ³ã‚¸æ­£è¦åŒ–ï¼‰
2. **Volume ã®ä¹¾æ¹¿æŒ‡æ¨™**
   - `volume_ratio_60` ã¨ `liquidity_dry_flag = (volume_ratio_20 < 0.5).int8()` ã§æµå‹•æ€§ä½ä¸‹ã‚’æ¤œçŸ¥
   - `tvr_ratio_20 = (Close * Volume) / mean(Close * Volume, 20)` ã§ä¾¡æ ¼Ã—å‡ºæ¥é«˜ã®äº¤äº’ä½œç”¨ã‚’è¿½è·¡
3. **Momentum ã®äºŒç›¸åˆ†é›¢**
   - åè»¢ç³»ã®è£œå¼·ã« `ret_1d_z_in_cs`ï¼ˆå½“æ—¥ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ Zã€å­¦ç¿’ fold å†…ã§ç®—å‡ºï¼‰
   - è¿½éšç³»ã®å¯è¦–åŒ–ã« `mom_20 = sum(returns_1d, 20)`

ä¸Šè¨˜ã¯ã„ãšã‚Œã‚‚æ´¾ç”Ÿæ•°è¡Œã§å®Ÿè£…ã§ãã€æ—¢å­˜ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«è¿½åŠ ã—ã¦ã‚‚ãƒªãƒ¼ã‚¯ã¯ç”Ÿã˜ã¾ã›ã‚“ã€‚

#### VVM å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆPolars ãƒŸãƒ‹ãƒ¬ã‚·ãƒ”ï¼‰
```python
import polars as pl

df = pl.read_parquet("output/ml_dataset_latest_enriched.parquet")

# 1. åˆ—å­˜åœ¨ãƒã‚§ãƒƒã‚¯
must_have = [
    "volatility_5d", "volatility_20d", "realized_volatility", "mkt_vol_20d", "sec_vol_20",
    "volume_ratio_5", "volume_ratio_20", "turnover_rate",
    "returns_1d", "returns_5d", "ema_5", "ema_20", "ma_gap_5_20", "bb_position", "rsi_2",
    "rel_strength_5d", "sec_mom_20", "rel_to_sec_5d", "idio_vol_ratio",
]
missing = [c for c in must_have if c not in df.columns]
print("missing:", missing)

# 2. æœ‰åŠ¹ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆå¿…è¦ã«å¿œã˜ã¦ãƒã‚¹ã‚¯åˆ—ã‚’é©ç”¨ï¼‰
def coverage(col: str, valid: str | None = None) -> float:
    series = df[col]
    if valid and valid in df.columns:
        mask = df[valid].fill_null(0) == 1
        series = series.filter(mask)
    return 1.0 - series.null_count() / series.len()

print("vol20 coverage:", coverage("volatility_20d"))
print("volume_ratio_20 coverage:", coverage("volume_ratio_20"))
print("ma_gap_5_20 coverage (valid_ma):", coverage("ma_gap_5_20", "is_valid_ma"))
```

#### ã¾ã¨ã‚
- Dataset ã¯ VVM ã‚’å¤šå±¤ãƒ»å¤šè¡¨ç¾ã§ã‚«ãƒãƒ¼æ¸ˆã¿ã§ã€çŸ­æœŸï½ä¸­æœŸãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã«å³å¿œå¯èƒ½
- ä¸‹æ–¹ãƒœãƒ© / æµå‹•æ€§ãƒ•ãƒ©ã‚° / å˜ç´”ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã‚’è£œã†ã ã‘ã§æ›´ãªã‚‹æ„Ÿåº¦å‘ä¸ŠãŒè¦‹è¾¼ã‚ã‚‹
- å­¦ç¿’å‰å‡¦ç†ã§ã¯å½“æ—¥ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ Z ã¨ `is_*_valid` ãƒã‚¹ã‚¯ã‚’æ¨™æº–åŒ–ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨

### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

```
Raw Data (Parquet)
    â†“
FeatureSelector (include/excludeã§ãƒ•ã‚£ãƒ«ã‚¿)
    â†“
PanelIndexer (CodeÃ—Date ã®é«˜é€Ÿã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)
    â†“
DayPanelDataset (æ—¥å˜ä½ã§ãƒãƒƒãƒåŒ–)
    â†“
Normalization (å½“æ—¥å†…Z-score)
    â†“
Model Input [B, L, F]
```

---

# ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ

## ğŸ—ï¸ å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APEX-Ranker System                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: [Batch, Lookback=180, Features=109]                 â”‚
â”‚         (ç¾çŠ¶åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´ã®ã¿)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   v0: Base Encoder                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ PatchTST Encoder                                      â”‚   â”‚
â”‚  â”‚ - Patch Embedding (Conv1D)                            â”‚   â”‚
â”‚  â”‚ - Multi-Head Self-Attention Ã— depth                   â”‚   â”‚
â”‚  â”‚ - Layer Norm + FFN + Residual                         â”‚   â”‚
â”‚  â”‚ Output: [B, d_model=192]                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          v1: Risk-Aware Enhancement (Optional)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Quantile Head (Ï„=0.1, 0.5, 0.9)                      â”‚   â”‚
â”‚  â”‚ - P10, Median, P90äºˆæ¸¬                                â”‚   â”‚
â”‚  â”‚ - Adjusted Score = score - Î»Ã—P10                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      v2: Cross-Sectional Context (Optional)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Adaptive KNN Graph Attention                          â”‚   â”‚
â”‚  â”‚ - å¸‚å ´çŠ¶æ…‹ã«å¿œã˜ã¦Kå‹•çš„èª¿æ•´ (k_min=5, k_max=30)         â”‚   â”‚
â”‚  â”‚ - GAT 1å±¤ã§éŠ˜æŸ„é–“ç›¸äº’ä½œç”¨                              â”‚   â”‚
â”‚  â”‚ Output: [B, d_model] (context-enhanced)               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Multi-Horizon Prediction Head                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Linear Heads Ã— 4 Horizons                             â”‚   â”‚
â”‚  â”‚ - 1æ—¥å…ˆ: score_1d  [B]                                â”‚   â”‚
â”‚  â”‚ - 5æ—¥å…ˆ: score_5d  [B]                                â”‚   â”‚
â”‚  â”‚ - 10æ—¥å…ˆ: score_10d [B]                               â”‚   â”‚
â”‚  â”‚ - 20æ—¥å…ˆ: score_20d [B]                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ranking Loss                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Composite Loss (åŒæ—¥ãƒãƒƒãƒå†…ã§è¨ˆç®—)                     â”‚   â”‚
â”‚  â”‚ - ListNet (70%): Top-Ké‡ã¿ä»˜ãKL divergence           â”‚   â”‚
â”‚  â”‚ - RankNet (30%): Pairwise logistic loss               â”‚   â”‚
â”‚  â”‚ - (Optional) MSE (åˆæœŸã®ã¿): å®‰å®šåŒ–ç”¨                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Evaluation Metrics                        â”‚
â”‚  - RankIC / ICIR (Spearmanç›¸é–¢)                              â”‚
â”‚  - Precision@K / Hit@K                                       â”‚
â”‚  - Top-K Portfolio (Sharpe, MDD, Turnover)                   â”‚
â”‚  - Cost Sensitivity Analysis (0/10/20/30bps)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”Œ æ‹¡å¼µã‚¹ãƒ­ãƒƒãƒˆè¨­è¨ˆï¼ˆå°†æ¥ã®å·®ã—è¾¼ã¿ç”¨ï¼‰

```python
class ExtensibleEncoder(nn.Module):
    """æ‹¡å¼µå¯èƒ½ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆå°†æ¥ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¿½åŠ ã«å¯¾å¿œï¼‰"""
    def __init__(self, config):
        super().__init__()
        
        # v0: åŸºæœ¬ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆå¸¸ã«æœ‰åŠ¹ï¼‰
        self.base_encoder = PatchTSTEncoder(
            in_feats=config.n_features_enabled,
            d_model=config.d_model,
            depth=config.depth
        )
        
        # v1: åˆ†ä½ç‚¹ãƒ˜ãƒƒãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        self.quantile_enabled = config.get('quantile_enabled', False)
        if self.quantile_enabled:
            self.quantile_head = QuantileHead(config.d_model, config.horizons)
        
        # v2: ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        self.crosssec_enabled = config.get('crosssec_enabled', False)
        if self.crosssec_enabled:
            self.crosssec_layer = AdaptiveKNNGraph(config.d_model)
        
        # æ‹¡å¼µã‚¹ãƒ­ãƒƒãƒˆï¼ˆå°†æ¥ç”¨ï¼‰
        self.iv_slot = ExtensionSlot(enabled=False)      # IVè¿½åŠ æ™‚ã«æœ‰åŠ¹åŒ–
        self.macro_slot = ExtensionSlot(enabled=False)   # ãƒã‚¯ãƒ­è¿½åŠ æ™‚ã«æœ‰åŠ¹åŒ–
        self.text_slot = ExtensionSlot(enabled=False)    # ãƒ†ã‚­ã‚¹ãƒˆè¿½åŠ æ™‚ã«æœ‰åŠ¹åŒ–
        
    def forward(self, X, extensions=None):
        # v0: åŸºæœ¬ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        z_base, tokens = self.base_encoder(X)  # [B, d_model]
        
        # v2: ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæœ‰åŠ¹æ™‚ã®ã¿ï¼‰
        if self.crosssec_enabled:
            z = self.crosssec_layer(z_base)[0]
        else:
            z = z_base
        
        # æ‹¡å¼µã‚¹ãƒ­ãƒƒãƒˆï¼ˆå°†æ¥ã®è¿½åŠ æ©Ÿèƒ½ï¼‰
        if extensions is not None:
            if self.iv_slot.enabled and 'iv' in extensions:
                z = self.iv_slot.fuse(z, extensions['iv'])
            if self.macro_slot.enabled and 'macro' in extensions:
                z = self.macro_slot.fuse(z, extensions['macro'])
            if self.text_slot.enabled and 'text' in extensions:
                z = self.text_slot.fuse(z, extensions['text'])
        
        return z, tokens

class ExtensionSlot(nn.Module):
    """å°†æ¥ã®æ©Ÿèƒ½è¿½åŠ ç”¨ã®æ‹¡å¼µã‚¹ãƒ­ãƒƒãƒˆ"""
    def __init__(self, enabled=False, fusion_type='concat'):
        super().__init__()
        self.enabled = enabled
        self.fusion_type = fusion_type
        self.fusion_layer = None
        
    def enable(self, input_dim, output_dim):
        """ã‚¹ãƒ­ãƒƒãƒˆã‚’æœ‰åŠ¹åŒ–ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¿½åŠ æ™‚ã«å‘¼ã¶ï¼‰"""
        self.enabled = True
        if self.fusion_type == 'concat':
            self.fusion_layer = nn.Linear(input_dim + output_dim, output_dim)
        elif self.fusion_type == 'gate':
            self.fusion_layer = GatedFusion(input_dim, output_dim)
    
    def fuse(self, z, extension_features):
        """ç‰¹å¾´èåˆ"""
        if not self.enabled or self.fusion_layer is None:
            return z
        return self.fusion_layer(torch.cat([z, extension_features], dim=-1))
```

---

# æ®µéšçš„å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

## ğŸ“… é–‹ç™ºãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³

### **Phase 0: ç’°å¢ƒæ§‹ç¯‰**ï¼ˆ3-5æ—¥ï¼‰

#### ã‚¿ã‚¹ã‚¯
- [ ] GPUç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆDocker/Condaï¼‰
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
- [ ] åŸºæœ¬ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£å®Ÿè£…

#### æˆæœç‰©
```
apex-ranker/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ environment.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ README.md
â””â”€â”€ scripts/
    â””â”€â”€ setup_env.sh
```

#### æ¤œè¨¼åŸºæº–
- [ ] GPUèªè­˜ç¢ºèªï¼ˆ`torch.cuda.is_available()`ï¼‰
- [ ] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸï¼ˆã‚µãƒ³ãƒ—ãƒ«100æ—¥åˆ†ï¼‰
- [ ] åŸºæœ¬ãƒ†ã‚¹ãƒˆé€šéï¼ˆpytestï¼‰

---

### **Phase 1: v0å®Ÿè£…ï¼ˆæœ€å°å¯å‹•ç‰ˆï¼‰**ï¼ˆ1-2é€±é–“ï¼‰

#### ã‚¿ã‚¹ã‚¯
- [ ] PatchTSTã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å®Ÿè£…
- [ ] ãƒãƒ«ãƒãƒ›ãƒ©ã‚¤ã‚ºãƒ³ãƒ˜ãƒƒãƒ‰å®Ÿè£…
- [ ] ListNet + RankNetæå¤±å®Ÿè£…
- [ ] DayBatchSamplerå®Ÿè£…
- [ ] å­¦ç¿’ãƒ«ãƒ¼ãƒ—æ§‹ç¯‰ï¼ˆLightningï¼‰
- [ ] è©•ä¾¡æŒ‡æ¨™å®Ÿè£…ï¼ˆRankIC, ICIR, P@Kï¼‰

#### ã‚³ã‚¢å®Ÿè£…

##### 1. PatchTSTã‚¨ãƒ³ã‚³ãƒ¼ãƒ€

```python
class PatchTSTEncoder(nn.Module):
    """
    æ™‚ç³»åˆ—ã‚’ãƒ‘ãƒƒãƒåˆ†å‰²ã—ã¦Transformerã§å‡¦ç†
    
    Args:
        in_feats: å…¥åŠ›ç‰¹å¾´æ•°ï¼ˆç¾çŠ¶109, å°†æ¥çš„ã«æ‹¡å¼µå¯èƒ½ï¼‰
        d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
        depth: Transformerãƒ–ãƒ­ãƒƒã‚¯æ•°
        patch_len: ãƒ‘ãƒƒãƒé•·
        stride: ãƒ‘ãƒƒãƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰
        n_heads: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°
        dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        channel_independent: å„ç‰¹å¾´ã‚’ç‹¬ç«‹å‡¦ç†ã™ã‚‹ã‹
    """
    def __init__(self, 
                 in_feats=109,  # ç¾çŠ¶åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´æ•°
                 d_model=192, 
                 depth=3, 
                 patch_len=16, 
                 stride=8,
                 n_heads=8,
                 dropout=0.1,
                 channel_independent=True):
        super().__init__()
        
        # ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
        if channel_independent:
            # å„ç‰¹å¾´ç‹¬ç«‹ã§Conv â†’ çµ±åˆ
            d_patch = 4
            self.patch_embed = nn.Sequential(
                nn.Conv1d(in_feats, in_feats * d_patch,
                         kernel_size=patch_len, stride=stride,
                         groups=in_feats, bias=False),
                Rearrange('b (c p) n -> b n (c p)', c=in_feats),
                nn.Linear(in_feats * d_patch, d_model)
            )
        else:
            # ãƒãƒ£ãƒãƒ«æ··åˆ
            self.patch_embed = nn.Sequential(
                nn.Conv1d(in_feats, d_model,
                         kernel_size=patch_len, stride=stride,
                         bias=False),
                Rearrange('b c n -> b n c')
            )
        
        # Transformerãƒ–ãƒ­ãƒƒã‚¯
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_model*4, dropout)
            for _ in range(depth)
        ])
        
        self.norm = nn.LayerNorm(d_model)
        
    def forward(self, x):
        # x: [B, L, F] â†’ [B, F, L]
        x = x.transpose(1, 2)
        
        # ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿: [B, F, L] â†’ [B, Np, d_model]
        tokens = self.patch_embed(x)
        
        # Transformerå‡¦ç†
        for block in self.blocks:
            tokens = block(tokens)
        
        tokens = self.norm(tokens)
        
        # ãƒ—ãƒ¼ãƒªãƒ³ã‚°: [B, Np, d_model] â†’ [B, d_model]
        z = tokens.mean(dim=1)
        
        return z, tokens


class TransformerBlock(nn.Module):
    """æ¨™æº–çš„ãªTransformerãƒ–ãƒ­ãƒƒã‚¯"""
    def __init__(self, d_model, n_heads, d_ff, dropout):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.attn = nn.MultiheadAttention(
            d_model, n_heads, dropout=dropout, batch_first=True
        )
        self.ln2 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        # Self-Attention
        h = self.ln1(x)
        h, _ = self.attn(h, h, h, need_weights=False)
        x = x + h
        
        # Feed-Forward
        h = self.ln2(x)
        h = self.ffn(h)
        x = x + h
        
        return x
```

##### 2. ãƒ©ãƒ³ã‚­ãƒ³ã‚°æå¤±

```python
class ListNetLoss(nn.Module):
    """
    ListNetæå¤±ï¼ˆæ¸©åº¦ä»˜ãã€Top-Ké‡ã¿å¯¾å¿œï¼‰
    
    Args:
        tau: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå°ã•ã„ã»ã©ä¸Šä½é‡è¦–ï¼‰
        topk: ä¸Šä½ä½•ä»¶ã‚’é‡è¦–ã™ã‚‹ã‹ï¼ˆNoneã§å…¨ä½“ï¼‰
        eps: æ•°å€¤å®‰å®šåŒ–ç”¨ã®å¾®å°å€¤
    """
    def __init__(self, tau=0.5, topk=None, eps=1e-12):
        super().__init__()
        self.tau = tau
        self.topk = topk
        self.eps = eps
    
    def forward(self, scores, labels):
        """
        Args:
            scores: [B] ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚¹ã‚³ã‚¢ï¼ˆåŒæ—¥ãƒãƒƒãƒï¼‰
            labels: [B] çœŸã®ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆåŒæ—¥ãƒãƒƒãƒï¼‰
        Returns:
            loss: ã‚¹ã‚«ãƒ©ãƒ¼
        """
        # åŒå€¤æ—¥ã®ãƒã‚§ãƒƒã‚¯
        if torch.isclose(labels.std(), torch.tensor(0., device=labels.device)):
            return torch.tensor(0., device=scores.device)
        
        # äºˆæ¸¬åˆ†å¸ƒ
        p = torch.softmax(scores / self.tau, dim=0)
        
        # æ•™å¸«åˆ†å¸ƒ
        q = torch.softmax(labels / self.tau, dim=0)
        
        # Top-Ké‡ã¿ä»˜ã‘
        if self.topk is not None and self.topk < len(labels):
            _, topk_idx = torch.topk(q, self.topk, largest=True, sorted=False)
            w = torch.zeros_like(q)
            w[topk_idx] = 1.0
            w = w / (w.sum() + self.eps)
            loss = -(w * torch.log(p + self.eps)).sum()
        else:
            loss = -(q * torch.log(p + self.eps)).sum()
        
        return loss


class RankNetLoss(nn.Module):
    """
    RankNetæå¤±ï¼ˆãƒšã‚¢ãƒ¯ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰
    
    Args:
        neg_sample: ãƒšã‚¢æ•°å‰Šæ¸›ç”¨ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°ï¼ˆNoneã§å…¨ãƒšã‚¢ï¼‰
    """
    def __init__(self, neg_sample=None):
        super().__init__()
        self.neg_sample = neg_sample
    
    def forward(self, scores, labels):
        """
        Args:
            scores: [B] ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚¹ã‚³ã‚¢
            labels: [B] çœŸã®ãƒªã‚¿ãƒ¼ãƒ³
        """
        B = labels.size(0)
        
        # å…¨ãƒšã‚¢ç”Ÿæˆ or ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if self.neg_sample is not None and B*(B-1)//2 > self.neg_sample:
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            n_pairs = self.neg_sample
            idx_i = torch.randint(0, B, (n_pairs,), device=scores.device)
            idx_j = torch.randint(0, B, (n_pairs,), device=scores.device)
            mask = idx_i != idx_j
            idx_i, idx_j = idx_i[mask], idx_j[mask]
        else:
            # å…¨ãƒšã‚¢
            idx_i, idx_j = torch.triu_indices(B, B, offset=1, device=scores.device)
        
        # ã‚¹ã‚³ã‚¢å·®ã¨æ•™å¸«å·®
        s_diff = scores[idx_i] - scores[idx_j]
        y_diff = labels[idx_i] - labels[idx_j]
        
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºãƒ­ã‚¹
        # y_diff > 0 ãªã‚‰ s_diff ã‚‚æ­£ã‚’æœ›ã‚€
        loss = torch.nn.functional.binary_cross_entropy_with_logits(
            s_diff, (y_diff > 0).float(), reduction='mean'
        )
        
        return loss


class CompositeLoss(nn.Module):
    """è¤‡åˆæå¤±ï¼ˆListNet + RankNet + ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§MSEï¼‰"""
    def __init__(self, config):
        super().__init__()
        self.listnet = ListNetLoss(
            tau=config.loss.listnet.tau,
            topk=config.loss.listnet.topk
        )
        self.ranknet = RankNetLoss(
            neg_sample=config.loss.ranknet.neg_sample
        )
        self.mse = nn.MSELoss()
        
        self.w_listnet = config.loss.listnet.weight
        self.w_ranknet = config.loss.ranknet.weight
        self.w_mse = config.loss.mse.weight
    
    def forward(self, scores, labels):
        loss = 0.0
        
        if self.w_listnet > 0:
            loss += self.w_listnet * self.listnet(scores, labels)
        
        if self.w_ranknet > 0:
            loss += self.w_ranknet * self.ranknet(scores, labels)
        
        if self.w_mse > 0:
            loss += self.w_mse * self.mse(scores, labels)
        
        return loss
```

##### 3. DayBatchSampler & Dataset

```python
class DayPanelDataset(torch.utils.data.Dataset):
    """
    æ—¥å˜ä½ã§ãƒãƒƒãƒã‚’è¿”ã™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    
    1ã¤ã®__getitem__å‘¼ã³å‡ºã—ã§ã€ãã®æ—¥ã®å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
    ï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°æå¤±ã¯åŒæ—¥å†…ã§è¨ˆç®—ã•ã‚Œã‚‹ãŸã‚ï¼‰
    """
    def __init__(self, df, feature_cols, target_cols, 
                 lookback=180, min_stocks=500):
        """
        Args:
            df: ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆ(Date, Code)ã§ã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
            feature_cols: ç‰¹å¾´åˆ—åãƒªã‚¹ãƒˆ
            target_cols: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—åãƒªã‚¹ãƒˆ
            lookback: éå»ä½•æ—¥åˆ†ã‚’ä½¿ã†ã‹
            min_stocks: æœ€ä½éŠ˜æŸ„æ•°ï¼ˆæœªæº€ã®æ—¥ã¯é™¤å¤–ï¼‰
        """
        self.df = df
        self.feature_cols = feature_cols
        self.target_cols = target_cols
        self.lookback = lookback
        
        # æ—¥ä»˜ãƒªã‚¹ãƒˆä½œæˆï¼ˆååˆ†ãªå±¥æ­´ãŒã‚ã‚‹æ—¥ã®ã¿ï¼‰
        all_dates = sorted(df['Date'].unique())
        self.dates = [
            d for i, d in enumerate(all_dates)
            if i >= lookback  # ååˆ†ãªå±¥æ­´ãŒã‚ã‚‹
        ]
        
        # å„æ—¥ã®éŠ˜æŸ„ãƒªã‚¹ãƒˆ
        self.stocks_by_date = {}
        for date in self.dates:
            stocks = df[df['Date'] == date]['Code'].unique()
            if len(stocks) >= min_stocks:
                self.stocks_by_date[date] = stocks
        
        # æœ€ä½éŠ˜æŸ„æ•°ã‚’æº€ãŸã™æ—¥ã®ã¿æ®‹ã™
        self.dates = [d for d in self.dates if d in self.stocks_by_date]
        
        # ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µæ§‹ç¯‰ï¼ˆé«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹ç”¨ï¼‰
        self.indexer = self._build_indexer()
    
    def _build_indexer(self):
        """CodeÃ—Date ã®ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µã‚’æ§‹ç¯‰"""
        indexer = {}
        for code in self.df['Code'].unique():
            code_df = self.df[self.df['Code'] == code].reset_index(drop=True)
            date_to_idx = {
                date: idx for idx, date in enumerate(code_df['Date'])
            }
            indexer[code] = {
                'df': code_df,
                'date_to_idx': date_to_idx
            }
        return indexer
    
    def __len__(self):
        return len(self.dates)
    
    def __getitem__(self, idx):
        date = self.dates[idx]
        stocks = self.stocks_by_date[date]
        
        X_list = []
        y_list = []
        valid_stocks = []
        
        for code in stocks:
            # éå»lookbackæ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
            data = self._get_window(code, date)
            if data is not None:
                X, y = data
                X_list.append(X)
                y_list.append(y)
                valid_stocks.append(code)
        
        if len(X_list) == 0:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé€šå¸¸èµ·ããªã„ï¼‰
            return None
        
        X = np.stack(X_list, axis=0)  # [B, L, F]
        y = np.stack(y_list, axis=0)  # [B, H]
        
        return {
            'X': X.astype(np.float32),
            'y': y.astype(np.float32),
            'codes': valid_stocks,
            'date': date
        }
    
    def _get_window(self, code, end_date):
        """æŒ‡å®šéŠ˜æŸ„ãƒ»æ—¥ä»˜ã®éå»lookbackæ—¥åˆ†ã‚’å–å¾—"""
        if code not in self.indexer:
            return None
        
        info = self.indexer[code]
        df = info['df']
        date_to_idx = info['date_to_idx']
        
        if end_date not in date_to_idx:
            return None
        
        end_idx = date_to_idx[end_date]
        start_idx = end_idx - self.lookback + 1
        
        if start_idx < 0:
            return None
        
        window_df = df.iloc[start_idx:end_idx+1]
        
        if len(window_df) != self.lookback:
            return None  # æ¬ æãŒã‚ã‚‹
        
        X = window_df[self.feature_cols].values  # [L, F]
        y = window_df.iloc[-1][self.target_cols].values  # [H]
        
        return X, y


def collate_day_batch(batch):
    """
    ãƒãƒƒãƒå†…ã¯1æ—¥åˆ†ï¼ˆã‚µãƒ³ãƒ—ãƒ©ãƒ¼ãŒdayå˜ä½ã§è¿”ã™æƒ³å®šï¼‰
    """
    if batch[0] is None:
        return None
    
    sample = batch[0]
    
    return {
        'X': torch.tensor(sample['X']),      # [B, L, F]
        'y': torch.tensor(sample['y']),      # [B, H]
        'codes': sample['codes'],
        'date': sample['date']
    }
```

##### 4. Lightningãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

```python
import pytorch_lightning as pl

class APEXRankerV0(pl.LightningModule):
    """
    v0: æœ€å°å¯å‹•ç‰ˆ
    - PatchTSTã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
    - ãƒãƒ«ãƒãƒ›ãƒ©ã‚¤ã‚ºãƒ³ãƒ˜ãƒƒãƒ‰
    - ListNet + RankNetæå¤±
    """
    def __init__(self, config):
        super().__init__()
        self.save_hyperparameters()
        self.config = config
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
        self.encoder = PatchTSTEncoder(
            in_feats=config.model.in_feats,
            d_model=config.model.d_model,
            depth=config.model.depth,
            patch_len=config.model.patch_len,
            stride=config.model.stride,
            n_heads=config.model.n_heads,
            dropout=config.model.dropout
        )
        
        # ãƒãƒ«ãƒãƒ›ãƒ©ã‚¤ã‚ºãƒ³ãƒ˜ãƒƒãƒ‰
        self.heads = nn.ModuleDict({
            f'h{h}': nn.Linear(config.model.d_model, 1)
            for h in config.head.horizons
        })
        
        # æå¤±é–¢æ•°
        self.criterion = CompositeLoss(config)
        
        # è©•ä¾¡æŒ‡æ¨™ã®ç´¯ç©ç”¨
        self.validation_outputs = []
    
    def forward(self, X):
        """
        Args:
            X: [B, L, F]
        Returns:
            scores: {horizon: [B]}
        """
        z, _ = self.encoder(X)  # [B, d_model]
        
        scores = {}
        for h, head in self.heads.items():
            scores[h] = head(z).squeeze(-1)  # [B]
        
        return scores
    
    def training_step(self, batch, batch_idx):
        if batch is None:
            return None
        
        X = batch['X']  # [B, L, F]
        Y = batch['y']  # [B, H]
        
        scores = self(X)  # {h: [B]}
        
        total_loss = 0.0
        logs = {}
        
        for i, h in enumerate(self.config.head.horizons):
            s = scores[f'h{h}']
            y = Y[:, i]
            
            # åŒå€¤æ—¥ã‚¹ã‚­ãƒƒãƒ—
            if torch.isclose(y.std(), torch.tensor(0., device=y.device)):
                continue
            
            loss_h = self.criterion(s, y)
            total_loss += loss_h
            logs[f'train_loss_h{h}'] = loss_h
        
        self.log_dict(logs, on_step=False, on_epoch=True, prog_bar=True)
        
        return total_loss
    
    def validation_step(self, batch, batch_idx):
        if batch is None:
            return None
        
        X = batch['X']
        Y = batch['y']
        
        scores = self(X)
        
        metrics = {}
        for i, h in enumerate(self.config.head.horizons):
            s = scores[f'h{h}'].detach()
            y = Y[:, i].detach()
            
            if torch.isclose(y.std(), torch.tensor(0., device=y.device)):
                continue
            
            # RankIC (Spearman)
            ic = spearman_rank_correlation(s, y)
            
            # Precision@K
            k = min(50, len(s)//10)
            pk = precision_at_k(s, y, k=k)
            
            metrics[f'val_RankIC_h{h}'] = ic
            metrics[f'val_P@{k}_h{h}'] = pk
        
        self.validation_outputs.append(metrics)
        
        return metrics
    
    def on_validation_epoch_end(self):
        if len(self.validation_outputs) == 0:
            return
        
        # å¹³å‡é›†è¨ˆ
        avg_metrics = {}
        all_keys = set()
        for output in self.validation_outputs:
            all_keys.update(output.keys())
        
        for key in all_keys:
            values = [o[key] for o in self.validation_outputs if key in o]
            if len(values) > 0:
                avg_metrics[key] = np.mean(values)
        
        self.log_dict(avg_metrics, prog_bar=True)
        
        self.validation_outputs.clear()
    
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.config.train.lr,
            weight_decay=self.config.train.weight_decay
        )
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.config.train.epochs
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'interval': 'epoch'
            }
        }


# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
def spearman_rank_correlation(pred, target):
    """Spearmané †ä½ç›¸é–¢ä¿‚æ•°"""
    pred_rank = torch.argsort(torch.argsort(pred, descending=True))
    target_rank = torch.argsort(torch.argsort(target, descending=True))
    
    n = len(pred)
    diff = (pred_rank - target_rank).float()
    
    rho = 1 - 6 * (diff**2).sum() / (n * (n**2 - 1))
    
    return rho.item()


def precision_at_k(scores, returns, k):
    """
    Precision@K: ä¸Šä½Kä»¶ã®ã†ã¡å®Ÿéš›ã«ãƒªã‚¿ãƒ¼ãƒ³ãŒæ­£ã ã£ãŸå‰²åˆ
    """
    _, topk_idx = torch.topk(scores, k=k, largest=True)
    topk_returns = returns[topk_idx]
    
    precision = (topk_returns > 0).float().mean()
    
    return precision.item()
```

#### æˆæœç‰©
```
apex-ranker/
â”œâ”€â”€ apex_ranker/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ patchtst.py       # PatchTSTã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
â”‚   â”‚   â”œâ”€â”€ heads.py          # äºˆæ¸¬ãƒ˜ãƒƒãƒ‰
â”‚   â”‚   â””â”€â”€ ranker.py         # APEXRankerV0
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ listnet.py
â”‚   â”‚   â”œâ”€â”€ ranknet.py
â”‚   â”‚   â””â”€â”€ composite.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py        # DayPanelDataset
â”‚   â”‚   â””â”€â”€ sampler.py        # DayBatchSampler
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ metrics.py        # RankIC, Precision@Kç­‰
â”‚       â””â”€â”€ config.py
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ v0_base.yaml
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_v0.py
â””â”€â”€ tests/
    â”œâ”€â”€ test_encoder.py
    â”œâ”€â”€ test_losses.py
    â””â”€â”€ test_dataset.py
```

#### æ¤œè¨¼åŸºæº–
- [ ] **å½¢çŠ¶ãƒ†ã‚¹ãƒˆ**: å…¥åŠ›`[B, L, F]` â†’ å‡ºåŠ›`[B, d_model]`æ­£å¸¸
- [ ] **æå¤±è¨ˆç®—**: ListNet/RankNetãŒåŒå€¤æ—¥ã§loss=0
- [ ] **å‹¾é…ãƒ•ãƒ­ãƒ¼**: `loss.backward()`å¾Œã«å‹¾é…ãŒéã‚¼ãƒ­
- [ ] **éå­¦ç¿’ãƒ†ã‚¹ãƒˆ**: 1æ—¥Ã—32éŠ˜æŸ„ã§RankICâ†’1.0ä»˜è¿‘
- [ ] **å­¦ç¿’å®Œäº†**: 1 foldï¼ˆ400æ—¥ï¼‰ã‚’15åˆ†ä»¥å†…ã§å®Œäº†
- [ ] **ç›®æ¨™é”æˆ**: val RankIC(5d) > 0.05, Sharpe > 1.2

---

### **Phase 2: v1å®Ÿè£…ï¼ˆãƒªã‚¹ã‚¯ç®¡ç†ï¼‰**ï¼ˆ1é€±é–“ï¼‰

#### ã‚¿ã‚¹ã‚¯
- [ ] åˆ†ä½ç‚¹ãƒ˜ãƒƒãƒ‰å®Ÿè£…ï¼ˆÏ„=0.1, 0.5, 0.9ï¼‰
- [ ] ãƒ”ãƒ³ãƒœãƒ¼ãƒ«æå¤±å®Ÿè£…
- [ ] ãƒªã‚¹ã‚¯èª¿æ•´ã‚¹ã‚³ã‚¢è¨ˆç®—
- [ ] åˆ†ä½æ ¡æ­£è©•ä¾¡

#### ã‚³ã‚¢å®Ÿè£…

```python
class QuantileHead(nn.Module):
    """
    åˆ†ä½ç‚¹äºˆæ¸¬ãƒ˜ãƒƒãƒ‰
    
    å„ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã«å¯¾ã—ã¦ã€è¤‡æ•°ã®åˆ†ä½ç‚¹ï¼ˆP10, P50, P90ç­‰ï¼‰ã‚’äºˆæ¸¬
    """
    def __init__(self, d_model, horizons, taus=[0.1, 0.5, 0.9]):
        super().__init__()
        self.horizons = horizons
        self.taus = taus
        
        # å„ãƒ›ãƒ©ã‚¤ã‚ºãƒ³Ã—å„åˆ†ä½ç‚¹ã®ãƒ˜ãƒƒãƒ‰
        self.heads = nn.ModuleDict()
        for h in horizons:
            self.heads[f'h{h}'] = nn.ModuleDict({
                f'q{int(tau*100)}': nn.Linear(d_model, 1)
                for tau in taus
            })
    
    def forward(self, z):
        """
        Args:
            z: [B, d_model]
        Returns:
            quantiles: {horizon: {quantile: [B]}}
        """
        outputs = {}
        for h in self.horizons:
            outputs[f'h{h}'] = {}
            for tau in self.taus:
                q_name = f'q{int(tau*100)}'
                outputs[f'h{h}'][q_name] = self.heads[f'h{h}'][q_name](z).squeeze(-1)
        
        return outputs


class PinballLoss(nn.Module):
    """ãƒ”ãƒ³ãƒœãƒ¼ãƒ«æå¤±ï¼ˆåˆ†ä½ç‚¹å›å¸°ç”¨ï¼‰"""
    def __init__(self, tau):
        super().__init__()
        self.tau = tau
    
    def forward(self, pred, target):
        """
        Args:
            pred: [B] åˆ†ä½ç‚¹äºˆæ¸¬
            target: [B] çœŸã®å€¤
        """
        error = target - pred
        loss = torch.maximum(self.tau * error, (self.tau - 1) * error)
        return loss.mean()


class APEXRankerV1(APEXRankerV0):
    """
    v1: ãƒªã‚¹ã‚¯ç®¡ç†ç‰ˆ
    - v0 + åˆ†ä½ç‚¹ãƒ˜ãƒƒãƒ‰
    - ãƒªã‚¹ã‚¯èª¿æ•´ã‚¹ã‚³ã‚¢ = median - Î»Ã—P10
    """
    def __init__(self, config):
        super().__init__(config)
        
        # åˆ†ä½ç‚¹ãƒ˜ãƒƒãƒ‰è¿½åŠ 
        self.quantile_head = QuantileHead(
            config.model.d_model,
            config.head.horizons,
            config.head.quantiles
        )
        
        # ãƒ”ãƒ³ãƒœãƒ¼ãƒ«æå¤±
        self.pinball_losses = {
            tau: PinballLoss(tau)
            for tau in config.head.quantiles
        }
        
        # ãƒªã‚¹ã‚¯èª¿æ•´ä¿‚æ•°
        self.risk_lambda = config.head.get('risk_lambda', 0.5)
    
    def forward(self, X, return_quantiles=False):
        z, _ = self.encoder(X)
        
        # ä¸­å¤®å€¤äºˆæ¸¬ï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨ï¼‰
        scores = {}
        for h in self.config.head.horizons:
            scores[f'h{h}'] = self.heads[f'h{h}'](z).squeeze(-1)
        
        if return_quantiles:
            # åˆ†ä½ç‚¹äºˆæ¸¬
            quantiles = self.quantile_head(z)
            
            # ãƒªã‚¹ã‚¯èª¿æ•´ã‚¹ã‚³ã‚¢
            adjusted_scores = {}
            for h in self.config.head.horizons:
                median = quantiles[f'h{h}']['q50']
                p10 = quantiles[f'h{h}']['q10']
                adjusted_scores[f'h{h}'] = median - self.risk_lambda * p10
            
            return scores, quantiles, adjusted_scores
        
        return scores
    
    def training_step(self, batch, batch_idx):
        if batch is None:
            return None
        
        X = batch['X']
        Y = batch['y']
        
        scores, quantiles, _ = self(X, return_quantiles=True)
        
        total_loss = 0.0
        logs = {}
        
        for i, h in enumerate(self.config.head.horizons):
            y = Y[:, i]
            
            if torch.isclose(y.std(), torch.tensor(0., device=y.device)):
                continue
            
            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°æå¤±ï¼ˆä¸­å¤®å€¤ã§ï¼‰
            s = scores[f'h{h}']
            ranking_loss = self.criterion(s, y)
            total_loss += ranking_loss
            logs[f'train_ranking_h{h}'] = ranking_loss
            
            # åˆ†ä½ç‚¹æå¤±
            for tau in self.config.head.quantiles:
                q_name = f'q{int(tau*100)}'
                q_pred = quantiles[f'h{h}'][q_name]
                pinball_loss = self.pinball_losses[tau](q_pred, y)
                total_loss += pinball_loss * 0.1  # é‡ã¿èª¿æ•´
                logs[f'train_pinball_{q_name}_h{h}'] = pinball_loss
        
        self.log_dict(logs, on_step=False, on_epoch=True)
        
        return total_loss
```

#### æ¤œè¨¼åŸºæº–
- [ ] **åˆ†ä½æ ¡æ­£**: P10è¶…éç‡ â‰ˆ 10% (Â±1%)
- [ ] **ãƒªã‚¹ã‚¯å‰Šæ¸›**: MDDæ”¹å–„ï¼ˆ-25% â†’ -20%ä»¥ä¸‹ï¼‰
- [ ] **Sharpeå‘ä¸Š**: +15%ä»¥ä¸Šï¼ˆv0æ¯”ï¼‰

---

### **Phase 3: v2å®Ÿè£…ï¼ˆã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰**ï¼ˆ1-2é€±é–“ï¼‰

#### ã‚¿ã‚¹ã‚¯
- [ ] AdaptiveKNNGraphå®Ÿè£…
- [ ] GATçµ±åˆ
- [ ] å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º

#### ã‚³ã‚¢å®Ÿè£…

```python
from torch_geometric.nn import GATConv

class AdaptiveKNNGraph(nn.Module):
    """
    å¸‚å ´çŠ¶æ…‹ã«å¿œã˜ã¦Kã‚’å‹•çš„èª¿æ•´ã™ã‚‹KNNã‚°ãƒ©ãƒ•æ³¨æ„
    
    Args:
        d_model: ç‰¹å¾´æ¬¡å…ƒ
        k_min: æœ€å°è¿‘å‚æ•°
        k_max: æœ€å¤§è¿‘å‚æ•°
        n_heads: GATãƒ˜ãƒƒãƒ‰æ•°
    """
    def __init__(self, d_model=192, k_min=5, k_max=30, n_heads=8):
        super().__init__()
        self.k_min = k_min
        self.k_max = k_max
        
        # å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºå™¨
        self.regime_detector = nn.Sequential(
            nn.Linear(d_model, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()  # 0-1ã§å¸‚å ´ã®çµåˆåº¦ã‚’æ¨å®š
        )
        
        # Graph Attention
        self.gat = GATConv(
            d_model, d_model // n_heads,
            heads=n_heads,
            concat=True,
            dropout=0.1,
            add_self_loops=False
        )
        
        self.fusion = nn.Linear(d_model, d_model)
    
    def forward(self, z):
        """
        Args:
            z: [B, d_model] å„éŠ˜æŸ„ã®åŸ‹ã‚è¾¼ã¿
        Returns:
            z_enhanced: [B, d_model] ã‚°ãƒ©ãƒ•æ–‡è„ˆã‚’å«ã‚€åŸ‹ã‚è¾¼ã¿
            k_used: int ä½¿ç”¨ã—ãŸè¿‘å‚æ•°
        """
        B = z.size(0)
        
        # 1. å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ ã®æ¨å®š
        regime = self.regime_detector(z).mean()  # ã‚¹ã‚«ãƒ©ãƒ¼
        k = int(self.k_min + (self.k_max - self.k_min) * regime.item())
        k = min(k, B - 1)  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¶…ãˆãªã„ã‚ˆã†ã«
        
        # 2. KNN ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
        z_norm = F.normalize(z, p=2, dim=1)
        sim = torch.mm(z_norm, z_norm.t())  # [B, B]
        
        # è‡ªå·±ãƒ«ãƒ¼ãƒ—ã‚’é™¤å¤–
        sim = sim.fill_diagonal_(-float('inf'))
        
        # Top-Ké¸æŠ
        topk_sim, topk_idx = torch.topk(sim, k=k, dim=1)  # [B, k]
        
        # ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆæ§‹ç¯‰
        edge_list = []
        edge_attr_list = []
        for i in range(B):
            for j_local in range(k):
                j_global = topk_idx[i, j_local].item()
                edge_list.append([i, j_global])
                edge_attr_list.append(topk_sim[i, j_local].item())
        
        edge_index = torch.tensor(edge_list, dtype=torch.long, device=z.device).t()  # [2, E]
        edge_attr = torch.tensor(edge_attr_list, dtype=torch.float, device=z.device).unsqueeze(-1)  # [E, 1]
        
        # 3. GATé©ç”¨
        z_gat = self.gat(z, edge_index, edge_attr=edge_attr)  # [B, d_model]
        
        # 4. å…ƒã®åŸ‹ã‚è¾¼ã¿ã¨çµ±åˆ
        z_enhanced = self.fusion(z + z_gat)
        
        return z_enhanced, k


class APEXRankerV2(APEXRankerV1):
    """
    v2: ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ³¨æ„ç‰ˆ
    - v1 + AdaptiveKNNGraph
    """
    def __init__(self, config):
        super().__init__(config)
        
        # KNNã‚°ãƒ©ãƒ•å±¤è¿½åŠ 
        self.knn_graph = AdaptiveKNNGraph(
            d_model=config.model.d_model,
            k_min=config.model.get('k_min', 5),
            k_max=config.model.get('k_max', 30),
            n_heads=config.model.n_heads
        )
    
    def forward(self, X, return_quantiles=False):
        z_base, _ = self.encoder(X)
        
        # ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ–‡è„ˆè¿½åŠ 
        z, k_used = self.knn_graph(z_base)
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢
        scores = {}
        for h in self.config.head.horizons:
            scores[f'h{h}'] = self.heads[f'h{h}'](z).squeeze(-1)
        
        if return_quantiles:
            quantiles = self.quantile_head(z)
            adjusted_scores = {}
            for h in self.config.head.horizons:
                median = quantiles[f'h{h}']['q50']
                p10 = quantiles[f'h{h}']['q10']
                adjusted_scores[f'h{h}'] = median - self.risk_lambda * p10
            
            return scores, quantiles, adjusted_scores
        
        return scores
```

#### æ¤œè¨¼åŸºæº–
- [ ] **RankICå‘ä¸Š**: +0.01-0.02ï¼ˆv1æ¯”ï¼‰
- [ ] **Kå€¤ã®å¦¥å½“æ€§**: å±æ©Ÿæ™‚â†’å¤§ããã€å®‰å®šæ™‚â†’å°ã•ã
- [ ] **è¨ˆç®—æ™‚é–“**: v1æ¯”ã§+20%ä»¥å†…

---

### **Phase 4: è©•ä¾¡ãƒ»æœ€é©åŒ–**ï¼ˆ1-2é€±é–“ï¼‰

#### ã‚¿ã‚¹ã‚¯
- [ ] Purged Walk-Forward CVå®Ÿè£…
- [ ] è©•ä¾¡æŒ‡æ¨™å®Œå…¨å®Ÿè£…
- [ ] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆOptunaï¼‰
- [ ] ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

#### æˆæœç‰©
- å®Œå…¨ãªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ
- æœ€é©ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœï¼ˆè¤‡æ•°ã‚³ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªï¼‰

---

### **Phase 5: v3-v5ï¼ˆé«˜åº¦åŒ–ï¼‰**ï¼ˆ2-6ãƒ¶æœˆï¼‰

è©³ç´°ã¯å¾Œè¿°ã®ã€Œé«˜åº¦åŒ–ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã€å‚ç…§

---

# è©³ç´°å®Ÿè£…ä»•æ§˜

## ğŸ”§ ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè©³ç´°

### 1. ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

#### FeatureSelector

```python
class FeatureSelector:
    """
    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ã„ã¦ç‰¹å¾´ã‚’å‹•çš„ã«é¸æŠ
    
    ä¸è¶³ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—ã—ã€å¾Œã‹ã‚‰è¿½åŠ å¯èƒ½
    """
    def __init__(self, config_path, metadata_path):
        self.config = self._load_config(config_path)
        self.metadata = self._load_metadata(metadata_path)
        
        self.selected_cols = self._select_features()
    
    def _select_features(self):
        """include/excludeã«åŸºã¥ã„ã¦ç‰¹å¾´åˆ—ã‚’ç¢ºå®š"""
        include = self.config['data']['features']['include']
        exclude = self.config['data']['features']['exclude']
        
        selected = []
        for group_name, group_info in self.metadata['feature_groups'].items():
            if group_info['enabled'] and group_name in include and group_name not in exclude:
                selected.extend(group_info['columns'])
        
        print(f"Selected {len(selected)} features from {len(include)} groups")
        return selected
    
    def get_feature_count(self):
        """æœ‰åŠ¹ç‰¹å¾´æ•°ã‚’è¿”ã™"""
        return len(self.selected_cols)
    
    def validate_data(self, df):
        """ãƒ‡ãƒ¼ã‚¿ã«å¿…è¦ãªåˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹æ¤œè¨¼"""
        missing = set(self.selected_cols) - set(df.columns)
        if missing:
            raise ValueError(f"Missing columns: {missing}")
        return True
```

#### æ­£è¦åŒ–

```python
class CrossSectionNormalizer:
    """
    å½“æ—¥å†…ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³Z-scoreæ­£è¦åŒ–
    
    å­¦ç¿’æ™‚: trainçµ±è¨ˆã§fit
    æ¨è«–æ™‚: å½“æ—¥çµ±è¨ˆã§å¤‰æ›ï¼ˆãƒªãƒ¼ã‚±ãƒ¼ã‚¸å›é¿ï¼‰
    """
    def __init__(self, clip_sigma=5.0):
        self.clip_sigma = clip_sigma
        self.train_stats = None
    
    def fit(self, X_train):
        """
        å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆè¨ˆç®—
        
        Args:
            X_train: [N_days, N_stocks, L, F] ã¾ãŸã¯ list of [N_stocks, L, F]
        """
        all_values = []
        for day_data in X_train:
            # å„æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„
            all_values.append(day_data.reshape(-1, day_data.shape[-1]))
        
        all_values = np.concatenate(all_values, axis=0)  # [N_total, F]
        
        self.train_stats = {
            'mean': np.mean(all_values, axis=0),
            'std': np.std(all_values, axis=0) + 1e-8
        }
    
    def transform(self, X, use_train_stats=False):
        """
        æ­£è¦åŒ–å®Ÿè¡Œ
        
        Args:
            X: [B, L, F] åŒæ—¥ãƒãƒƒãƒ
            use_train_stats: Trueãªã‚‰å­¦ç¿’çµ±è¨ˆã€Falseãªã‚‰å½“æ—¥çµ±è¨ˆ
        """
        if use_train_stats and self.train_stats is not None:
            # å­¦ç¿’çµ±è¨ˆã§æ­£è¦åŒ–ï¼ˆå­¦ç¿’æ™‚ï¼‰
            mean = self.train_stats['mean']
            std = self.train_stats['std']
        else:
            # å½“æ—¥çµ±è¨ˆã§æ­£è¦åŒ–ï¼ˆæ¨è«–æ™‚ï¼‰
            mean = X.mean(dim=(0, 1), keepdim=True)  # [1, 1, F]
            std = X.std(dim=(0, 1), keepdim=True) + 1e-8
        
        X_norm = (X - mean) / std
        
        # å¤–ã‚Œå€¤ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        X_norm = torch.clamp(X_norm, -self.clip_sigma, self.clip_sigma)
        
        return X_norm
```

### 2. å­¦ç¿’ãƒ»è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

#### Purged Walk-Forward Split

```python
class PurgedWalkForwardSplit:
    """
    æ™‚ç³»åˆ—ã‚’è€ƒæ…®ã—ãŸæ¤œè¨¼åˆ†å‰²
    
    - Purging: ãƒ†ã‚¹ãƒˆæœŸé–“å‰ã®embargoæ—¥æ•°ã‚’å­¦ç¿’ã‹ã‚‰é™¤å¤–
    - Embargo: ãƒ†ã‚¹ãƒˆæœŸé–“ã¨ã®é‡è¤‡å›é¿
    """
    def __init__(self, n_splits=6, embargo_days=20, test_ratio=0.15):
        self.n_splits = n_splits
        self.embargo_days = embargo_days
        self.test_ratio = test_ratio
    
    def split(self, dates):
        """
        Args:
            dates: æ˜‡é †ã®ãƒªã‚¹ãƒˆ
        Yields:
            (train_dates, val_dates, test_dates)
        """
        dates = sorted(dates)
        N = len(dates)
        
        # åˆ†å‰²ç‚¹è¨ˆç®—
        split_size = N // self.n_splits
        
        for i in range(self.n_splits):
            # ãƒ†ã‚¹ãƒˆæœŸé–“
            test_start = i * split_size
            test_end = min((i + 1) * split_size, N)
            test_dates = dates[test_start:test_end]
            
            # Embargoé©ç”¨
            train_end = max(0, test_start - self.embargo_days)
            
            # å­¦ç¿’æœŸé–“ï¼ˆãƒ†ã‚¹ãƒˆé–‹å§‹å‰ã¾ã§ï¼‰
            train_dates = dates[:train_end]
            
            # æ¤œè¨¼æœŸé–“ï¼ˆå­¦ç¿’ã®æœ«å°¾20%ç¨‹åº¦ï¼‰
            val_size = int(len(train_dates) * 0.2)
            val_dates = train_dates[-val_size:] if val_size > 0 else []
            train_dates = train_dates[:-val_size] if val_size > 0 else train_dates
            
            if len(train_dates) < 100:  # æœ€ä½æ—¥æ•°ãƒã‚§ãƒƒã‚¯
                continue
            
            yield {
                'fold': i,
                'train': train_dates,
                'val': val_dates,
                'test': test_dates
            }
```

#### è©•ä¾¡æŒ‡æ¨™

```python
class RankingMetrics:
    """ãƒ©ãƒ³ã‚­ãƒ³ã‚°è©•ä¾¡æŒ‡æ¨™ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³"""
    
    @staticmethod
    def rank_ic(scores, returns):
        """Spearman Rank IC"""
        return spearmanr(scores, returns)[0]
    
    @staticmethod
    def rank_icir(daily_ics):
        """ICIR = mean(IC) / std(IC)"""
        return np.mean(daily_ics) / (np.std(daily_ics) + 1e-8)
    
    @staticmethod
    def precision_at_k(scores, returns, k):
        """ä¸Šä½Kä»¶ã®æ­£è§£ç‡"""
        topk_idx = np.argsort(scores)[-k:]
        topk_returns = returns[topk_idx]
        return (topk_returns > 0).mean()
    
    @staticmethod
    def top_k_sharpe(daily_portfolio_returns):
        """Top-Kãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®å¹´ç‡Sharpe"""
        mean_ret = np.mean(daily_portfolio_returns)
        std_ret = np.std(daily_portfolio_returns)
        sharpe = mean_ret / (std_ret + 1e-8) * np.sqrt(252)
        return sharpe
    
    @staticmethod
    def max_drawdown(cumulative_returns):
        """æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³"""
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdown = (cumulative_returns - running_max) / running_max
        return drawdown.min()
```

---

# æ¤œè¨¼ãƒ—ãƒ­ãƒˆã‚³ãƒ«

## âœ… å¤šå±¤æ¤œè¨¼æˆ¦ç•¥

### ãƒ¬ãƒ™ãƒ«1: ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ

```python
# tests/test_encoder.py
def test_patchtst_shape():
    """å½¢çŠ¶ãƒ†ã‚¹ãƒˆ"""
    B, L, F = 64, 180, 109
    model = PatchTSTEncoder(in_feats=F, d_model=192, depth=3)
    
    X = torch.randn(B, L, F)
    z, tokens = model(X)
    
    assert z.shape == (B, 192)
    # ãƒ‘ãƒƒãƒæ•° = floor((L - patch_len) / stride) + 1
    expected_patches = (180 - 16) // 8 + 1  # = 21
    assert tokens.shape == (B, expected_patches, 192)


def test_loss_invariance():
    """æå¤±ã®ç½®æ›ä¸å¤‰æ€§"""
    B = 100
    scores = torch.randn(B)
    labels = torch.randn(B)
    
    loss_fn = ListNetLoss()
    loss1 = loss_fn(scores, labels)
    
    # åŒã˜é †åºã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    perm = torch.randperm(B)
    loss2 = loss_fn(scores[perm], labels[perm])
    
    assert torch.isclose(loss1, loss2, atol=1e-5)


def test_same_return_day():
    """åŒå€¤æ—¥ã®ã‚¹ã‚­ãƒƒãƒ—"""
    scores = torch.randn(100)
    labels = torch.ones(100) * 3.14  # å…¨ã¦åŒå€¤
    
    loss_fn = ListNetLoss()
    loss = loss_fn(scores, labels)
    
    assert torch.isclose(loss, torch.tensor(0.0))
```

### ãƒ¬ãƒ™ãƒ«2: çµ±åˆãƒ†ã‚¹ãƒˆ

```python
# tests/test_integration.py
def test_overfit_sanity():
    """éå­¦ç¿’ã‚µãƒ‹ãƒ†ã‚£ãƒã‚§ãƒƒã‚¯"""
    # 1æ—¥Ã—32éŠ˜æŸ„ã®å°ãƒ‡ãƒ¼ã‚¿ã§å®Œå…¨éå­¦ç¿’ã§ãã‚‹ã‹
    B, L, F = 32, 180, 109
    
    X = torch.randn(B, L, F)
    y = torch.randn(B)
    
    model = APEXRankerV0(config)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    for epoch in range(1000):
        scores = model(X)['h5']
        loss = model.criterion(scores, y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if epoch % 100 == 0:
            ic = spearman_rank_correlation(scores, y)
            print(f"Epoch {epoch}: IC = {ic:.4f}")
    
    final_ic = spearman_rank_correlation(scores, y)
    assert final_ic > 0.9, f"Failed to overfit: IC={final_ic}"


def test_full_pipeline():
    """ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ"""
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ â†’ å­¦ç¿’ â†’ è©•ä¾¡
    config = load_config('configs/v0_base.yaml')
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
    dataset = DayPanelDataset(...)
    train_loader = DataLoader(dataset, batch_size=None, collate_fn=collate_day_batch)
    
    # ãƒ¢ãƒ‡ãƒ«
    model = APEXRankerV0(config)
    
    # Trainer
    trainer = pl.Trainer(
        max_epochs=5,
        devices=1,
        accelerator='gpu',
        precision='16-mixed'
    )
    
    # å­¦ç¿’
    trainer.fit(model, train_loader)
    
    # æ¤œè¨¼
    metrics = trainer.validate(model, train_loader)
    
    assert metrics[0]['val_RankIC_h5'] > 0.0
```

### ãƒ¬ãƒ™ãƒ«3: ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```python
class BacktestEngine:
    """ã‚³ã‚¹ãƒˆè€ƒæ…®å‹ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, cost_bps=10, initial_capital=1e6):
        self.cost_bps = cost_bps
        self.initial_capital = initial_capital
    
    def run(self, predictions, returns, top_k=50):
        """
        Args:
            predictions: {date: {code: score}}
            returns: {date: {code: return}}
            top_k: ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚µã‚¤ã‚º
        
        Returns:
            metrics: {'sharpe', 'mdd', 'total_return', ...}
        """
        daily_returns = []
        positions = {}  # å‰æ—¥ãƒã‚¸ã‚·ãƒ§ãƒ³
        
        for date in sorted(predictions.keys()):
            # ä»Šæ—¥ã®ã‚¹ã‚³ã‚¢
            scores = predictions[date]
            
            # Top-Ké¸æŠ
            sorted_codes = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            new_positions = {code: 1.0/top_k for code, _ in sorted_codes[:top_k]}
            
            # ã‚¿ãƒ¼ãƒ³ã‚ªãƒ¼ãƒãƒ¼è¨ˆç®—
            turnover = self._compute_turnover(positions, new_positions)
            
            # ã‚³ã‚¹ãƒˆ
            cost = turnover * self.cost_bps / 10000
            
            # ãƒªã‚¿ãƒ¼ãƒ³
            port_return = sum(
                weight * returns[date].get(code, 0)
                for code, weight in new_positions.items()
            )
            
            # ã‚³ã‚¹ãƒˆæ§é™¤å¾Œãƒªã‚¿ãƒ¼ãƒ³
            net_return = port_return - cost
            daily_returns.append(net_return)
            
            positions = new_positions
        
        # æŒ‡æ¨™è¨ˆç®—
        daily_returns = np.array(daily_returns)
        cumulative = np.cumprod(1 + daily_returns)
        
        metrics = {
            'sharpe': RankingMetrics.top_k_sharpe(daily_returns),
            'mdd': RankingMetrics.max_drawdown(cumulative),
            'total_return': cumulative[-1] - 1,
            'avg_turnover': np.mean([self._compute_turnover(...) for ...]),
        }
        
        return metrics, daily_returns
    
    def _compute_turnover(self, old_pos, new_pos):
        """ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªå¤‰æ›´ç‡"""
        all_codes = set(old_pos.keys()) | set(new_pos.keys())
        turnover = sum(
            abs(new_pos.get(c, 0) - old_pos.get(c, 0))
            for c in all_codes
        )
        return turnover / 2  # ç‰‡é“
```

---

# é‹ç”¨ãƒ»æœ€é©åŒ–

## ğŸš€ GPUæœ€é©åŒ–å®Ÿè·µ

### 1. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

```python
# Gradient Checkpointing
class MemoryEfficientPatchTST(PatchTSTEncoder):
    def forward(self, x):
        x = x.transpose(1, 2)
        tokens = self.patch_embed(x)
        
        # Transformerãƒ–ãƒ­ãƒƒã‚¯ã§checkpointé©ç”¨
        for block in self.blocks:
            tokens = checkpoint(block, tokens, use_reentrant=False)
        
        tokens = self.norm(tokens)
        z = tokens.mean(dim=1)
        
        return z, tokens


# Flash Attentionï¼ˆPyTorch 2.0+ï¼‰
from torch.nn.functional import scaled_dot_product_attention

class FlashAttentionBlock(nn.Module):
    def __init__(self, d_model, n_heads, dropout):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        
        self.qkv = nn.Linear(d_model, d_model * 3)
        self.proj = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        B, N, C = x.shape
        
        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, heads, N, head_dim]
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Flash Attentionï¼ˆPyTorchçµ„ã¿è¾¼ã¿ï¼‰
        out = scaled_dot_product_attention(q, k, v, is_causal=False)
        
        out = out.transpose(1, 2).reshape(B, N, C)
        out = self.proj(out)
        
        return out
```

### 2. åˆ†æ•£å­¦ç¿’ï¼ˆDDPï¼‰

```python
# scripts/train_ddp.py
import torch.distributed as dist
import torch.multiprocessing as mp

def main_worker(rank, world_size, config):
    # ãƒ—ãƒ­ã‚»ã‚¹åˆæœŸåŒ–
    dist.init_process_group(
        backend='nccl',
        init_method='tcp://localhost:12355',
        world_size=world_size,
        rank=rank
    )
    
    torch.cuda.set_device(rank)
    
    # ãƒ¢ãƒ‡ãƒ«
    model = APEXRankerV0(config).cuda(rank)
    model = DDP(model, device_ids=[rank], find_unused_parameters=False)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆå„ãƒ—ãƒ­ã‚»ã‚¹ã§ç•°ãªã‚‹foldã‚’æ‹…å½“ï¼‰
    fold_ids = list(range(6))
    my_fold = fold_ids[rank % len(fold_ids)]
    
    dataset = get_fold_dataset(my_fold)
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)
    loader = DataLoader(dataset, batch_size=None, sampler=sampler)
    
    # Trainer
    trainer = pl.Trainer(
        devices=[rank],
        strategy='ddp',
        precision='16-mixed'
    )
    
    trainer.fit(model, loader)
    
    dist.destroy_process_group()


if __name__ == '__main__':
    world_size = torch.cuda.device_count()
    mp.spawn(main_worker, args=(world_size, config), nprocs=world_size)
```

### 3. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

```python
import optuna

def objective(trial):
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ææ¡ˆ
    config = {
        'd_model': trial.suggest_categorical('d_model', [128, 192, 256]),
        'depth': trial.suggest_int('depth', 2, 6),
        'patch_len': trial.suggest_categorical('patch_len', [12, 16, 20]),
        'stride': trial.suggest_categorical('stride', [6, 8, 12]),
        'lr': trial.suggest_loguniform('lr', 1e-4, 1e-2),
        'weight_decay': trial.suggest_loguniform('wd', 1e-5, 1e-3),
        'listnet_weight': trial.suggest_uniform('list_w', 0.5, 0.9),
    }
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    model = APEXRankerV0(config)
    
    # å­¦ç¿’
    trainer = pl.Trainer(max_epochs=20, devices=1, precision='16-mixed')
    trainer.fit(model, train_loader, val_loader)
    
    # è©•ä¾¡
    metrics = trainer.validate(model, val_loader)
    val_ic = metrics[0]['val_RankIC_h5']
    
    return val_ic


# æœ€é©åŒ–å®Ÿè¡Œ
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50, n_jobs=4)

print("Best params:", study.best_params)
print("Best IC:", study.best_value)
```

---

## ğŸ“Š ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»å¯è¦–åŒ–

### TensorBoardçµ±åˆ

```python
from pytorch_lightning.loggers import TensorBoardLogger

logger = TensorBoardLogger('logs/', name='apex_ranker')

trainer = pl.Trainer(
    logger=logger,
    log_every_n_steps=10
)

# ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚°
class APEXRankerV0(pl.LightningModule):
    def training_step(self, batch, batch_idx):
        # ... (æå¤±è¨ˆç®—)
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚°
        self.log('gpu_memory', torch.cuda.memory_allocated() / 1e9)  # GB
        self.log('batch_size', len(batch['X']))
        
        return loss
```

### Weights & Biasesçµ±åˆ

```python
import wandb
from pytorch_lightning.loggers import WandbLogger

wandb_logger = WandbLogger(project='apex-ranker', name='v0_baseline')

trainer = pl.Trainer(logger=wandb_logger)

# äºˆæ¸¬åˆ†å¸ƒã®å¯è¦–åŒ–
wandb.log({
    'score_distribution': wandb.Histogram(scores.cpu().numpy()),
    'ic_over_time': wandb.plot.line_series(
        xs=dates,
        ys=[daily_ics],
        keys=['RankIC'],
        title='Daily RankIC'
    )
})
```

---

## ğŸ”’ å†ç¾æ€§ã®ç¢ºä¿

```python
def set_seed(seed=42):
    """å®Œå…¨ãªå†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # æ±ºå®šçš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆé€Ÿåº¦ä½ä¸‹ã‚ã‚Šï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # PyTorch Lightningã®è¨­å®š
    pl.seed_everything(seed, workers=True)


# å®Ÿè¡Œæ™‚
set_seed(42)

trainer = pl.Trainer(
    deterministic=True,  # å®Œå…¨å†ç¾ãƒ¢ãƒ¼ãƒ‰
    ...
)
```

---

## ğŸ“¦ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

### æ¨è«–APIï¼ˆFastAPIï¼‰

```python
from fastapi import FastAPI
import onnxruntime as ort

app = FastAPI()

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆONNXå¤‰æ›æ¸ˆã¿ï¼‰
session = ort.InferenceSession(
    'models/apex_ranker_v2.onnx',
    providers=['CUDAExecutionProvider']
)

@app.post('/predict')
async def predict(data: dict):
    """
    Args:
        data: {'stocks': [{'code': 'AAPL', 'features': [...]}]}
    Returns:
        {'predictions': [{'code': 'AAPL', 'score': 0.85}]}
    """
    # å‰å‡¦ç†
    X = preprocess(data['stocks'])  # [B, L, F]
    
    # æ¨è«–
    input_name = session.get_inputs()[0].name
    outputs = session.run(None, {input_name: X})
    
    scores = outputs[0]  # [B, H]
    
    # å¾Œå‡¦ç†
    results = [
        {'code': stock['code'], 'score': float(score[2])}  # 5då…ˆã®ã‚¹ã‚³ã‚¢
        for stock, score in zip(data['stocks'], scores)
    ]
    
    return {'predictions': results}
```

---

## ğŸ¯ ã¾ã¨ã‚

### å®Ÿè£…ã®æ®µéšçš„é€²åŒ–

| ãƒ•ã‚§ãƒ¼ã‚º | æœŸé–“ | æˆæœç‰© | æ€§èƒ½ç›®æ¨™ |
|---------|------|--------|---------|
| **Phase 0** | 3-5æ—¥ | ç’°å¢ƒæ§‹ç¯‰ | GPUèªè­˜ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ |
| **Phase 1** | 1-2é€± | v0å®Ÿè£… | IC>0.05, Sharpe>1.2 |
| **Phase 2** | 1é€± | v1å®Ÿè£… | MDDæ”¹å–„-15% |
| **Phase 3** | 1-2é€± | v2å®Ÿè£… | IC>0.08 |
| **Phase 4** | 1-2é€± | è©•ä¾¡ãƒ»æœ€é©åŒ– | æœ¬ç•ªæº–å‚™å®Œäº† |
| **Phase 5** | 2-6ãƒ¶æœˆ | v3-v5 | IC>0.10, Sharpe>2.5 |

### é‡è¦ãªè¨­è¨ˆåŸå‰‡

1. **ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†é›¢**: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã§ã‚‚å‹•ä½œã€å¾Œã‹ã‚‰æ‹¡å¼µå¯èƒ½
2. **æ®µéšçš„æ¤œè¨¼**: å„ãƒ•ã‚§ãƒ¼ã‚ºã§æ˜ç¢ºãªç›®æ¨™ã¨æ¤œè¨¼åŸºæº–
3. **GPUæœ€é©åŒ–**: å®Ÿç”¨çš„ãªè¨ˆç®—æ™‚é–“ã‚’å®Ÿç¾
4. **å†ç¾æ€§**: å®Œå…¨ãªå†ç¾ã¨åŠ¹æœæ¸¬å®š

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

```bash
# ç’°å¢ƒæ§‹ç¯‰
conda create -n apex-ranker python=3.10
conda activate apex-ranker
pip install -r requirements.txt

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
python scripts/prepare_data.py

# v0å­¦ç¿’
python scripts/train_v0.py --config configs/v0_base.yaml

# è©•ä¾¡
python scripts/evaluate.py --checkpoint logs/version_0/checkpoints/best.ckpt
```

---

**å®Ÿè£…æ”¯æ´ãŒå¿…è¦ãªå ´åˆã¯ã€å„ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Œå…¨ç‰ˆã€ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã€Dockerè¨­å®šç­‰ã‚’æä¾›å¯èƒ½ã§ã™ï¼**
