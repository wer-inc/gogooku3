"""
é‡‘èMLãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦å®Ÿéš›ã®æ ªä¾¡äºˆæ¸¬ã‚’å®Ÿè¡Œ
"""

import json
import logging
import warnings
from datetime import datetime
from pathlib import Path

import joblib
import lightgbm as lgb
import numpy as np
import pandas as pd

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StockPredictor:
    """æ ªä¾¡äºˆæ¸¬ã‚¯ãƒ©ã‚¹"""

    def __init__(self, model_dir: str):
        """åˆæœŸåŒ–"""
        self.model_dir = Path(model_dir)
        self.model = None
        self.scaler = None
        self.feature_cols = None
        self.feature_info = None

        self._load_model_components()

    def _load_model_components(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“ ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆèª­ã¿è¾¼ã¿ä¸­...")

        try:
            # LightGBMãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model_path = self.model_dir / 'lightgbm_model.txt'
            self.model = lgb.Booster(model_file=str(model_path))
            logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_path}")

            # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿
            scaler_path = self.model_dir / 'scaler.pkl'
            self.scaler = joblib.load(scaler_path)
            logger.info(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿: {scaler_path}")

            # ç‰¹å¾´é‡æƒ…å ±èª­ã¿è¾¼ã¿
            info_path = self.model_dir / 'feature_info.json'
            with open(info_path, encoding='utf-8') as f:
                self.feature_info = json.load(f)

            self.feature_cols = self.feature_info['feature_columns']
            logger.info(f"âœ… ç‰¹å¾´é‡æƒ…å ±èª­ã¿è¾¼ã¿: {len(self.feature_cols)}å€‹ã®ç‰¹å¾´é‡")

        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def load_prediction_data(self, data_path: str, n_recent_days: int = 60):
        """äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        logger.info(f"ğŸ“Š äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {data_path}")

        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = pd.read_parquet(data_path)
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}è¡Œ Ã— {len(df.columns)}åˆ—")

            # æœ€æ–°ã®æ—¥ä»˜ã‚’å–å¾—
            latest_date = df['date'].max()
            cutoff_date = latest_date - pd.Timedelta(days=n_recent_days)

            # æœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠï¼ˆç‰¹å¾´é‡è¨ˆç®—ã«å¿…è¦ãªæœŸé–“ï¼‰
            recent_df = df[df['date'] >= cutoff_date].copy()
            logger.info(f"ğŸ“… äºˆæ¸¬å¯¾è±¡æœŸé–“: {cutoff_date} ~ {latest_date}")

            # éŠ˜æŸ„ã”ã¨ã«æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ
            latest_data = []
            for code in recent_df['Code'].unique():
                stock_data = recent_df[recent_df['Code'] == code].copy()
                stock_data = stock_data.sort_values('date')

                # æœ€ä½10æ—¥ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹éŠ˜æŸ„ã®ã¿
                if len(stock_data) >= 10:
                    latest_record = stock_data.iloc[-1].copy()
                    latest_data.append(latest_record)

            if not latest_data:
                raise ValueError("äºˆæ¸¬å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            prediction_df = pd.DataFrame(latest_data)
            logger.info(f"ğŸ¯ äºˆæ¸¬å¯¾è±¡éŠ˜æŸ„æ•°: {len(prediction_df):,}éŠ˜æŸ„")

            return prediction_df

        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def calculate_prediction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """äºˆæ¸¬ç”¨ç‰¹å¾´é‡ã®è¨ˆç®—"""
        logger.info("ğŸ”§ äºˆæ¸¬ç”¨ç‰¹å¾´é‡è¨ˆç®—ã‚’é–‹å§‹...")

        # å¿…è¦ãªç‰¹å¾´é‡ã®ã¿æŠ½å‡º
        features_df = df[['Code', 'date'] + self.feature_cols].copy()

        # æ¬ æå€¤å‡¦ç†
        for col in self.feature_cols:
            if features_df[col].isnull().any():
                features_df[col] = features_df[col].fillna(method='ffill')

        # ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        logger.info("ğŸ“Š ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œ...")
        scaled_features = self.scaler.transform(features_df[self.feature_cols])
        scaled_df = pd.DataFrame(scaled_features, columns=self.feature_cols,
                               index=features_df.index)

        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ
        result_df = features_df[['Code', 'date']].copy()
        result_df[self.feature_cols] = scaled_df

        logger.info("âœ… ç‰¹å¾´é‡è¨ˆç®—å®Œäº†")
        return result_df

    def predict_returns(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """ãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬å®Ÿè¡Œ"""
        logger.info("ğŸ¯ ãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬ã‚’é–‹å§‹...")

        # äºˆæ¸¬å®Ÿè¡Œ
        predictions = self.model.predict(features_df[self.feature_cols])

        # çµæœãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        results_df = features_df[['Code', 'date']].copy()
        results_df['predicted_return'] = predictions

        # äºˆæ¸¬ã‚’ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨è¨˜ã«å¤‰æ›
        results_df['predicted_return_pct'] = results_df['predicted_return'] * 100

        # äºˆæ¸¬æ–¹å‘ã®åˆ¤å®š
        results_df['prediction_direction'] = np.where(
            results_df['predicted_return'] > 0, 'UP', 'DOWN'
        )

        # äºˆæ¸¬å¼·åº¦ã®è¨ˆç®—ï¼ˆçµ¶å¯¾å€¤ï¼‰
        results_df['prediction_strength'] = np.abs(results_df['predicted_return'])

        logger.info("âœ… ãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬å®Œäº†")
        return results_df

    def get_top_predictions(self, results_df: pd.DataFrame,
                          top_n: int = 20,
                          direction: str = 'both') -> pd.DataFrame:
        """ãƒˆãƒƒãƒ—äºˆæ¸¬çµæœã‚’å–å¾—"""
        if direction == 'up':
            filtered = results_df[results_df['prediction_direction'] == 'UP']
            sorted_results = filtered.sort_values('predicted_return', ascending=False)
        elif direction == 'down':
            filtered = results_df[results_df['prediction_direction'] == 'DOWN']
            sorted_results = filtered.sort_values('predicted_return', ascending=True)
        else:  # both
            sorted_results = results_df.sort_values('prediction_strength', ascending=False)

        return sorted_results.head(top_n)

    def analyze_predictions(self, results_df: pd.DataFrame) -> dict:
        """äºˆæ¸¬çµæœã®åˆ†æ"""
        analysis = {}

        # å…¨ä½“çµ±è¨ˆ
        analysis['total_stocks'] = len(results_df)
        analysis['up_predictions'] = (results_df['prediction_direction'] == 'UP').sum()
        analysis['down_predictions'] = (results_df['prediction_direction'] == 'DOWN').sum()
        analysis['avg_prediction'] = results_df['predicted_return'].mean()
        analysis['std_prediction'] = results_df['predicted_return'].std()

        # äºˆæ¸¬åˆ†å¸ƒ
        analysis['prediction_ranges'] = {
            'strong_up': (results_df['predicted_return'] > 0.02).sum(),
            'moderate_up': ((results_df['predicted_return'] > 0.01) & (results_df['predicted_return'] <= 0.02)).sum(),
            'weak_up': ((results_df['predicted_return'] > 0) & (results_df['predicted_return'] <= 0.01)).sum(),
            'weak_down': ((results_df['predicted_return'] < 0) & (results_df['predicted_return'] >= -0.01)).sum(),
            'moderate_down': ((results_df['predicted_return'] < -0.01) & (results_df['predicted_return'] >= -0.02)).sum(),
            'strong_down': (results_df['predicted_return'] < -0.02).sum()
        }

        return analysis

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 80)
    print("ğŸ¯ é‡‘èMLãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 80)

    # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    model_dir = 'models/ml_model_20250829_135853'
    data_path = 'data/processed/ml_dataset_20250829_135002.parquet'

    try:
        # äºˆæ¸¬å™¨åˆæœŸåŒ–
        print("\\nğŸ¤– Step 1: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿")
        predictor = StockPredictor(model_dir)

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        print("\\nğŸ“Š Step 2: äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™")
        prediction_data = predictor.load_prediction_data(data_path, n_recent_days=60)

        # ç‰¹å¾´é‡è¨ˆç®—
        print("\\nğŸ”§ Step 3: ç‰¹å¾´é‡è¨ˆç®—")
        features_df = predictor.calculate_prediction_features(prediction_data)

        # äºˆæ¸¬å®Ÿè¡Œ
        print("\\nğŸ¯ Step 4: ãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬å®Ÿè¡Œ")
        results_df = predictor.predict_returns(features_df)

        # çµæœåˆ†æ
        print("\\nğŸ“ˆ Step 5: äºˆæ¸¬çµæœåˆ†æ")
        analysis = predictor.analyze_predictions(results_df)

        # çµæœè¡¨ç¤º
        print("\\n" + "=" * 80)
        print("ğŸ‰ äºˆæ¸¬å®Œäº†ï¼")
        print("=" * 80)

        print("\\nğŸ“Š å…¨ä½“çµ±è¨ˆ:")
        print(f"   äºˆæ¸¬å¯¾è±¡éŠ˜æŸ„æ•°: {analysis['total_stocks']:,}")
        print(f"   ä¸Šæ˜‡äºˆæ¸¬: {analysis['up_predictions']:,} ({analysis['up_predictions']/analysis['total_stocks']*100:.1f}%)")
        print(f"   ä¸‹é™äºˆæ¸¬: {analysis['down_predictions']:,} ({analysis['down_predictions']/analysis['total_stocks']*100:.1f}%)")
        print(f"   å¹³å‡äºˆæ¸¬ãƒªã‚¿ãƒ¼ãƒ³: {analysis['avg_prediction']:.6f}")
        print(f"   äºˆæ¸¬ãƒªã‚¿ãƒ¼ãƒ³æ¨™æº–åå·®: {analysis['std_prediction']:.6f}")

        print("\\nğŸ“ˆ äºˆæ¸¬å¼·åº¦åˆ†å¸ƒ:")
        ranges = analysis['prediction_ranges']
        print(f"   å¼·æ°—ä¸Šæ˜‡ (>2%): {ranges['strong_up']:,}")
        print(f"   ä¸­ä¸Šæ˜‡ (1-2%): {ranges['moderate_up']:,}")
        print(f"   å¼±ä¸Šæ˜‡ (0-1%): {ranges['weak_up']:,}")
        print(f"   å¼±ä¸‹é™ (0-1%): {ranges['weak_down']:,}")
        print(f"   ä¸­ä¸‹é™ (1-2%): {ranges['moderate_down']:,}")
        print(f"   å¼·æ°—ä¸‹é™ (>2%): {ranges['strong_down']:,}")

        # ãƒˆãƒƒãƒ—äºˆæ¸¬è¡¨ç¤º
        print("\\nğŸ† ãƒˆãƒƒãƒ—20äºˆæ¸¬ï¼ˆå¼·åº¦é †ï¼‰:")
        top_predictions = predictor.get_top_predictions(results_df, top_n=20, direction='both')
        for _i, row in top_predictions.head(10).iterrows():
            direction_icon = "ğŸ“ˆ" if row['prediction_direction'] == 'UP' else "ğŸ“‰"
            print(f"   {direction_icon} {row['Code']} | äºˆæ¸¬: {row['predicted_return_pct']:.4f}% | å¼·åº¦: {row['prediction_strength']:.4f}")
        # çµæœä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = f'results/predictions_{timestamp}.csv'
        Path('results').mkdir(exist_ok=True)
        results_df.to_csv(output_path, index=False)
        print(f"\\nğŸ’¾ çµæœä¿å­˜: {output_path}")

        # è¿½åŠ åˆ†æ
        print("\\nğŸ” è©³ç´°åˆ†æ:")
        print(f"   äºˆæ¸¬æœŸé–“: {results_df['date'].min()} ~ {results_df['date'].max()}")

        if analysis['up_predictions'] > analysis['down_predictions']:
            print("   ğŸ“Š å…¨ä½“çš„ã«å¼·æ°—ç›¸å ´äºˆæ¸¬")
        elif analysis['up_predictions'] < analysis['down_predictions']:
            print("   ğŸ“Š å…¨ä½“çš„ã«å¼±æ°—ç›¸å ´äºˆæ¸¬")
        else:
            print("   ğŸ“Š ä¸­ç«‹çš„ç›¸å ´äºˆæ¸¬")

        print("\\nâœ… äºˆæ¸¬å®Œäº†ï¼")

    except Exception as e:
        logger.error(f"âŒ äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
