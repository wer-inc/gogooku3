#!/usr/bin/env python3
"""
gogooku3-standalone ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å£Šã‚Œãšãƒ»å¼·ããƒ»é€Ÿã ã‚’å®Ÿç¾ã™ã‚‹é‡‘èML ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆå®Ÿè¡Œ

åˆ©ç”¨å¯èƒ½ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:
1. å®‰å…¨å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆSafe Training Pipelineï¼‰
2. MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ï¼ˆML Dataset Builderï¼‰
3. ç›´æ¥APIãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ï¼ˆDirect API Dataset Builderï¼‰
4. å®Œå…¨ATFTå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆComplete ATFT Training Pipelineï¼‰
5. ATFTæ¨è«–ï¼ˆATFT Inferenceï¼‰
"""

import sys
import asyncio
import argparse
import logging
from pathlib import Path
from typing import Optional, List, Dict

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(str(Path(__file__).parent))

from scripts.run_safe_training import SafeTrainingPipeline
from scripts.data.ml_dataset_builder import MLDatasetBuilder
from scripts.data.direct_api_dataset_builder import DirectAPIDatasetBuilder
from scripts.integrated_ml_training_pipeline import CompleteATFTTrainingPipeline

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/main.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class GoGooKu3MainRunner:
    """gogooku3-standalone ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.ensure_directories()
    
    def ensure_directories(self):
        """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        dirs = [
            "logs",
            "data/processed", 
            "output",
            "output/results",
            "output/checkpoints",
            "output/atft_data"
        ]
        
        for dir_path in dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)

    def run_safe_training(self, mode: str = "full"):
        """å®‰å…¨å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting Safe Training Pipeline...")
        
        pipeline = SafeTrainingPipeline()
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼ˆåŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        result = pipeline.run_full_pipeline()

        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if 'error' in result:
            logger.error(f"âŒ Safe Training Pipeline failed: {result['error']}")
            return False, result
        else:
            logger.info("âœ… Safe Training Pipeline completed successfully!")
            logger.info(f"ğŸ“Š Report saved: {result.get('report_path', 'N/A')}")
            return True, result

    def run_ml_dataset_builder(self):
        """MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting ML Dataset Builder...")
        
        builder = MLDatasetBuilder()
        result = builder.build_enhanced_dataset()
        
        if result:
            logger.info("âœ… ML Dataset Builder completed successfully!")
            logger.info(f"ğŸ“Š Dataset: {len(result['df'])} rows, {result['metadata']['stocks']} stocks")
            return True, result
        else:
            logger.error("âŒ ML Dataset Builder failed")
            return False, {}

    def run_expand_dataset(self, max_stocks: int = 500):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ‹¡å¼µã®å®Ÿè¡Œï¼ˆéŠ˜æŸ„ãƒ™ãƒ¼ã‚¹ï¼‰"""
        logger.info("ğŸš€ Starting Dataset Expansion...")
        logger.info(f"ğŸ“Š Target: {max_stocks} new stocks")

        try:
            from scripts.data.expand_dataset import JQuantsDatasetExpander

            expander = JQuantsDatasetExpander()
            success = expander.expand_dataset(max_stocks=max_stocks)

            if success:
                logger.info("âœ… Dataset expansion completed successfully!")
                return True, {"message": f"Dataset expanded with {max_stocks} new stocks"}
            else:
                logger.error("âŒ Dataset expansion failed")
                return False, {"error": "Dataset expansion failed"}

        except ImportError as e:
            logger.error(f"âŒ Failed to import expansion module: {e}")
            return False, {"error": f"Import error: {e}"}
        except Exception as e:
            logger.error(f"âŒ Dataset expansion failed with exception: {e}")
            return False, {"error": str(e)}

    def run_expand_dataset_by_date(self, date: str, exclude_market_codes: List[str] = None):
        """æ—¥ä»˜ãƒ™ãƒ¼ã‚¹å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—ã®å®Ÿè¡Œï¼ˆMarketCodeãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œï¼‰"""
        logger.info("ğŸš€ Starting Date-based Dataset Expansion...")
        logger.info(f"ğŸ“… Target Date: {date}")
        logger.info(f"ğŸ“Š Exclude MarketCodes: {exclude_market_codes or ['0105', '0109']}")

        try:
            from scripts.data.expand_dataset import JQuantsDatasetExpander

            expander = JQuantsDatasetExpander()
            all_quotes = expander.expand_dataset_by_date(date, exclude_market_codes=exclude_market_codes)

            if all_quotes:
                logger.info("âœ… Date-based dataset expansion completed successfully!")
                return True, {"message": f"Retrieved {len(all_quotes)} filtered stocks for {date}"}
            else:
                logger.error("âŒ Date-based dataset expansion failed")
                return False, {"error": "Date-based dataset expansion failed"}

        except ImportError as e:
            logger.error(f"âŒ Failed to import expansion module: {e}")
            return False, {"error": f"Import error: {e}"}
        except Exception as e:
            logger.error(f"âŒ Date-based dataset expansion failed with exception: {e}")
            return False, {"error": str(e)}

    def run_expand_dataset_by_range(self, start_date: str, end_date: str):
        """æœŸé–“ãƒ™ãƒ¼ã‚¹å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—ã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting Range-based Dataset Expansion...")
        logger.info(f"ğŸ“… Date Range: {start_date} ~ {end_date}")

        try:
            from scripts.data.expand_dataset import JQuantsDatasetExpander

            expander = JQuantsDatasetExpander()
            all_quotes = expander.expand_dataset_by_date_range(start_date, end_date)

            if all_quotes:
                logger.info("âœ… Range-based dataset expansion completed successfully!")
                return True, {"message": f"Retrieved {len(all_quotes)} records for {start_date} ~ {end_date}"}
            else:
                logger.error("âŒ Range-based dataset expansion failed")
                return False, {"error": "Range-based dataset expansion failed"}

        except ImportError as e:
            logger.error(f"âŒ Failed to import expansion module: {e}")
            return False, {"error": f"Import error: {e}"}
        except Exception as e:
            logger.error(f"âŒ Range-based dataset expansion failed with exception: {e}")
            return False, {"error": str(e)}

    def run_expand_historical_all_stocks(self, years: int = 5, max_days: int = None, exclude_market_codes: List[str] = None):
        """å–å¼•ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‚’ä½¿ã£ãŸéå»Nå¹´åˆ†ã®å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—ã®å®Ÿè¡Œï¼ˆMarketCodeãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œï¼‰"""
        logger.info("ğŸš€ Starting Historical All Stocks Dataset Expansion...")
        logger.info(f"ğŸ“… Years: {years}")
        logger.info(f"ğŸ“Š Max Days: {max_days if max_days else 'No limit'}")
        logger.info(f"ğŸ“Š Exclude MarketCodes: {exclude_market_codes or ['0105', '0109']}")

        try:
            from scripts.data.expand_dataset import JQuantsDatasetExpander

            expander = JQuantsDatasetExpander()
            all_quotes = expander.get_historical_all_stocks(years=years, max_days=max_days, exclude_market_codes=exclude_market_codes)

            if all_quotes:
                # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                df = pd.DataFrame(all_quotes)

                # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‡¦ç†
                if "Date" in df.columns:
                    df["Date"] = pd.to_datetime(df["Date"])
                    df = df.rename(columns={"Date": "date"})

                # ãƒ•ã‚¡ã‚¤ãƒ«åã«æœŸé–“ã‚’å«ã‚ã‚‹
                output_file = expander.data_dir / f"historical_filtered_stocks_{years}years_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
                df.to_parquet(output_file, index=False)

                logger.info(f"ğŸ’¾ éå»{years}å¹´åˆ†ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {output_file}")
                logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿: {len(df)}è¡Œ, {df['Code'].nunique()}éŠ˜æŸ„")

                logger.info("âœ… Historical all stocks dataset expansion completed successfully!")
                return True, {
                    "message": f"Retrieved {len(all_quotes)} filtered records for {years} years",
                    "file": str(output_file),
                    "stocks": df['Code'].nunique(),
                    "total_records": len(df)
                }
            else:
                logger.error("âŒ Historical all stocks dataset expansion failed")
                return False, {"error": "Historical all stocks dataset expansion failed"}

        except ImportError as e:
            logger.error(f"âŒ Failed to import expansion module: {e}")
            return False, {"error": f"Import error: {e}"}
        except Exception as e:
            logger.error(f"âŒ Historical all stocks dataset expansion failed with exception: {e}")
            return False, {"error": str(e)}

    def create_ml_dataset(self, years: int = 5, exclude_market_codes: List[str] = None, use_existing_data: bool = True):
        """éå»å–ã‚Œã‚‹å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦MLç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        logger.info("ğŸš€ Creating ML Dataset from Historical Data...")
        logger.info(f"ğŸ“… Years: {years}")
        logger.info(f"ğŸ“Š Exclude MarketCodes: {exclude_market_codes or ['0105', '0109']}")
        logger.info(f"ğŸ“Š Use Existing Data: {use_existing_data}")

        try:
            all_quotes = []

            if use_existing_data:
                # Step 1: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨
                logger.info("ğŸ“Š Step 1: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹")
                existing_quotes = self._load_existing_data()
                if existing_quotes:
                    all_quotes.extend(existing_quotes)
                    logger.info(f"âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(existing_quotes)}ä»¶")

            # Step 2: æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆAPIãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            if not use_existing_data or len(all_quotes) < 10000:  # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã¯APIå–å¾—
                logger.info("ğŸ“Š Step 2: æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚’é–‹å§‹")
                try:
                    from scripts.data.expand_dataset import JQuantsDatasetExpander
                    expander = JQuantsDatasetExpander()

                    new_quotes = expander.get_historical_all_stocks(
                        years=min(years, 2),  # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦æœ€å¤§2å¹´ã«åˆ¶é™
                        max_days=50,  # æœ€å¤§50æ—¥é–“ã«åˆ¶é™
                        exclude_market_codes=exclude_market_codes
                    )

                    if new_quotes:
                        all_quotes.extend(new_quotes)
                        logger.info(f"âœ… æ–°è¦ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(new_quotes)}ä»¶")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ–°è¦ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¹ã‚­ãƒƒãƒ—: {e}")

            if not all_quotes:
                logger.error("âŒ åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return False, {"error": "No data available"}

            # Step 3: MLç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            logger.info("ğŸ”§ Step 3: MLç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚’é–‹å§‹")
            ml_dataset_path = self._create_ml_dataset_from_quotes(all_quotes)

            if not ml_dataset_path:
                logger.error("âŒ MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False, {"error": "Failed to create ML dataset"}

            logger.info("âœ… ML Dataset Creation completed successfully!")

            return True, {
                "message": f"Created ML dataset from {len(all_quotes)} historical records",
                "raw_data_records": len(all_quotes),
                "ml_dataset_path": ml_dataset_path,
                "years": years,
                "excluded_markets": exclude_market_codes or ['0105', '0109'],
                "used_existing_data": use_existing_data
            }

        except ImportError as e:
            logger.error(f"âŒ Failed to import expansion module: {e}")
            return False, {"error": f"Import error: {e}"}
        except Exception as e:
            logger.error(f"âŒ ML Dataset creation failed with exception: {e}")
            return False, {"error": str(e)}

    def _create_ml_dataset_from_quotes(self, all_quotes: List[Dict]) -> str:
        """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰MLç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        logger.info("ğŸ”§ MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå‡¦ç†ã‚’é–‹å§‹")

        try:
            import pandas as pd
            import numpy as np
            from pathlib import Path
            from datetime import datetime

            # ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
            df = pd.DataFrame(all_quotes)

            # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‡¦ç†
            if "Date" in df.columns:
                df["Date"] = pd.to_datetime(df["Date"])
                df = df.rename(columns={"Date": "date"})

            logger.info(f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿: {len(df)}è¡Œ")

            # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
            df = self._clean_stock_data(df)

            # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            df = self._create_ml_features(df)

            # MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ä¿å­˜
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = Path("data/processed") / f"ml_dataset_{timestamp}.parquet"
            output_path.parent.mkdir(parents=True, exist_ok=True)

            df.to_parquet(output_path, index=False)

            logger.info(f"ğŸ’¾ MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜: {output_path}")
            logger.info(f"ğŸ“Š MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")

            return str(output_path)

        except Exception as e:
            logger.error(f"âŒ MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _load_existing_data(self) -> List[Dict]:
        """æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§çµ±åˆ"""
        logger.info("ğŸ“ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹")

        try:
            import polars as pl
            import pandas as pd
            from pathlib import Path

            data_dir = Path("data/raw/large_scale")
            all_quotes = []

            # åˆ©ç”¨å¯èƒ½ãªparquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            parquet_files = list(data_dir.glob("*.parquet"))

            for file_path in parquet_files:
                try:
                    logger.info(f"ğŸ“„ {file_path.name} ã‚’èª­ã¿è¾¼ã¿ä¸­...")

                    # Polarsã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                    df = pl.read_parquet(file_path)

                    # pandas DataFrameã«å¤‰æ›ã—ã¦è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                    pandas_df = df.to_pandas()

                    # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‡¦ç†
                    if "Date" in pandas_df.columns:
                        pandas_df["Date"] = pd.to_datetime(pandas_df["Date"])
                        pandas_df = pandas_df.rename(columns={"Date": "date"})

                    # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚«ãƒ©ãƒ ã®çµ±ä¸€ï¼ˆCode or code â†’ Codeï¼‰
                    if "code" in pandas_df.columns and "Code" not in pandas_df.columns:
                        pandas_df = pandas_df.rename(columns={"code": "Code"})

                    # ä¾¡æ ¼ã‚«ãƒ©ãƒ ã®çµ±ä¸€ï¼ˆClose/c_use â†’ Closeï¼‰
                    if "c_use" in pandas_df.columns and "Close" not in pandas_df.columns:
                        pandas_df = pandas_df.rename(columns={"c_use": "Close"})
                    elif "close" in pandas_df.columns and "Close" not in pandas_df.columns:
                        pandas_df = pandas_df.rename(columns={"close": "Close"})

                    # å¿…è¦ãªã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
                    if "Code" in pandas_df.columns and "date" in pandas_df.columns and "Close" in pandas_df.columns:
                        # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿æŠ½å‡º
                        pandas_df = pandas_df[["Code", "date", "Close"]]
                        quotes = pandas_df.to_dict('records')
                        all_quotes.extend(quotes)
                        logger.info(f"   âœ… {len(quotes)}ä»¶èª­ã¿è¾¼ã¿å®Œäº†")
                    else:
                        logger.warning(f"   âš ï¸ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚‰ãªã„: Code={('Code' in pandas_df.columns)}, date={('date' in pandas_df.columns)}, Close={('Close' in pandas_df.columns)}")
                        continue

                    logger.info(f"   ğŸ“Š ç¾åœ¨ã®ç´¯è¨ˆ: {len(all_quotes)}ä»¶")

                except Exception as e:
                    logger.warning(f"   âš ï¸ {file_path.name} ã®èª­ã¿è¾¼ã¿ã‚¹ã‚­ãƒƒãƒ—: {e}")
                    continue

            # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®é™¤å»ï¼ˆåŒã˜éŠ˜æŸ„ãƒ»æ—¥ä»˜ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ã‚½ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆï¼‰
            if all_quotes:
                logger.info("ğŸ”„ é‡è¤‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’é–‹å§‹")
                temp_df = pd.DataFrame(all_quotes)

                # ã¾ãšã€å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
                logger.info(f"ğŸ“Š é‡è¤‡å‡¦ç†å‰: {len(temp_df)}è¡Œ")

                # Codeã¨dateã§é‡è¤‡ã‚’é™¤å»ï¼ˆã‚ˆã‚Šè©³ç´°ãªæ¡ä»¶ã§ï¼‰
                if 'Code' in temp_df.columns and 'date' in temp_df.columns:
                    before_count = len(temp_df)
                    # åŒã˜éŠ˜æŸ„ãƒ»åŒã˜æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
                    temp_df = temp_df.sort_values(['Code', 'date'])
                    temp_df = temp_df.drop_duplicates(subset=['Code', 'date'], keep='first')
                    after_count = len(temp_df)

                    duplicates_removed = before_count - after_count
                    logger.info(f"ğŸ§¹ é‡è¤‡ãƒ‡ãƒ¼ã‚¿é™¤å»: {duplicates_removed}ä»¶")

                    # éŠ˜æŸ„æ•°ã®å¤‰åŒ–ã‚’ç¢ºèª
                    unique_stocks_before = before_count  # ã“ã‚Œã¯æ­£ç¢ºã§ã¯ãªã„ãŒå‚è€ƒå€¤
                    unique_stocks_after = temp_df['Code'].nunique()
                    logger.info(f"ğŸ“Š éŠ˜æŸ„æ•°: {unique_stocks_after}éŠ˜æŸ„")
                else:
                    logger.warning("âš ï¸ Codeã¾ãŸã¯dateã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚é‡è¤‡é™¤å»ã‚’ã‚¹ã‚­ãƒƒãƒ—")

                # è¾æ›¸ãƒªã‚¹ãƒˆã«æˆ»ã™
                all_quotes = temp_df.to_dict('records')

            logger.info(f"ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(all_quotes)}ä»¶")
            return all_quotes

        except Exception as e:
            logger.error(f"âŒ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _clean_stock_data(self, df):
        """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆç·©å’Œç‰ˆï¼‰"""
        logger.info("ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚’é–‹å§‹")
        original_count = len(df)

        # æ¬ æå€¤å‡¦ç†ï¼ˆå¿…é ˆé …ç›®ã®ã¿ï¼‰
        df = df.dropna(subset=['Close'])  # çµ‚å€¤ã¯å¿…é ˆ

        # ç•°å¸¸å€¤é™¤å»ï¼ˆä¾¡æ ¼ãŒ0ä»¥ä¸‹ã¯é™¤å¤–ï¼‰
        df = df[df['Close'] > 0]

        # OHLCãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ã‚‹å ´åˆã®ã¿ï¼‰
        if all(col in df.columns for col in ['Open', 'High', 'Low']):
            df = df[(df['Open'] > 0) & (df['High'] > 0) & (df['Low'] > 0)]

        # å‡ºæ¥é«˜ã®ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ã‚‹å ´åˆã®ã¿ï¼‰
        if 'Volume' in df.columns:
            # å‡ºæ¥é«˜ãŒ0ã®ãƒ‡ãƒ¼ã‚¿ã‚‚æ®‹ã™ï¼ˆå‡ºæ¥é«˜0ã§ã‚‚ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã¯æœ‰åŠ¹ãªå ´åˆãŒã‚ã‚‹ï¼‰
            pass

        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        df = df.sort_values(['Code', 'date']).reset_index(drop=True)

        cleaned_count = len(df)
        removed_count = original_count - cleaned_count

        logger.info(f"âœ… ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å®Œäº†: {cleaned_count}è¡Œ (é™¤å»: {removed_count}è¡Œ)")

        return df

    def _create_ml_features(self, df):
        """MLç”¨ç‰¹å¾´é‡ã®ä½œæˆ"""
        logger.info("ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’é–‹å§‹")

        try:
            import pandas as pd
        except ImportError:
            logger.error("âŒ pandasãŒimportã§ãã¾ã›ã‚“")
            return df

        # éŠ˜æŸ„ã”ã¨ã«ç‰¹å¾´é‡ã‚’ä½œæˆ
        df_list = []

        for code in df['Code'].unique():
            stock_df = df[df['Code'] == code].copy()

            if len(stock_df) < 2:  # æœ€ä½2æ—¥ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼ˆç·©å’Œï¼‰
                continue

            # ä¾¡æ ¼å¤‰å‹•ç‡
            stock_df['price_change'] = stock_df['Close'].pct_change()

            # ãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ãŸç‰¹å¾´é‡ä½œæˆ
            data_length = len(stock_df)

            # ç§»å‹•å¹³å‡ï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ã¦ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºèª¿æ•´ï¼‰
            if data_length >= 5:
                stock_df['ma5'] = stock_df['Close'].rolling(window=min(5, data_length)).mean()
            if data_length >= 10:
                stock_df['ma10'] = stock_df['Close'].rolling(window=min(10, data_length)).mean()
            if data_length >= 20:
                stock_df['ma20'] = stock_df['Close'].rolling(window=min(20, data_length)).mean()
            if data_length >= 60:
                stock_df['ma60'] = stock_df['Close'].rolling(window=min(60, data_length)).mean()

            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ã¦èª¿æ•´ï¼‰
            if data_length >= 5:
                vol_window = min(20, data_length)
                stock_df['volatility'] = stock_df['price_change'].rolling(window=vol_window).std()

            # RSIï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ã¦èª¿æ•´ï¼‰
            if data_length >= 14:
                rsi_window = min(14, data_length)
                delta = stock_df['Close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=rsi_window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=rsi_window).mean()
                rs = gain / loss
                stock_df['rsi'] = 100 - (100 / (1 + rs))

            # MACDï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ã¦èª¿æ•´ï¼‰
            if data_length >= 26:
                exp1 = stock_df['Close'].ewm(span=min(12, data_length//2), adjust=False).mean()
                exp2 = stock_df['Close'].ewm(span=min(26, data_length), adjust=False).mean()
                stock_df['macd'] = exp1 - exp2
                if data_length >= 35:  # ã‚·ã‚°ãƒŠãƒ«ãƒ©ã‚¤ãƒ³ç”¨
                    stock_df['macd_signal'] = stock_df['macd'].ewm(span=min(9, data_length//4), adjust=False).mean()

            # ç›®çš„å¤‰æ•°: ç¿Œæ—¥ã®ä¾¡æ ¼å¤‰å‹•ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒ2æ—¥ä»¥ä¸Šã‚ã‚‹å ´åˆã®ã¿ï¼‰
            if data_length >= 2:
                stock_df['target'] = stock_df['Close'].shift(-1) / stock_df['Close'] - 1

            df_list.append(stock_df)

        if df_list:
            result_df = pd.concat(df_list, ignore_index=True)

            # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã‚’å‹•çš„ã«å–å¾—
            potential_features = ['price_change', 'ma5', 'ma10', 'ma20', 'ma60',
                                'volatility', 'rsi', 'macd', 'macd_signal']
            available_features = [col for col in potential_features if col in result_df.columns]

            # ç‰¹å¾´é‡ã®æ¬ æå€¤ã‚’åŸ‹ã‚ã‚‹ï¼ˆå‰å€¤è£œå®Œï¼‰
            if available_features:
                result_df[available_features] = result_df[available_features].fillna(method='ffill')

            # targetãŒNaNã®è¡Œã‚’é™¤å¤–
            if 'target' in result_df.columns:
                result_df = result_df.dropna(subset=['target'])

            logger.info(f"âœ… ç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(result_df)}è¡Œ")
            logger.info(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: {available_features}")

            return result_df
        else:
            logger.warning("âš ï¸ ç‰¹å¾´é‡ä½œæˆå¯¾è±¡ã®éŠ˜æŸ„ãŒã‚ã‚Šã¾ã›ã‚“")
            return df

    async def run_direct_api_dataset_builder(self):
        """ç›´æ¥APIãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting Direct API Dataset Builder...")
        
        builder = DirectAPIDatasetBuilder()
        result = await builder.build_direct_api_dataset()
        
        if result:
            logger.info("âœ… Direct API Dataset Builder completed successfully!")
            logger.info(f"ğŸ“Š Dataset: {len(result['df'])} rows, {result['df']['Code'].n_unique()} stocks")
            return True, result
        else:
            logger.error("âŒ Direct API Dataset Builder failed")
            return False, {}

    async def run_complete_atft_training(self):
        """å®Œå…¨ATFTå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting Complete ATFT Training Pipeline...")
        
        pipeline = CompleteATFTTrainingPipeline()
        success, result = await pipeline.run_complete_training_pipeline()
        
        if success:
            logger.info("âœ… Complete ATFT Training Pipeline completed successfully!")
            logger.info(f"ğŸ¯ Target Sharpe: 0.849")
            return True, result
        else:
            logger.error(f"âŒ Complete ATFT Training Pipeline failed: {result.get('error')}")
            return False, result

    def run_workflow(self, workflow: str, mode: str = "full", stocks: int = 500, date: str = None, start_date: str = None, end_date: str = None, years: int = 5, max_days: int = None, exclude_market_codes: List[str] = None, use_existing_data: bool = True):
        """æŒ‡å®šã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ"""
        logger.info(f"ğŸ¬ Starting workflow: {workflow}")

        try:
            if workflow == "safe-training":
                return self.run_safe_training(mode)
            elif workflow == "ml-dataset":
                return self.run_ml_dataset_builder()
            elif workflow == "direct-api-dataset":
                return asyncio.run(self.run_direct_api_dataset_builder())
            elif workflow == "complete-atft":
                return asyncio.run(self.run_complete_atft_training())
            elif workflow == "expand-dataset":
                return self.run_expand_dataset(max_stocks=stocks)
            elif workflow == "expand-dataset-by-date":
                if date is None:
                    logger.error("âŒ date parameter is required for expand-dataset-by-date")
                    return False, {"error": "date parameter is required"}
                return self.run_expand_dataset_by_date(date, exclude_market_codes=exclude_market_codes)
            elif workflow == "expand-dataset-by-range":
                if start_date is None or end_date is None:
                    logger.error("âŒ start_date and end_date parameters are required for expand-dataset-by-range")
                    return False, {"error": "start_date and end_date parameters are required"}
                return self.run_expand_dataset_by_range(start_date, end_date)
            elif workflow == "expand-historical-all-stocks":
                return self.run_expand_historical_all_stocks(years=years, max_days=max_days, exclude_market_codes=exclude_market_codes)
            elif workflow == "create-ml-dataset":
                return self.create_ml_dataset(years=years, exclude_market_codes=exclude_market_codes, use_existing_data=use_existing_data)
            else:
                logger.error(f"âŒ Unknown workflow: {workflow}")
                return False, {"error": f"Unknown workflow: {workflow}"}

        except Exception as e:
            logger.error(f"âŒ Workflow {workflow} failed with exception: {e}")
            import traceback
            traceback.print_exc()
            return False, {"error": str(e)}


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="gogooku3-standalone - å£Šã‚Œãšãƒ»å¼·ããƒ»é€Ÿã ã‚’å®Ÿç¾ã™ã‚‹é‡‘èML ã‚·ã‚¹ãƒ†ãƒ ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
åˆ©ç”¨å¯èƒ½ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:
  safe-training               å®‰å…¨å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆæ¨å¥¨ï¼‰
  ml-dataset                  MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
  direct-api-dataset          ç›´æ¥APIãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
  complete-atft               å®Œå…¨ATFTå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
  expand-dataset              J-Quants APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‹¡å¼µï¼ˆéŠ˜æŸ„ãƒ™ãƒ¼ã‚¹ï¼‰
  expand-dataset-by-date      J-Quants APIã‹ã‚‰æ—¥ä»˜ãƒ™ãƒ¼ã‚¹å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
  expand-dataset-by-range     J-Quants APIã‹ã‚‰æœŸé–“ãƒ™ãƒ¼ã‚¹å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
  expand-historical-all-stocks J-Quantså–å¼•ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼APIã‚’ä½¿ã£ã¦éå»Nå¹´åˆ†ã®å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
  create-ml-dataset           éå»å–ã‚Œã‚‹å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦MLç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ

ä½¿ç”¨ä¾‹:
  python main.py safe-training --mode full
  python main.py ml-dataset
  python main.py direct-api-dataset
  python main.py complete-atft
  python main.py expand-dataset --stocks 1000
  python main.py expand-dataset-by-date --date 2024-08-29
  python main.py expand-dataset-by-range --start-date 2024-08-01 --end-date 2024-08-29
  python main.py expand-historical-all-stocks --years 5 --max-days 100
  python main.py expand-historical-all-stocks --years 3 --exclude-market-codes 0105 0109 0110
  python main.py create-ml-dataset --years 5 --exclude-market-codes 0105 0109
  python main.py create-ml-dataset --years 2 --use-existing-data --exclude-market-codes 0105 0109
  python main.py create-ml-dataset --years 1 --no-existing-data
        """
    )
    
    parser.add_argument(
        "workflow",
        choices=["safe-training", "ml-dataset", "direct-api-dataset", "complete-atft", "expand-dataset", "expand-dataset-by-date", "expand-dataset-by-range", "expand-historical-all-stocks", "create-ml-dataset"],
        help="å®Ÿè¡Œã™ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"
    )

    parser.add_argument(
        "--mode",
        choices=["quick", "full"],
        default="full",
        help="å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ï¼ˆsafe-trainingã®ã¿ï¼šquick=1ã‚¨ãƒãƒƒã‚¯, full=å®Œå…¨å­¦ç¿’ï¼‰"
    )

    parser.add_argument(
        "--stocks",
        type=int,
        default=500,
        help="expand-datasetæ™‚ã®å–å¾—éŠ˜æŸ„æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 500ï¼‰"
    )

    parser.add_argument(
        "--date",
        type=str,
        help="expand-dataset-by-dateæ™‚ã®å¯¾è±¡æ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰"
    )

    parser.add_argument(
        "--start-date",
        type=str,
        help="expand-dataset-by-rangeæ™‚ã®é–‹å§‹æ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰"
    )

    parser.add_argument(
        "--end-date",
        type=str,
        help="expand-dataset-by-rangeæ™‚ã®çµ‚äº†æ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰"
    )

    parser.add_argument(
        "--years",
        type=int,
        default=5,
        help="expand-historical-all-stocksæ™‚ã®éå»å¹´æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰"
    )

    parser.add_argument(
        "--max-days",
        type=int,
        help="expand-historical-all-stocksæ™‚ã®æœ€å¤§å–å¾—æ—¥æ•°ï¼ˆåˆ¶é™ãªã—ã®å ´åˆã¯æŒ‡å®šãªã—ï¼‰"
    )

    parser.add_argument(
        "--exclude-market-codes",
        type=str,
        nargs='*',
        default=['0105', '0109'],
        help="é™¤å¤–ã™ã‚‹MarketCodeã®ãƒªã‚¹ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0105 0109ï¼‰"
    )

    parser.add_argument(
        "--use-existing-data",
        action='store_true',
        default=True,
        help="æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰"
    )

    parser.add_argument(
        "--no-existing-data",
        action='store_false',
        dest='use_existing_data',
        help="æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ç„¡è¦–ã—ã¦æ–°è¦å–å¾—ã®ã¿ã‚’è¡Œã†"
    )
    
    args = parser.parse_args()
    
    # ãƒãƒŠãƒ¼è¡¨ç¤º
    print("=" * 80)
    print("ğŸš€ gogooku3-standalone - å£Šã‚Œãšãƒ»å¼·ããƒ»é€Ÿã")
    print("ğŸ“ˆ é‡‘èML ã‚·ã‚¹ãƒ†ãƒ çµ±åˆå®Ÿè¡Œç’°å¢ƒ")
    print("=" * 80)
    print(f"Workflow: {args.workflow}")
    print(f"Mode: {args.mode}")
    print("=" * 80)
    
    # ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒŠãƒ¼å®Ÿè¡Œ
    runner = GoGooKu3MainRunner()
    
    # åŒæœŸå®Ÿè¡Œ
    success, result = runner.run_workflow(
        args.workflow,
        args.mode,
        args.stocks,
        args.date,
        args.start_date,
        args.end_date,
        args.years,
        args.max_days,
        args.exclude_market_codes,
        args.use_existing_data
    )
    
    # çµæœè¡¨ç¤º
    print("\n" + "=" * 80)
    if success:
        print("ğŸ‰ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡ŒæˆåŠŸ!")
        print("=" * 80)
        
        # çµæœã‚µãƒãƒªãƒ¼
        if args.workflow == "safe-training":
            summary = result.get("summary", {})
            print(f"ğŸ“Š å­¦ç¿’çµæœ:")
            print(f"   - ã‚¨ãƒãƒƒã‚¯æ•°: {summary.get('epochs', 'N/A')}")
            print(f"   - æœ€çµ‚æå¤±: {summary.get('final_loss', 'N/A')}")
            print(f"   - å®Ÿè¡Œæ™‚é–“: {summary.get('elapsed_time', 'N/A'):.2f}ç§’")
        elif args.workflow in ["ml-dataset", "direct-api-dataset"]:
            if "df" in result:
                print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰çµæœ:")
                print(f"   - è¡Œæ•°: {len(result['df']):,}")
                print(f"   - éŠ˜æŸ„æ•°: {result['df']['Code'].n_unique()}")
                if "metadata" in result:
                    print(f"   - ç‰¹å¾´é‡æ•°: {result['metadata']['features']['count']}")
        elif args.workflow == "complete-atft":
            validation_info = result.get("validation_info", {})
            print(f"ğŸ¯ ATFTå­¦ç¿’çµæœ:")
            print(f"   - ç›®æ¨™Sharpe: 0.849")
            if validation_info.get('sharpe_ratio') is not None:
                print(f"   - é”æˆSharpe: {validation_info.get('sharpe_ratio')}")
            print(f"   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {validation_info.get('param_count', 'N/A'):,}")
        elif args.workflow == "create-ml-dataset":
            print(f"ğŸ¤– MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆçµæœ:")
            print(f"   - å…ƒãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {result.get('raw_data_records', 'N/A'):,}")
            print(f"   - å¯¾è±¡å¹´æ•°: {result.get('years', 'N/A')}å¹´")
            print(f"   - é™¤å¤–å¸‚å ´: {', '.join(result.get('excluded_markets', []))}")
            print(f"   - æ—¢å­˜ãƒ‡ãƒ¼ã‚¿æ´»ç”¨: {result.get('used_existing_data', 'N/A')}")
            if "ml_dataset_path" in result:
                print(f"   - ä¿å­˜å…ˆ: {result['ml_dataset_path']}")

        print("=" * 80)
        print("âœ… å®Ÿè¡Œå®Œäº†")
        
    else:
        print("âŒ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œå¤±æ•—")
        print(f"ã‚¨ãƒ©ãƒ¼: {result.get('error', 'Unknown error')}")
        print("=" * 80)
        
        sys.exit(1)


if __name__ == "__main__":
    main()
