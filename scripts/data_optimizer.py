#!/usr/bin/env python3
"""
Data Management Best Practices for gogooku3
ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å®Ÿè£…
"""

import sys
import logging
import hashlib
from pathlib import Path
from typing import Dict, List
from functools import lru_cache
import polars as pl
from datetime import datetime, timedelta
import json

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DataOptimizer:
    """ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å®Ÿè£…ã‚¯ãƒ©ã‚¹"""

    def __init__(self, base_dir: str = "output"):
        self.base_dir = Path(base_dir)
        self.cache_dir = self.base_dir / "cache"
        self.compressed_dir = self.base_dir / "compressed"
        self.metadata_dir = self.base_dir / "metadata"

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        for dir_path in [self.cache_dir, self.compressed_dir, self.metadata_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

        # æœ€é©åŒ–è¨­å®š
        self.parquet_settings = {
            "compression": "gzip",  # snappy â†’ gzipï¼ˆåœ§ç¸®ç‡å‘ä¸Šï¼‰
            "row_group_size": 100000,
            "use_dictionary": True,
            "use_byte_stream_split": True,
        }

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.cache_settings = {"max_size": 100, "ttl_hours": 24}

    def optimize_parquet_files(self, input_dir: str, output_dir: str = None) -> Dict:
        """parquetãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€é©åŒ–"""
        if output_dir is None:
            output_dir = self.compressed_dir

        input_path = Path(input_dir)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        optimization_results = {
            "original_size": 0,
            "optimized_size": 0,
            "compression_ratio": 0,
            "files_processed": 0,
        }

        try:
            # å…¨parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            parquet_files = list(input_path.rglob("*.parquet"))

            for file_path in parquet_files:
                relative_path = file_path.relative_to(input_path)
                output_file = output_path / relative_path
                output_file.parent.mkdir(parents=True, exist_ok=True)

                # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                df = pl.read_parquet(file_path)
                original_size = file_path.stat().st_size

                # æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šã§ä¿å­˜
                df.write_parquet(
                    output_file, compression=self.parquet_settings["compression"]
                )

                optimized_size = output_file.stat().st_size
                compression_ratio = (original_size - optimized_size) / original_size

                optimization_results["original_size"] += original_size
                optimization_results["optimized_size"] += optimized_size
                optimization_results["files_processed"] += 1

                logger.info(
                    f"Optimized: {file_path.name} "
                    f"({original_size/1024:.1f}KB â†’ {optimized_size/1024:.1f}KB, "
                    f"compression: {compression_ratio:.1%})"
                )

            optimization_results["compression_ratio"] = (
                optimization_results["original_size"]
                - optimization_results["optimized_size"]
            ) / optimization_results["original_size"]

            logger.info(
                f"Optimization completed: {optimization_results['files_processed']} files, "
                f"compression ratio: {optimization_results['compression_ratio']:.1%}"
            )

            return optimization_results

        except Exception as e:
            logger.error(f"Optimization failed: {e}")
            raise

    @lru_cache(maxsize=100)
    def load_stock_data(self, stock_code: str, date_range: str = None) -> pl.DataFrame:
        """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ããƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼"""
        cache_key = f"{stock_code}_{date_range or 'all'}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        cache_file = (
            self.cache_dir / f"{hashlib.md5(cache_key.encode()).hexdigest()}.parquet"
        )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã—ã€æœ‰åŠ¹æœŸé™å†…ã‹ãƒã‚§ãƒƒã‚¯
        if cache_file.exists():
            cache_age = datetime.now() - datetime.fromtimestamp(
                cache_file.stat().st_mtime
            )
            if cache_age < timedelta(hours=self.cache_settings["ttl_hours"]):
                logger.info(f"Loading from cache: {stock_code}")
                return pl.read_parquet(cache_file)

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        data_file = self.base_dir / "atft_data" / "train" / f"{stock_code}.parquet"
        if not data_file.exists():
            raise FileNotFoundError(f"Data file not found: {data_file}")

        df = pl.read_parquet(data_file)

        # æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if date_range:
            start_date, end_date = date_range.split("_")
            df = df.filter(
                (pl.col("date") >= start_date) & (pl.col("date") <= end_date)
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        df.write_parquet(cache_file)
        logger.info(f"Cached data: {stock_code}")

        return df

    def batch_load_data(
        self, stock_codes: List[str], batch_size: int = 10
    ) -> List[pl.DataFrame]:
        """ãƒãƒƒãƒãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        results = []

        for i in range(0, len(stock_codes), batch_size):
            batch = stock_codes[i : i + batch_size]
            logger.info(f"Loading batch {i//batch_size + 1}: {len(batch)} stocks")

            batch_data = []
            for stock_code in batch:
                try:
                    df = self.load_stock_data(stock_code)
                    batch_data.append(df)
                except Exception as e:
                    logger.warning(f"Failed to load {stock_code}: {e}")
                    continue

            results.extend(batch_data)

        return results

    def create_data_metadata(self, data_dir: str) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
        data_path = Path(data_dir)
        metadata = {
            "created_at": datetime.now().isoformat(),
            "data_structure": {},
            "file_stats": {},
            "quality_metrics": {},
        }

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®åˆ†æ
        for subdir in ["train", "val", "test"]:
            subdir_path = data_path / subdir
            if subdir_path.exists():
                files = list(subdir_path.glob("*.parquet"))
                metadata["data_structure"][subdir] = {
                    "file_count": len(files),
                    "total_size": sum(f.stat().st_size for f in files),
                    "file_names": [f.name for f in files],
                }

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å“è³ªãƒã‚§ãƒƒã‚¯
        sample_file = next(data_path.rglob("*.parquet"), None)
        if sample_file:
            df = pl.read_parquet(sample_file)
            metadata["quality_metrics"] = {
                "columns": df.columns,
                "shape": df.shape,
                "null_counts": df.null_count().to_dict(),
                "data_types": df.schema,
            }

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata_file = (
            self.metadata_dir
            / f"data_metadata_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(metadata_file, "w") as f:
            json.dump(metadata, f, indent=2, default=str)

        logger.info(f"Metadata created: {metadata_file}")
        return metadata

    def cleanup_cache(self, max_age_hours: int = 24) -> int:
        """å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤"""
        cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
        deleted_count = 0

        for cache_file in self.cache_dir.glob("*.parquet"):
            if datetime.fromtimestamp(cache_file.stat().st_mtime) < cutoff_time:
                cache_file.unlink()
                deleted_count += 1

        logger.info(f"Cleaned up {deleted_count} old cache files")
        return deleted_count


class DataValidator:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.validation_rules = {
            "required_columns": ["code", "date", "close", "volume"],
            "date_format": "%Y-%m-%d",
            "min_records_per_stock": 20,
            "max_null_ratio": 0.1,
        }

    def validate_dataset(self, df: pl.DataFrame) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å“è³ªæ¤œè¨¼"""
        validation_results = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "metrics": {},
        }

        # å¿…é ˆåˆ—ã®ãƒã‚§ãƒƒã‚¯
        missing_columns = set(self.validation_rules["required_columns"]) - set(
            df.columns
        )
        if missing_columns:
            validation_results["valid"] = False
            validation_results["errors"].append(
                f"Missing required columns: {missing_columns}"
            )

        # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        if "date" in df.columns:
            try:
                df.select(
                    pl.col("date").str.strptime(
                        pl.Datetime, self.validation_rules["date_format"]
                    )
                )
            except Exception as e:
                validation_results["warnings"].append(f"Date format issues: {e}")

        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        null_counts = df.null_count()
        for col, null_count in null_counts.items():
            null_ratio = null_count / len(df)
            if null_ratio > self.validation_rules["max_null_ratio"]:
                validation_results["warnings"].append(
                    f"High null ratio in {col}: {null_ratio:.1%}"
                )

        # éŠ˜æŸ„ã”ã¨ã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ãƒã‚§ãƒƒã‚¯
        if "code" in df.columns:
            stock_counts = df.groupby("code").count()
            low_record_stocks = stock_counts.filter(
                pl.col("count") < self.validation_rules["min_records_per_stock"]
            )
            if len(low_record_stocks) > 0:
                validation_results["warnings"].append(
                    f"Stocks with few records: {len(low_record_stocks)}"
                )

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        validation_results["metrics"] = {
            "total_records": len(df),
            "total_stocks": df["code"].n_unique() if "code" in df.columns else 0,
            "date_range": {
                "start": df["date"].min() if "date" in df.columns else None,
                "end": df["date"].max() if "date" in df.columns else None,
            },
            "null_ratios": null_counts.to_dict(),
        }

        return validation_results


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    optimizer = DataOptimizer()
    validator = DataValidator()  # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ç”¨

    # ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–
    print("ğŸ”§ Optimizing parquet files...")
    results = optimizer.optimize_parquet_files("output/atft_data")
    print(f"Optimization results: {results}")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    print("ğŸ“Š Creating metadata...")
    metadata = optimizer.create_data_metadata("output/atft_data")
    print(f"Metadata created: {metadata['created_at']}")

    # ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼
    print("ğŸ” Validating data quality...")
    sample_file = next(Path("output/atft_data").rglob("*.parquet"), None)
    if sample_file:
        df = pl.read_parquet(sample_file)
        validation_results = validator.validate_dataset(df)
        print(f"Data validation results: {validation_results}")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    print("ğŸ§¹ Cleaning up cache...")
    deleted_count = optimizer.cleanup_cache()
    print(f"Deleted {deleted_count} old cache files")


if __name__ == "__main__":
    main()
