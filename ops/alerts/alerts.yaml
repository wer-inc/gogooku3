# ===========================================
# gogooku3-standalone Alert Configuration
# ===========================================
# Prometheus Alert Manager configuration for application monitoring
# Copy this to your Prometheus alertmanager configuration directory

groups:
  - name: gogooku3.application
    rules:
      # Application availability alerts
      - alert: Gogooku3Down
        expr: up{job="gogooku3"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "gogooku3 application is down"
          description: "gogooku3 has been down for more than 5 minutes."

      # System resource alerts
      - alert: HighMemoryUsage
        expr: gogooku3_memory_usage_percent > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on gogooku3"
          description: "Memory usage is {{ $value }}% (threshold: 90%)"

      - alert: CriticalMemoryUsage
        expr: gogooku3_memory_usage_percent > 95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical memory usage on gogooku3"
          description: "Memory usage is {{ $value }}% (threshold: 95%)"

      - alert: HighCpuUsage
        expr: gogooku3_cpu_usage_percent > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on gogooku3"
          description: "CPU usage is {{ $value }}% (threshold: 90%)"

      - alert: CriticalCpuUsage
        expr: gogooku3_cpu_usage_percent > 95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical CPU usage on gogooku3"
          description: "CPU usage is {{ $value }}% (threshold: 95%)"

      - alert: HighDiskUsage
        expr: gogooku3_disk_usage_percent > 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage on gogooku3"
          description: "Disk usage is {{ $value }}% (threshold: 90%)"

      - alert: CriticalDiskUsage
        expr: gogooku3_disk_usage_percent > 95
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk usage on gogooku3"
          description: "Disk usage is {{ $value }}% (threshold: 95%)"

      # Process-level alerts
      - alert: ProcessHighMemory
        expr: gogooku3_process_memory_bytes > 8 * 1024 * 1024 * 1024  # 8GB
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "gogooku3 process using high memory"
          description: "Process memory usage is {{ $value | humanizeBytes }} (threshold: 8GB)"

      - alert: ProcessHighCpu
        expr: gogooku3_process_cpu_percent > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "gogooku3 process using high CPU"
          description: "Process CPU usage is {{ $value }}% (threshold: 80%)"

      # Data and log alerts
      - alert: LargeLogFiles
        expr: gogooku3_log_files_size_bytes > 1024 * 1024 * 1024  # 1GB
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "Large log files detected"
          description: "Log files size is {{ $value | humanizeBytes }} (threshold: 1GB)"

      - alert: ManyDataFiles
        expr: gogooku3_data_files_count > 10000
        for: 15m
        labels:
          severity: info
        annotations:
          summary: "High number of data files"
          description: "Data files count is {{ $value }} (threshold: 10,000)"

      # Network alerts
      - alert: HighNetworkTraffic
        expr: rate(gogooku3_network_bytes_sent[5m]) + rate(gogooku3_network_bytes_recv[5m]) > 100 * 1024 * 1024  # 100MB/s
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High network traffic"
          description: "Network traffic is {{ $value | humanizeBytes }}/s (threshold: 100MB/s)"

  - name: gogooku3.business
    rules:
      # Business logic alerts (customize based on your ML pipeline)
      - alert: SlowTrainingPipeline
        expr: histogram_quantile(0.95, rate(gogooku3_training_duration_seconds_bucket[10m])) > 3600  # 1 hour
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow ML training pipeline"
          description: "95th percentile training duration is {{ $value }}s (threshold: 3600s)"

      - alert: TrainingFailures
        expr: increase(gogooku3_training_failures_total[1h]) > 3
        for: 5m
        labels:
          severity: error
        annotations:
          summary: "Multiple training failures"
          description: "{{ $value }} training failures in the last hour (threshold: 3)"

      - alert: DataQualityIssues
        expr: gogooku3_data_quality_score < 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Data quality issues detected"
          description: "Data quality score is {{ $value }} (threshold: 0.8)"

# ===========================================
# Alert Manager Configuration Template
# ===========================================
# This should be used with alertmanager.yml

# alertmanager.yml template:
# global:
#   smtp_smarthost: 'smtp.example.com:587'
#   smtp_from: 'alerts@example.com'
#   smtp_auth_username: 'alerts@example.com'
#   smtp_auth_password: 'your_password'
#
# route:
#   group_by: ['alertname']
#   group_wait: 10s
#   group_interval: 10s
#   repeat_interval: 1h
#   receiver: 'email'
#   routes:
#   - match:
#       severity: critical
#     receiver: 'pagerduty'
#
# receivers:
# - name: 'email'
#   email_configs:
#   - to: 'team@example.com'
#     send_resolved: true
#
# - name: 'pagerduty'
#   pagerduty_configs:
#   - service_key: 'your_pagerduty_integration_key'

# ===========================================
# Slack Integration Example
# ===========================================
# receivers:
# - name: 'slack'
#   slack_configs:
#   - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
#     channel: '#alerts'
#     send_resolved: true
#     title: '{{ template "slack.title" . }}'
#     text: '{{ template "slack.text" . }}'
#
# templates:
# - /etc/alertmanager/templates/slack.tmpl

# ===========================================
# Notification Templates
# ===========================================

# slack.tmpl example:
# {{ define "slack.title" }}
# [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}
# {{ end }}
#
# {{ define "slack.text" }}
# {{ range .Alerts }}
# *Alert:* {{ .Annotations.summary }}
# *Description:* {{ .Annotations.description }}
# *Severity:* {{ .Labels.severity }}
# *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05 UTC" }}
# {{ end }}
# {{ end }}
