#!/usr/bin/env python3
"""
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ£šå¸ã—ãƒ„ãƒ¼ãƒ« - Gogooku3ãƒªãƒã‚¸ãƒˆãƒªã®å…¨.mdãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ
"""

import os
import re
import csv
import hashlib
from pathlib import Path
from datetime import datetime
import subprocess
import argparse


def get_file_hash(filepath):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®SHA256ãƒãƒƒã‚·ãƒ¥ã‚’å–å¾—"""
    try:
        with open(filepath, 'rb') as f:
            return hashlib.sha256(f.read()).hexdigest()
    except:
        return "error"


def extract_title(content):
    """Markdownå†…å®¹ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«(h1)ã‚’æŠ½å‡º"""
    lines = content.split('\n')
    for line in lines[:20]:  # æœ€åˆã®20è¡Œã‹ã‚‰æ¢ç´¢
        line = line.strip()
        if line.startswith('# '):
            return line[2:].strip()
    return "no_title"


def detect_language(content, filepath):
    """è¨€èªã‚’åˆ¤å®šï¼ˆæ—¥æœ¬èª/è‹±èª/ãã®ä»–ï¼‰"""
    # ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®åˆ¤å®š
    if '/ja/' in str(filepath) or '_ja.' in str(filepath):
        return 'ja'
    if '/en/' in str(filepath) or '_en.' in str(filepath):
        return 'en'
    
    # å†…å®¹ã§ã®åˆ¤å®šï¼ˆç°¡æ˜“ï¼‰
    japanese_chars = len(re.findall(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]', content))
    english_words = len(re.findall(r'\b[a-zA-Z]+\b', content))
    
    if japanese_chars > english_words * 0.1:
        return 'ja'
    elif english_words > 50:
        return 'en'
    else:
        return 'other'


def classify_doc_type(filepath, content):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
    path_str = str(filepath).lower()
    title = extract_title(content).lower()
    
    # ãƒ‘ã‚¹ãƒ™ãƒ¼ã‚¹åˆ¤å®š
    if 'readme' in path_str:
        return 'guide'
    if 'specification' in path_str or 'spec' in path_str:
        return 'spec'
    if 'migration' in path_str or 'changelog' in path_str:
        return 'changelog'
    if 'adr' in path_str or 'rfc' in path_str:
        return 'adr'
    if 'api' in path_str:
        return 'api'
    if 'runbook' in path_str or 'operation' in path_str:
        return 'runbook'
    
    # å†…å®¹ãƒ™ãƒ¼ã‚¹åˆ¤å®š
    if any(word in title for word in ['guide', 'tutorial', 'getting started']):
        return 'guide'
    if any(word in title for word in ['specification', 'design', 'architecture']):
        return 'spec'
    if any(word in title for word in ['api', 'reference']):
        return 'api'
    if any(word in title for word in ['decision', 'adr']):
        return 'adr'
    if any(word in title for word in ['report', 'status', 'plan']):
        return 'memo'
    
    return 'memo'


def count_links(content):
    """ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯ãƒ»ã‚¢ã‚¦ãƒˆãƒã‚¦ãƒ³ãƒ‰ãƒªãƒ³ã‚¯ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç°¡æ˜“ï¼‰"""
    # Markdownãƒªãƒ³ã‚¯ [text](url) ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    outbound = len(re.findall(r'\[.*?\]\(.*?\)', content))
    
    # ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯ã¯åˆ¥é€”è§£æãŒå¿…è¦ï¼ˆã“ã“ã§ã¯0ï¼‰
    backlinks = 0
    
    return backlinks, outbound


def get_last_commit_date(filepath):
    """æœ€çµ‚ã‚³ãƒŸãƒƒãƒˆæ—¥ã‚’å–å¾—"""
    try:
        result = subprocess.run(
            ['git', 'log', '-1', '--format=%ci', str(filepath)],
            capture_output=True, text=True, cwd=filepath.parent
        )
        if result.returncode == 0:
            return result.stdout.strip().split()[0]  # YYYY-MM-DDéƒ¨åˆ†
    except:
        pass
    return "unknown"


def analyze_markdown_files(repo_root):
    """ãƒªãƒã‚¸ãƒˆãƒªå†…ã®å…¨.mdãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ"""
    repo_path = Path(repo_root)
    md_files = list(repo_path.glob('**/*.md'))
    
    results = []
    
    for filepath in md_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
        except:
            content = ""
        
        # çµ±è¨ˆæƒ…å ±åé›†
        size = filepath.stat().st_size if filepath.exists() else 0
        hash_value = get_file_hash(filepath)
        title = extract_title(content)
        language = detect_language(content, filepath)
        doc_type = classify_doc_type(filepath, content)
        last_commit = get_last_commit_date(filepath)
        backlinks, outbound_links = count_links(content)
        
        # ç›¸å¯¾ãƒ‘ã‚¹å–å¾—
        try:
            rel_path = filepath.relative_to(repo_path)
        except:
            rel_path = filepath
        
        results.append({
            'path': str(rel_path),
            'size': size,
            'sha256': hash_value[:16],  # çŸ­ç¸®ç‰ˆ
            'title': title,
            'lang': language,
            'type': doc_type,
            'last_commit_date': last_commit,
            'backlinks_count': backlinks,
            'outbound_links_count': outbound_links
        })
    
    return results


def save_to_csv(results, output_path):
    """çµæœã‚’CSVã«ä¿å­˜"""
    fieldnames = [
        'path', 'size', 'sha256', 'title', 'lang', 'type',
        'last_commit_date', 'backlinks_count', 'outbound_links_count'
    ]
    
    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)
    
    print(f"ğŸ“Š è§£æçµæœã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print(f"ğŸ“ åˆè¨ˆ {len(results)} å€‹ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†")


def main():
    parser = argparse.ArgumentParser(description='Markdownæ–‡æ›¸æ£šå¸ã—ãƒ„ãƒ¼ãƒ«')
    parser.add_argument('--repo', default='.', help='ãƒªãƒã‚¸ãƒˆãƒªãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹')
    parser.add_argument('--out', default='docs_inventory.csv', help='å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«')
    
    args = parser.parse_args()
    
    print(f"ğŸ” {args.repo} é…ä¸‹ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æä¸­...")
    results = analyze_markdown_files(args.repo)
    save_to_csv(results, args.out)
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\nğŸ“‹ ã‚µãƒãƒªãƒ¼:")
    print(f"   ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(results)}")
    print(f"   è¨€èªåˆ†å¸ƒ: {dict(sorted([(lang, sum(1 for r in results if r['lang'] == lang)) for lang in set(r['lang'] for r in results)]))}")
    print(f"   ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ: {dict(sorted([(doc_type, sum(1 for r in results if r['type'] == doc_type)) for doc_type in set(r['type'] for r in results)]))}")


if __name__ == '__main__':
    main()