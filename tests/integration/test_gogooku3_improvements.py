#!/usr/bin/env python3
"""
Gogooku3 Improvements Integration Test
gogooku3æ”¹å–„ç‚¹ã®çµ±åˆãƒ†ã‚¹ãƒˆ

ãƒ†ã‚¹ãƒˆå¯¾è±¡:
- Cross-sectional Z-scoreæ­£è¦åŒ–ï¼ˆPolarsãƒ™ãƒ¼ã‚¹ï¼‰
- Walk-Forward + Embargo=20
- Parquet lazy scan + åˆ—å°„å½±
- ç³»åˆ—ç›¸é–¢ã‚°ãƒ©ãƒ•æ§‹ç¯‰
- GBMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
- é«˜å“è³ªç‰¹å¾´é‡ç”Ÿæˆ
"""

import sys
import os
from pathlib import Path
import pandas as pd
import numpy as np
import polars as pl
import torch
import logging
from datetime import datetime, timedelta, date
from typing import Dict, List, Any
import tempfile
import shutil

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# æ”¹å–„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from src.data.safety.cross_sectional_v2 import CrossSectionalNormalizerV2
    from src.data.safety.walk_forward_v2 import WalkForwardSplitterV2
    from src.data.loaders.production_loader_v3 import ProductionDatasetV3
    from src.data.utils.graph_builder import FinancialGraphBuilder
    from src.models.baseline.lightgbm_baseline import LightGBMFinancialBaseline
    from src.features.quality_features import QualityFinancialFeaturesGenerator
    COMPONENTS_AVAILABLE = True
    print("âœ… All improvement components loaded successfully")
except ImportError as e:
    COMPONENTS_AVAILABLE = False
    print(f"âŒ Component loading failed: {e}")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_synthetic_financial_data(n_stocks=50, n_days=300) -> pd.DataFrame:
    """åˆæˆé‡‘èãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    dates = pd.date_range(end=pd.Timestamp.now(), periods=n_days, freq='D')
    codes = [f"STOCK_{i:04d}" for i in range(n_stocks)]
    
    data = []
    
    # å„éŠ˜æŸ„ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯
    for code in codes:
        price = 1000.0  # åˆæœŸä¾¡æ ¼
        volume_base = np.random.uniform(100000, 1000000)
        
        for date in dates:
            # ä¾¡æ ¼ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯
            daily_return = np.random.normal(0.0001, 0.02)  # å¹³å‡0.01%ã€æ¨™æº–åå·®2%
            price *= (1 + daily_return)
            
            # å‡ºæ¥é«˜ï¼ˆä¾¡æ ¼å¤‰å‹•ã¨ç›¸é–¢ï¼‰
            volume = volume_base * (1 + np.abs(daily_return) * 5 + np.random.normal(0, 0.3))
            volume = max(volume, 1000)  # æœ€å°å‡ºæ¥é«˜
            
            # OHLC
            high = price * (1 + np.abs(np.random.normal(0, 0.005)))
            low = price * (1 - np.abs(np.random.normal(0, 0.005)))
            open_price = price * (1 + np.random.normal(0, 0.003))
            
            # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            rsi = np.random.uniform(20, 80)
            ema_20 = price * (1 + np.random.normal(0, 0.01))
            volatility = np.abs(np.random.normal(0.02, 0.01))
            
            # ãƒªã‚¿ãƒ¼ãƒ³ç³»
            returns_1d = daily_return
            returns_5d = daily_return * 5 + np.random.normal(0, 0.01)
            returns_20d = daily_return * 20 + np.random.normal(0, 0.05)
            
            row = {
                'date': date,
                'code': code,
                'adjustment_close': price,
                'adjustment_open': open_price,
                'adjustment_high': high,
                'adjustment_low': low,
                'adjustment_volume': volume,
                'turnover_value': price * volume,
                'return_1d': returns_1d,
                'return_5d': returns_5d,
                'return_10d': returns_5d * 2 + np.random.normal(0, 0.02),
                'return_20d': returns_20d,
                'rsi14': rsi,
                'ema20': ema_20,
                'volatility_20d': volatility,
                # ç‰¹å¾´é‡å€™è£œ
                'feat_ret_1d': daily_return,
                'feat_vol': volatility,
                'feat_momentum': rsi / 100.0
            }
            
            data.append(row)
    
    df = pd.DataFrame(data)
    logger.info(f"Created synthetic data: {len(df)} rows, {n_days} dates, {n_stocks} stocks")
    return df


class TestCrossSectionalNormalizerV2:
    """CrossSectionalNormalizerV2ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def test_polars_performance(self):
        """Polarsç‰ˆã®æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ§ª Testing CrossSectionalNormalizerV2 (Polars-based)...")
        
        df = create_synthetic_financial_data(n_stocks=100, n_days=200)
        
        # è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
        split_date = df['date'].quantile(0.7)
        train_df = df[df['date'] <= split_date]
        test_df = df[df['date'] > split_date]
        
        normalizer = CrossSectionalNormalizerV2(
            cache_stats=True,
            robust_clip=5.0,
            verbose=True
        )
        
        # ãƒ•ã‚£ãƒƒãƒˆï¼†å¤‰æ›
        start_time = datetime.now()
        train_norm = normalizer.fit_transform(train_df)
        test_norm = normalizer.transform(test_df)
        elapsed = (datetime.now() - start_time).total_seconds()
        
        # æ¤œè¨¼
        validation = normalizer.validate_transform(train_norm)
        
        print(f"âœ… Polars normalization completed in {elapsed:.2f}s")
        print(f"   Train samples: {len(train_norm)}, Test samples: {len(test_norm)}")
        print(f"   Warnings: {len(validation['warnings'])}")
        
        assert len(validation['warnings']) <= 2, "Too many normalization warnings"
        assert len(train_norm) > 0, "Training normalization failed"
        assert len(test_norm) > 0, "Test normalization failed"
        
        return True


class TestWalkForwardV2:
    """WalkForwardSplitterV2ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def test_embargo_20_splits(self):
        """embargo=20ã®åˆ†å‰²ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ“… Testing Walk-Forward with embargo=20...")
        
        df = create_synthetic_financial_data(n_stocks=30, n_days=500)
        
        splitter = WalkForwardSplitterV2(
            n_splits=3,
            embargo_days=20,
            min_train_days=100,
            min_test_days=30,
            verbose=True
        )
        
        # åˆ†å‰²æ¤œè¨¼
        validation = splitter.validate_split(df)
        splits = list(splitter.split(df))
        
        print(f"âœ… Generated {len(splits)} splits")
        print(f"   Total overlaps: {len(validation['overlaps'])}")
        print(f"   Average gap: {np.mean([g['gap_days'] for g in validation['gaps']]):.1f} days")
        
        # å„åˆ†å‰²ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’ãƒã‚§ãƒƒã‚¯
        for i, (train_idx, test_idx) in enumerate(splits):
            train_dates = pd.to_datetime(df.iloc[train_idx]['date'])
            test_dates = pd.to_datetime(df.iloc[test_idx]['date'])
            
            gap_days = (test_dates.min() - train_dates.max()).days
            print(f"   Fold {i}: Gap = {gap_days} days (embargo requirement: â‰¥{splitter.embargo_days})")
            
            assert gap_days >= splitter.embargo_days, f"Insufficient embargo in fold {i}"
        
        return True


class TestProductionLoaderV3:
    """ProductionDatasetV3ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def test_lazy_loading(self):
        """Lazy loading & åˆ—å°„å½±ãƒ†ã‚¹ãƒˆ"""
        print("\nâš¡ Testing ProductionDatasetV3 (Lazy Loading)...")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        temp_dir = Path(tempfile.mkdtemp())
        try:
            df = create_synthetic_financial_data(n_stocks=20, n_days=100)
            
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            parquet_files = []
            for i in range(3):
                chunk = df[i*len(df)//3:(i+1)*len(df)//3].copy()
                file_path = temp_dir / f"data_chunk_{i}.parquet"
                chunk.to_parquet(file_path)
                parquet_files.append(file_path)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            dataset = ProductionDatasetV3(
                data_files=parquet_files,
                config=type('Config', (), {
                    'data': type('Data', (), {
                        'time_series': type('TimeSeries', (), {
                            'prediction_horizons': [1, 5, 10, 20]
                        })()
                    })()
                })(),
                sequence_length=20,
                use_lazy_loading=True,
                memory_limit_gb=4.0
            )
            
            # ãƒ†ã‚¹ãƒˆ
            data_info = dataset.get_data_info()
            
            print(f"âœ… Dataset created: {data_info['num_sequences']} sequences")
            print(f"   Feature dim: {data_info['feature_dim']}")
            print(f"   Memory usage: {data_info['memory_usage_mb']:.1f} MB")
            print(f"   Date range: {data_info['date_range']['min']} to {data_info['date_range']['max']}")
            
            # ã‚µãƒ³ãƒ—ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ
            if len(dataset) > 0:
                sample = dataset[0]
                assert 'features' in sample, "Features not found in sample"
                assert sample['features'].shape[0] == 20, "Incorrect sequence length"
                print(f"   Sample features shape: {sample['features'].shape}")
            
            return True
            
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)


class TestGraphBuilder:
    """FinancialGraphBuilderãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def test_time_series_correlation(self):
        """æ™‚ç³»åˆ—ç›¸é–¢ã‚°ãƒ©ãƒ•æ§‹ç¯‰ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ•¸ï¸ Testing FinancialGraphBuilder (Time Series Correlation)...")
        
        df = create_synthetic_financial_data(n_stocks=25, n_days=150)
        codes = df['code'].unique()[:20]  # 20éŠ˜æŸ„ã§ãƒ†ã‚¹ãƒˆ
        
        graph_builder = FinancialGraphBuilder(
            correlation_window=60,
            correlation_threshold=0.3,
            max_edges_per_node=5,
            include_negative_correlation=True,
            verbose=True
        )
        
        # ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        end_date = df['date'].max()
        graph_result = graph_builder.build_graph(
            data=df,
            codes=codes,
            date_end=end_date,
            return_column='return_1d'
        )
        
        print(f"âœ… Graph built: {graph_result['n_nodes']} nodes, {graph_result['n_edges']} edges")
        
        # peerç‰¹å¾´é‡ãƒ†ã‚¹ãƒˆ
        peer_features = graph_builder.get_peer_features_for_codes(codes[:5], end_date)
        
        for code, features in list(peer_features.items())[:3]:
            print(f"   {code}: peer_count={features['peer_count']}, "
                  f"peer_correlation_mean={features['peer_correlation_mean']:.3f}")
        
        # ã‚°ãƒ©ãƒ•çµ±è¨ˆ
        stats = graph_builder.analyze_graph_statistics(end_date)
        if stats:
            print(f"   Graph density: {stats.get('network_stats', {}).get('density', 'N/A')}")
        
        assert graph_result['n_nodes'] > 0, "No nodes in graph"
        assert len(peer_features) > 0, "No peer features generated"
        
        return True


class TestLightGBMBaseline:
    """LightGBMBaselineãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def test_multi_horizon_baseline(self):
        """å¤šãƒ›ãƒ©ã‚¤ã‚ºãƒ³GBMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸŒ² Testing LightGBM Multi-Horizon Baseline...")
        
        df = create_synthetic_financial_data(n_stocks=30, n_days=200)
        
        # è»½é‡è¨­å®šã§ãƒ†ã‚¹ãƒˆ
        lgb_params = {
            'objective': 'regression',
            'metric': 'rmse',
            'num_leaves': 15,
            'learning_rate': 0.1,
            'n_estimators': 10,  # é«˜é€ŸåŒ–ã®ãŸã‚å°‘ãªã
            'verbosity': -1,
            'seed': 42
        }
        
        baseline = LightGBMFinancialBaseline(
            prediction_horizons=[1, 5],  # 2ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã®ã¿ã§ãƒ†ã‚¹ãƒˆ
            lgb_params=lgb_params,
            n_splits=2,  # 2åˆ†å‰²ã§é«˜é€ŸåŒ–
            embargo_days=20,
            normalize_features=True,
            verbose=True
        )
        
        # å­¦ç¿’å®Ÿè¡Œ
        baseline.fit(df)
        
        # çµæœè©•ä¾¡
        performance = baseline.evaluate_performance()
        results_summary = baseline.get_results_summary()
        
        print(f"âœ… Baseline training completed")
        print(f"   Performance summary:")
        
        for horizon, metrics in performance.items():
            print(f"   {horizon}: mean_ic={metrics['mean_ic']:.3f}, "
                  f"mean_rank_ic={metrics['mean_rank_ic']:.3f}, "
                  f"positive_rate={metrics['ic_positive_rate']:.1%}")
        
        # ç‰¹å¾´é‡é‡è¦åº¦ãƒ†ã‚¹ãƒˆ
        if 1 in baseline.feature_importance:
            importance_df = baseline.get_feature_importance(horizon=1, top_k=5)
            print(f"   Top 5 features for 1d horizon:")
            for _, row in importance_df.iterrows():
                print(f"     {row['feature']}: {row['importance']:.1f}")
        
        assert len(performance) > 0, "No performance metrics generated"
        assert len(results_summary) > 0, "No results summary generated"
        
        return True


class TestQualityFeatures:
    """QualityFinancialFeaturesGeneratorãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def test_quality_features_generation(self):
        """é«˜å“è³ªç‰¹å¾´é‡ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        print("\nâœ¨ Testing Quality Features Generation...")
        
        df = create_synthetic_financial_data(n_stocks=20, n_days=100)
        
        generator = QualityFinancialFeaturesGenerator(
            use_cross_sectional_quantiles=True,
            sigma_threshold=2.0,
            verbose=True
        )
        
        # ç‰¹å¾´é‡ç”Ÿæˆ
        original_cols = len(df.columns)
        enhanced_df = generator.generate_quality_features(df)
        final_cols = len(enhanced_df.columns)
        
        print(f"âœ… Features enhanced: {original_cols} â†’ {final_cols} columns (+{final_cols - original_cols})")
        
        # ç‰¹å¾´é‡å“è³ªæ¤œè¨¼
        validation = generator.validate_features(enhanced_df)
        
        print(f"   Quality validation:")
        print(f"     Zero variance features: {len(validation['zero_variance_features'])}")
        print(f"     High missing features: {len(validation['high_missing_features'])}")
        print(f"     Infinite features: {len(validation['infinite_features'])}")
        
        # ç‰¹å¾´é‡ã‚«ãƒ†ã‚´ãƒª
        categories = validation['feature_categories']
        for category, features in categories.items():
            if features:
                print(f"     {category}: {len(features)} features")
        
        # daily_volåˆ—ã®å­˜åœ¨ç¢ºèª
        assert not validation['missing_daily_vol'], "daily_vol column not generated"
        assert 'sharpe_features' in categories, "Sharpe features not generated"
        assert final_cols > original_cols, "No new features added"
        
        return True


class TestIntegration:
    """çµ±åˆãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def test_full_pipeline(self):
        """ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ”— Testing Full Pipeline Integration...")
        
        # 1. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        df = create_synthetic_financial_data(n_stocks=40, n_days=250)
        print(f"   Step 1: Generated {len(df)} samples")
        
        # 2. é«˜å“è³ªç‰¹å¾´é‡ç”Ÿæˆ
        generator = QualityFinancialFeaturesGenerator(verbose=False)
        df_enhanced = generator.generate_quality_features(df)
        print(f"   Step 2: Enhanced to {len(df_enhanced.columns)} features")
        
        # 3. Cross-sectionalæ­£è¦åŒ–
        normalizer = CrossSectionalNormalizerV2(verbose=False)
        split_date = df_enhanced['date'].quantile(0.7)
        
        if isinstance(df_enhanced, pl.DataFrame):
            train_mask = df_enhanced['date'] <= pl.lit(split_date)
            test_mask = df_enhanced['date'] > pl.lit(split_date)
            train_df = df_enhanced.filter(train_mask)
            test_df = df_enhanced.filter(test_mask)
        else:
            train_df = df_enhanced[df_enhanced['date'] <= split_date]
            test_df = df_enhanced[df_enhanced['date'] > split_date]
        
        train_norm = normalizer.fit_transform(train_df)
        test_norm = normalizer.transform(test_df)
        print(f"   Step 3: Normalized {len(train_norm)} train + {len(test_norm)} test samples")
        
        # 4. ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        codes = df['code'].unique()[:15]  # 15éŠ˜æŸ„ã§è»½é‡åŒ–
        graph_builder = FinancialGraphBuilder(verbose=False)
        graph_result = graph_builder.build_graph(
            data=df, codes=codes, date_end=df['date'].max()
        )
        print(f"   Step 4: Built graph with {graph_result['n_nodes']} nodes, {graph_result['n_edges']} edges")
        
        # 5. è»½é‡GBMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        df_pandas = df_enhanced.to_pandas() if isinstance(df_enhanced, pl.DataFrame) else df_enhanced
        baseline = LightGBMFinancialBaseline(
            prediction_horizons=[1, 5],
            n_splits=2,
            embargo_days=10,  # ãƒ†ã‚¹ãƒˆç”¨ã«çŸ­ç¸®
            verbose=False
        )
        
        # æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
        sample_df = df_pandas.sample(n=min(1000, len(df_pandas)))
        baseline.fit(sample_df)
        performance = baseline.evaluate_performance()
        print(f"   Step 5: GBM baseline trained, performance: {len(performance)} horizons")
        
        # 6. æœ€çµ‚æ¤œè¨¼
        success_indicators = [
            len(df_enhanced.columns) > len(df.columns),  # ç‰¹å¾´é‡æ‹¡å¼µ
            len(train_norm) > 0,  # æ­£è¦åŒ–æˆåŠŸ
            graph_result['n_nodes'] > 0,  # ã‚°ãƒ©ãƒ•æ§‹ç¯‰æˆåŠŸ
            len(performance) > 0,  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å­¦ç¿’æˆåŠŸ
        ]
        
        success_rate = sum(success_indicators) / len(success_indicators)
        
        print(f"âœ… Full pipeline integration: {success_rate:.1%} success rate")
        
        pipeline_result = {
            'original_features': len(df.columns),
            'enhanced_features': len(df_enhanced.columns),
            'normalized_samples': len(train_norm) + len(test_norm),
            'graph_nodes': graph_result['n_nodes'],
            'graph_edges': graph_result['n_edges'],
            'model_horizons': len(performance),
            'success_rate': success_rate
        }
        
        return pipeline_result


def run_all_tests():
    """å…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    if not COMPONENTS_AVAILABLE:
        print("âŒ Components not available, skipping tests")
        return False
    
    print("ğŸš€ Starting Gogooku3 Improvements Integration Tests...\n")
    
    test_results = {}
    
    try:
        # å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ†ã‚¹ãƒˆ
        test_results['normalizer_v2'] = TestCrossSectionalNormalizerV2().test_polars_performance()
        test_results['walk_forward_v2'] = TestWalkForwardV2().test_embargo_20_splits()
        test_results['loader_v3'] = TestProductionLoaderV3().test_lazy_loading()
        test_results['graph_builder'] = TestGraphBuilder().test_time_series_correlation()
        test_results['gbm_baseline'] = TestLightGBMBaseline().test_multi_horizon_baseline()
        test_results['quality_features'] = TestQualityFeatures().test_quality_features_generation()
        
        # çµ±åˆãƒ†ã‚¹ãƒˆ
        test_results['full_pipeline'] = TestIntegration().test_full_pipeline()
        
        # çµæœã‚µãƒãƒªãƒ¼
        passed_tests = sum(1 for result in test_results.values() if result)
        total_tests = len(test_results)
        
        print(f"\nğŸ‰ Test Results: {passed_tests}/{total_tests} tests passed")
        
        for test_name, result in test_results.items():
            status = "âœ… PASS" if result else "âŒ FAIL"
            print(f"   {test_name}: {status}")
        
        # çµ±åˆãƒ†ã‚¹ãƒˆçµæœã®è©³ç´°
        if isinstance(test_results['full_pipeline'], dict):
            pipeline_result = test_results['full_pipeline']
            print(f"\nğŸ“Š Pipeline Integration Details:")
            print(f"   Features: {pipeline_result['original_features']} â†’ {pipeline_result['enhanced_features']}")
            print(f"   Normalized samples: {pipeline_result['normalized_samples']:,}")
            print(f"   Graph: {pipeline_result['graph_nodes']} nodes, {pipeline_result['graph_edges']} edges")
            print(f"   Models: {pipeline_result['model_horizons']} horizons")
            print(f"   Success rate: {pipeline_result['success_rate']:.1%}")
        
        overall_success = passed_tests == total_tests
        
        print(f"\nğŸ† Overall Result: {'âœ… ALL TESTS PASSED' if overall_success else 'âš ï¸ SOME TESTS FAILED'}")
        
        return overall_success
        
    except Exception as e:
        print(f"âŒ Test execution failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)