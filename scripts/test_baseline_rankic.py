#!/usr/bin/env python3
"""
Phase 0: Test baseline model to verify data predictability
ç›®çš„: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã§RankICãŒè¨ˆç®—å¯èƒ½ã‹ç¢ºèª
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from lightgbm import LGBMRegressor
from scipy.stats import spearmanr
import warnings
warnings.filterwarnings('ignore')

def calculate_rankic(y_true, y_pred):
    """RankIC (Spearman correlation) ã‚’è¨ˆç®—"""
    # NaNã‚’é™¤å»
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    if mask.sum() < 10:
        return np.nan

    corr, _ = spearmanr(y_true[mask], y_pred[mask])
    return corr

def test_baseline_models(data_path=None):
    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã§RankICã‚’ãƒ†ã‚¹ãƒˆ"""

    print("=" * 60)
    print("ğŸ§ª Baseline Model RankIC Test")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“‚ Loading data...")
    if data_path:
        df = pd.read_parquet(data_path)
    else:
        df = pd.read_parquet('/home/ubuntu/gogooku3-standalone/output/ml_dataset_latest_full.parquet')
    print(f"âœ… Data shape: {df.shape}")

    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢
    feature_cols = [col for col in df.columns if not col.startswith('returns_') and col not in ['Date', 'Code']]
    target_cols = ['returns_1d', 'returns_5d', 'returns_10d', 'returns_20d']

    print(f"\nğŸ“Š Features: {len(feature_cols)}")
    print(f"ğŸ¯ Targets: {target_cols}")

    # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
    df = df.sort_values('Date')

    # Train/Valåˆ†å‰² (æ™‚ç³»åˆ—ã‚’è€ƒæ…®)
    split_date = df['Date'].quantile(0.8)
    train_df = df[df['Date'] < split_date]
    val_df = df[df['Date'] >= split_date]

    print(f"\nğŸ“… Train: {train_df['Date'].min()} to {train_df['Date'].max()} ({len(train_df):,} samples)")
    print(f"ğŸ“… Val: {val_df['Date'].min()} to {val_df['Date'].max()} ({len(val_df):,} samples)")

    # æ¬ æå€¤å‡¦ç†
    print("\nğŸ”§ Preparing data...")
    # æ•°å€¤åˆ—ã®ã¿ã‚’é¸æŠ
    numeric_features = [col for col in feature_cols if train_df[col].dtype in ['float64', 'float32', 'int64', 'int32']]
    print(f"   Using {len(numeric_features)} numeric features (from {len(feature_cols)} total)")

    X_train = train_df[numeric_features].fillna(0)
    X_val = val_df[numeric_features].fillna(0)

    # ç•°å¸¸å€¤ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
    X_train = X_train.clip(-10, 10)
    X_val = X_val.clip(-10, 10)

    print("âœ… Data prepared")

    # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«å¯¾ã—ã¦ãƒ†ã‚¹ãƒˆ
    results = {}

    for target_col in target_cols:
        print(f"\n" + "=" * 60)
        print(f"ğŸ¯ Testing: {target_col}")
        print("=" * 60)

        y_train = train_df[target_col].fillna(0)
        y_val = val_df[target_col].fillna(0)

        # 1. ãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
        print("\n1ï¸âƒ£ Random Baseline:")
        y_pred_random = np.random.randn(len(y_val)) * y_train.std() + y_train.mean()
        rankic_random = calculate_rankic(y_val.values, y_pred_random)
        print(f"   RankIC: {rankic_random:.6f} (expected ~0)")
        results[f"{target_col}_random"] = rankic_random

        # 2. å¹³å‡äºˆæ¸¬
        print("\n2ï¸âƒ£ Mean Baseline:")
        y_pred_mean = np.full(len(y_val), y_train.mean())
        rankic_mean = calculate_rankic(y_val.values, y_pred_mean)
        print(f"   RankIC: {rankic_mean:.6f} (expected ~0)")
        results[f"{target_col}_mean"] = rankic_mean

        # 3. ç·šå½¢å›å¸°
        print("\n3ï¸âƒ£ Linear Regression:")
        lr = LinearRegression()

        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’åˆ¶é™ï¼ˆãƒ¡ãƒ¢ãƒªå¯¾ç­–ï¼‰
        sample_size = min(100000, len(X_train))
        indices = np.random.choice(len(X_train), sample_size, replace=False)

        lr.fit(X_train.iloc[indices], y_train.iloc[indices])
        y_pred_lr = lr.predict(X_val)
        rankic_lr = calculate_rankic(y_val.values, y_pred_lr)
        print(f"   RankIC: {rankic_lr:.6f}")
        results[f"{target_col}_lr"] = rankic_lr

        # 4. LightGBM (ã‚·ãƒ³ãƒ—ãƒ«è¨­å®š)
        print("\n4ï¸âƒ£ LightGBM:")
        lgb = LGBMRegressor(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            num_leaves=31,
            random_state=42,
            verbosity=-1
        )

        lgb.fit(
            X_train.iloc[indices],
            y_train.iloc[indices],
            eval_set=[(X_val.iloc[:10000], y_val.iloc[:10000])],
            callbacks=[lambda x: None]  # Suppress output
        )
        y_pred_lgb = lgb.predict(X_val)
        rankic_lgb = calculate_rankic(y_val.values, y_pred_lgb)
        print(f"   RankIC: {rankic_lgb:.6f}")
        results[f"{target_col}_lgb"] = rankic_lgb

        # 5. å›ºå®šå€¤äºˆæ¸¬ãƒ†ã‚¹ãƒˆï¼ˆ0.0719ã®è¬ã‚’è§£æ˜ï¼‰
        print("\n5ï¸âƒ£ Fixed Value Test (debugging 0.0719):")
        # ã„ãã¤ã‹ã®å›ºå®šå€¤ã§ãƒ†ã‚¹ãƒˆ
        test_values = [0, 0.001, 0.01, 0.1, 1.0]
        for test_val in test_values:
            y_pred_fixed = np.full(len(y_val), test_val)
            rankic_fixed = calculate_rankic(y_val.values, y_pred_fixed)
            print(f"   Fixed {test_val}: RankIC = {rankic_fixed:.6f}")

            # 0.0719ã«è¿‘ã„å€¤ã‚’ç™ºè¦‹ã—ãŸã‚‰è©³ç´°åˆ†æ
            if abs(rankic_fixed - 0.0719) < 0.001:
                print(f"   ğŸ”´ FOUND! Fixed value {test_val} gives RankIC â‰ˆ 0.0719")

        # 6. ã‚ãšã‹ãªãƒã‚¤ã‚ºã‚’åŠ ãˆãŸå›ºå®šå€¤
        print("\n6ï¸âƒ£ Fixed + Small Noise:")
        y_pred_noise = np.full(len(y_val), y_train.mean()) + np.random.randn(len(y_val)) * 1e-6
        rankic_noise = calculate_rankic(y_val.values, y_pred_noise)
        print(f"   RankIC: {rankic_noise:.6f}")

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("ğŸ“Š Summary Results")
    print("=" * 60)

    for target_col in target_cols:
        print(f"\n{target_col}:")
        print(f"  Random:  {results[f'{target_col}_random']:.6f}")
        print(f"  Mean:    {results[f'{target_col}_mean']:.6f}")
        print(f"  Linear:  {results[f'{target_col}_lr']:.6f}")
        print(f"  LightGBM: {results[f'{target_col}_lgb']:.6f}")

    # è¨ºæ–­
    print("\n" + "=" * 60)
    print("ğŸ Diagnosis")
    print("=" * 60)

    # LightGBMã®RankICã‚’ãƒã‚§ãƒƒã‚¯
    lgb_rankics = [results[f'{t}_lgb'] for t in target_cols]
    max_rankic = max(lgb_rankics)

    if max_rankic < 0.01:
        print("ğŸ”´ CRITICAL: Even LightGBM cannot achieve meaningful RankIC!")
        print("   Possible issues:")
        print("   1. Data leakage in preprocessing")
        print("   2. Target-feature temporal misalignment")
        print("   3. Features not predictive of targets")
    elif max_rankic < 0.05:
        print("ğŸŸ¡ WARNING: Low predictability in data")
        print("   Maximum RankIC from LightGBM: {:.4f}".format(max_rankic))
        print("   The ATFT model should be able to exceed this")
    else:
        print("ğŸŸ¢ OK: Data has predictive signal")
        print("   Maximum RankIC from LightGBM: {:.4f}".format(max_rankic))
        print("   â†’ ATFT-GAT-FAN should achieve > {:.4f}".format(max_rankic))

    # 0.0719ã®è¬
    print("\nğŸ” About the stuck RankIC = 0.0719:")
    if any(abs(results[f'{t}_mean'] - 0.0719) < 0.01 for t in target_cols):
        print("   â†’ Likely the model is outputting constant/near-constant predictions")
        print("   â†’ Check model initialization and gradient flow")
    else:
        print("   â†’ The value doesn't match simple baselines")
        print("   â†’ Could be a specific pattern in model outputs")

if __name__ == "__main__":
    import sys
    data_path = sys.argv[1] if len(sys.argv) > 1 else None
    test_baseline_models(data_path)