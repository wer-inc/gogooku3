name: production

# Batch configuration optimized for A100 GPU with stability
batch:
  train_batch_size: 4096
  val_batch_size: 6144
  test_batch_size: 6144
  num_workers: 16
  prefetch_factor: 8
  persistent_workers: true
  pin_memory: true
  gradient_accumulation_steps: 1

# Optimizer configuration optimized for financial ML
optimizer:
  type: adamw
  lr: 0.0002            # Tuned for higher throughput GPUs
  weight_decay: 0.05    # Increased from 0.01 for stronger regularization
  betas: [0.9, 0.999]
  eps: 1e-7

# Learning rate scheduler with aggressive plateau detection
scheduler:
  type: ReduceLROnPlateau
  factor: 0.5           
  patience: 3           # Reduced from 5 for faster adaptation
  min_lr: 1e-6          # Lowered from 1e-5 for finer tuning
  verbose: true
  threshold: 0.0001
  threshold_mode: rel
  # Enhanced parameters for better convergence
  warmup_steps: 2000    # Increased from 1500 for gradual warmup
  gamma: 0.95           # Reduced from 0.98 for faster decay

# Training configuration
trainer:
  max_epochs: 75
  gradient_clip_val: 0.8
  gradient_clip_algorithm: norm
  accumulate_grad_batches: 1
  val_check_interval: 1.0
  check_val_every_n_epoch: 1
  log_every_n_steps: 25
  enable_checkpointing: true
  enable_model_summary: true
  enable_progress_bar: true
  detect_anomaly: false
  benchmark: true
  deterministic: false
  precision: 16-mixed
  accelerator: gpu
  devices: 1

# Throughput tuning
throughput:
  min_gpu_utilization: 0.85
  target_prefetch_size_mb: 2048

# Early stopping configuration
early_stopping:
  monitor: val/sharpe_ratio
  mode: max
  patience: 9
  min_delta: 0.001
  verbose: true

# Multi-horizon prediction configuration
prediction:
  horizons: [1, 5, 10, 20]  # Prediction horizons in days
  horizon_specific_architecture: true  # Enable horizon-specific heads
  horizon_weights: [1.0, 1.0, 1.0, 1.0]  # Balanced weights to reduce short-term noise domination

# Loss configuration (optimized for financial prediction)
loss:
  type: huber_multi_horizon  # Changed from MSE to Huber for robustness
  huber_delta: 0.01  # Optimized for financial returns
  multi_horizon_weights: [1.0, 1.0, 1.0, 1.0]  # Balanced weights - fixes short-term noise domination per PDF diagnosis
  use_coverage_penalty: true  # Enable coverage penalty
  coverage_alpha: 0.02  # Increased from 0.01
  rankic_weight: 0.1  # Enable RankIC loss
  sharpe_weight: 0.05  # Enable Sharpe loss
  l2_lambda: 0.001  # Enable L2 regularization

# Model checkpointing
checkpoint:
  monitor: val/sharpe_ratio
  mode: max
  save_top_k: 3
  save_last: false
  save_on_train_epoch_end: false
  filename: 'production-{epoch:03d}-{val_sharpe_ratio:.6f}'

# Logging
logging:
  log_every_n_steps: 10
  log_grad_norm: true
  log_learning_rate: true


# Mixed precision settings
grad_scaler:
  init_scale: 65536.0  # 2^16
  growth_factor: 2.0
  backoff_factor: 0.5
  growth_interval: 2000

# EMA settings (improved - default OFF for backward compatibility)
ema:
  enabled: false  # Default OFF - set to true to enable EMA
  decay: 0.999
  update_every_step: true

# Validation stability
validation:
  per_date_metrics: true
  outlier_detection: true
  outlier_threshold: 3.0

# Data safety
data:
  correlation_eps: 1e-8
  correlation_clamp: [-0.999, 0.999]
  nan_fill_value: 0.0
