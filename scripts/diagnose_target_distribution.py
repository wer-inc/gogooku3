#!/usr/bin/env python3
"""
Phase 0: Diagnose target variable distribution
ç›®çš„: returns_*d ã®åˆ†å¸ƒã‚’ç¢ºèªã—ã€å­¦ç¿’å¯èƒ½ãªçŠ¶æ…‹ã‹ã‚’æ¤œè¨¼
"""

import pandas as pd
import numpy as np
import polars as pl
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

def analyze_target_distribution():
    """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†å¸ƒã‚’è©³ç´°ã«åˆ†æ"""

    print("=" * 60)
    print("ğŸ” ATFT-GAT-FAN Target Distribution Diagnosis")
    print("=" * 60)

    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_path = Path("/home/ubuntu/gogooku3-standalone/output/ml_dataset_latest_full.parquet")

    if not data_path.exists():
        print(f"âŒ Data file not found: {data_path}")
        return

    print(f"ğŸ“‚ Loading data from: {data_path}")
    df = pd.read_parquet(data_path)
    print(f"âœ… Data loaded: {df.shape}")

    # 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ç‰¹å®š
    target_columns = [col for col in df.columns if col.startswith('returns_') and col.endswith('d')]
    print(f"\nğŸ“Š Target columns found: {target_columns}")

    if not target_columns:
        print("âŒ No target columns (returns_*d) found!")
        return

    # 3. å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®çµ±è¨ˆåˆ†æ
    print("\n" + "=" * 60)
    print("ğŸ“ˆ Target Distribution Statistics")
    print("=" * 60)

    analysis_results = {}

    for col in target_columns:
        print(f"\nğŸ¯ Analyzing: {col}")
        print("-" * 40)

        values = df[col].dropna()

        # åŸºæœ¬çµ±è¨ˆ
        stats_dict = {
            'count': len(values),
            'mean': values.mean(),
            'std': values.std(),
            'min': values.min(),
            'max': values.max(),
            'median': values.median(),
            'skew': values.skew(),
            'kurtosis': values.kurtosis(),
            'q01': values.quantile(0.01),
            'q05': values.quantile(0.05),
            'q25': values.quantile(0.25),
            'q75': values.quantile(0.75),
            'q95': values.quantile(0.95),
            'q99': values.quantile(0.99),
            'zero_ratio': (values == 0).sum() / len(values),
            'near_zero_ratio': (np.abs(values) < 1e-6).sum() / len(values),
            'unique_values': values.nunique(),
            'iqr': values.quantile(0.75) - values.quantile(0.25)
        }

        analysis_results[col] = stats_dict

        # çµ±è¨ˆå€¤è¡¨ç¤º
        print(f"  Count:        {stats_dict['count']:,.0f}")
        print(f"  Mean:         {stats_dict['mean']:.6f}")
        print(f"  Std:          {stats_dict['std']:.6f}")
        print(f"  Min:          {stats_dict['min']:.6f}")
        print(f"  Max:          {stats_dict['max']:.6f}")
        print(f"  Median:       {stats_dict['median']:.6f}")
        print(f"  IQR:          {stats_dict['iqr']:.6f}")
        print(f"  Skewness:     {stats_dict['skew']:.3f}")
        print(f"  Kurtosis:     {stats_dict['kurtosis']:.3f}")

        # åˆ†ä½ç‚¹
        print(f"\n  Quantiles:")
        print(f"    1%:         {stats_dict['q01']:.6f}")
        print(f"    5%:         {stats_dict['q05']:.6f}")
        print(f"    25%:        {stats_dict['q25']:.6f}")
        print(f"    75%:        {stats_dict['q75']:.6f}")
        print(f"    95%:        {stats_dict['q95']:.6f}")
        print(f"    99%:        {stats_dict['q99']:.6f}")

        # å•é¡Œã®æ¤œå‡º
        print(f"\n  âš ï¸ Potential Issues:")
        print(f"    Zero ratio:      {stats_dict['zero_ratio']:.2%}")
        print(f"    Near-zero ratio: {stats_dict['near_zero_ratio']:.2%}")
        print(f"    Unique values:   {stats_dict['unique_values']:,}")

        # ã‚¹ã‚±ãƒ¼ãƒ«ã®å•é¡Œåˆ¤å®š
        if stats_dict['std'] < 0.001:
            print(f"    ğŸ”´ CRITICAL: Standard deviation too small ({stats_dict['std']:.6f})")
            print(f"       â†’ Model cannot learn from such small variations!")
        elif stats_dict['std'] < 0.01:
            print(f"    ğŸŸ¡ WARNING: Small standard deviation ({stats_dict['std']:.6f})")
            print(f"       â†’ May cause learning difficulties")
        else:
            print(f"    ğŸŸ¢ OK: Standard deviation acceptable ({stats_dict['std']:.6f})")

        # IQRãƒã‚§ãƒƒã‚¯
        if stats_dict['iqr'] < 0.001:
            print(f"    ğŸ”´ CRITICAL: IQR too small ({stats_dict['iqr']:.6f})")
            print(f"       â†’ Most values are nearly identical!")

        # å¤–ã‚Œå€¤ã®æ¤œå‡º
        outlier_threshold = 3
        z_scores = np.abs((values - values.mean()) / values.std())
        outlier_ratio = (z_scores > outlier_threshold).sum() / len(values)
        print(f"    Outliers (>3Ïƒ): {outlier_ratio:.2%}")

    # 4. æ¨å¥¨äº‹é …
    print("\n" + "=" * 60)
    print("ğŸ’¡ Recommendations")
    print("=" * 60)

    for col, stats_dict in analysis_results.items():
        print(f"\n{col}:")

        if stats_dict['std'] < 0.001:
            print("  ğŸ”´ URGENT: Data scaling required!")
            print("     Actions:")
            print("     1. Multiply by 100 (convert to percentage)")
            print("     2. Apply robust scaling")
            print("     3. Use percentile normalization")

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®äºˆæ¸¬å€¤
            scaled_std = stats_dict['std'] * 100
            print(f"     â†’ After scaling (*100): std = {scaled_std:.4f}")

        elif stats_dict['std'] < 0.01:
            print("  ğŸŸ¡ Consider data scaling:")
            print("     - StandardScaler or RobustScaler")
            print("     - Clip outliers at 99th percentile")
        else:
            print("  ğŸŸ¢ Data scale acceptable")
            print("     - Consider outlier clipping if needed")

    # 5. ç›¸é–¢åˆ†æ
    print("\n" + "=" * 60)
    print("ğŸ“Š Target Correlations")
    print("=" * 60)

    if len(target_columns) > 1:
        corr_matrix = df[target_columns].corr()
        print("\nCorrelation Matrix:")
        print(corr_matrix.round(3))

        # é«˜ç›¸é–¢ã®è­¦å‘Š
        for i in range(len(target_columns)):
            for j in range(i+1, len(target_columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.9:
                    print(f"\nâš ï¸ High correlation ({corr_val:.3f}) between {target_columns[i]} and {target_columns[j]}")

    # 6. ã‚µãƒ³ãƒ—ãƒ«å€¤ã®è¡¨ç¤º
    print("\n" + "=" * 60)
    print("ğŸ“ Sample Values (first 10 non-zero)")
    print("=" * 60)

    for col in target_columns[:2]:  # æœ€åˆã®2ã¤ã ã‘è¡¨ç¤º
        non_zero = df[df[col] != 0][col].head(10).values
        print(f"\n{col}:")
        print(f"  {non_zero}")

    # 7. æœ€çµ‚è¨ºæ–­
    print("\n" + "=" * 60)
    print("ğŸ Final Diagnosis")
    print("=" * 60)

    critical_issues = []
    warnings = []

    for col, stats_dict in analysis_results.items():
        if stats_dict['std'] < 0.001:
            critical_issues.append(f"{col}: Scale too small (std={stats_dict['std']:.6f})")
        elif stats_dict['std'] < 0.01:
            warnings.append(f"{col}: Small scale (std={stats_dict['std']:.6f})")

        if stats_dict['near_zero_ratio'] > 0.5:
            warnings.append(f"{col}: {stats_dict['near_zero_ratio']:.1%} near-zero values")

    if critical_issues:
        print("\nğŸ”´ CRITICAL ISSUES (must fix):")
        for issue in critical_issues:
            print(f"  - {issue}")
        print("\n  â†’ This explains why Val RankIC is stuck at 0.0719!")
        print("  â†’ Model cannot learn from such small variations")

    if warnings:
        print("\nğŸŸ¡ WARNINGS:")
        for warning in warnings:
            print(f"  - {warning}")

    if not critical_issues and not warnings:
        print("\nğŸŸ¢ No major issues detected in target distribution")

    # 8. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
    print("\n" + "=" * 60)
    print("ğŸš€ Next Steps")
    print("=" * 60)

    if critical_issues:
        print("\n1. Immediate action: Scale target variables")
        print("   - Create scripts/fix_target_scaling.py")
        print("   - Apply scaling factor (e.g., *100 for percentage)")
        print("   - Regenerate ATFT data with scaled targets")
        print("\n2. Verify RankIC calculation logic")
        print("   - Check if RankIC uses correct scale")
        print("   - Ensure no division by near-zero std")
        print("\n3. Re-run training with fixed data")
    else:
        print("\n1. Proceed with Phase 1: Data quality improvements")
        print("2. Check feature distributions next")

if __name__ == "__main__":
    analyze_target_distribution()