#!/usr/bin/env python3
"""
Critical: Detect and fix data leakage
ç›®çš„: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ãŒç‰¹å¾´é‡ã«å«ã¾ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
"""

import numpy as np
import pandas as pd


def check_normalization_stats(df, feature_cols, verbose=True):
    """
    Check normalization statistics validity

    Verifies that feature statistics (mean, std) are reasonable
    and not indicative of data quality issues.

    Args:
        df: DataFrame with features
        feature_cols: List of feature column names
        verbose: Print detailed info

    Returns:
        bool: True if stats look reasonable, False if warnings detected
    """
    if verbose:
        print("\n" + "=" * 60)
        print("5ï¸âƒ£ Checking normalization statistics...")
        print("=" * 60)

    # Compute statistics for numeric features
    numeric_features = [
        col for col in feature_cols
        if df[col].dtype in ['float64', 'float32', 'int64', 'int32']
    ]

    if len(numeric_features) == 0:
        if verbose:
            print("âš ï¸ No numeric features found")
        return False

    # Sample for efficiency (100k rows)
    sample_df = df.sample(min(100000, len(df)))

    means = sample_df[numeric_features].mean()
    stds = sample_df[numeric_features].std()

    if verbose:
        print(f"\nğŸ“Š Statistics from {len(sample_df)} samples:")
        print(f"   Mean range: [{means.min():.4f}, {means.max():.4f}]")
        print(f"   Std range:  [{stds.min():.6f}, {stds.max():.4f}]")

    # Detect anomalies (conservative thresholds)
    warnings = []

    # 1. Extremely large absolute means (> 1000)
    extreme_means = [c for c in numeric_features if abs(means[c]) > 1000]
    if extreme_means:
        warnings.append(
            f"âš ï¸  {len(extreme_means)} features with |mean| > 1000 (may need scaling)"
        )
        if verbose:
            print(f"\n   Extreme means: {extreme_means[:5]}")

    # 2. Near-constant features (std < 1e-8)
    tiny_stds = [c for c in numeric_features if stds[c] < 1e-8]
    if tiny_stds:
        warnings.append(
            f"âš ï¸  {len(tiny_stds)} features with std < 1e-8 (near-constant, consider removal)"
        )
        if verbose:
            print(f"   Near-constant features: {tiny_stds[:5]}")

    # 3. Extremely large std (> 10000)
    huge_stds = [c for c in numeric_features if stds[c] > 10000]
    if huge_stds:
        warnings.append(
            f"âš ï¸  {len(huge_stds)} features with std > 10000 (outliers or scale issue)"
        )
        if verbose:
            print(f"   Huge std features: {huge_stds[:5]}")

    # 4. NaN/Inf check
    nan_counts = sample_df[numeric_features].isna().sum()
    high_nan_features = [c for c in numeric_features if nan_counts[c] > len(sample_df) * 0.5]
    if high_nan_features:
        warnings.append(
            f"âš ï¸  {len(high_nan_features)} features with >50% missing values"
        )
        if verbose:
            print(f"   High missing features: {high_nan_features[:5]}")

    # Print summary
    if verbose:
        print("\n" + "-" * 60)
        if warnings:
            for w in warnings:
                print(w)
            print("\nğŸ’¡ Recommendation: Review these features before training")
            return False
        else:
            print("âœ… Normalization statistics look reasonable")
            return True

    return len(warnings) == 0


def detect_data_leakage():
    """ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’æ¤œå‡º"""

    print("=" * 60)
    print("ğŸ” DATA LEAKAGE DETECTION")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    import sys
    data_path = sys.argv[1] if len(sys.argv) > 1 else 'output/ml_dataset_latest_full.parquet'
    print(f"\nğŸ“‚ Loading data: {data_path}")
    df = pd.read_parquet(data_path)
    print(f"âœ… Data shape: {df.shape}")

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
    target_cols = ['returns_1d', 'returns_5d', 'returns_10d', 'returns_20d', 'returns_60d', 'returns_120d']
    print(f"\nğŸ¯ Target columns: {target_cols}")

    # ç‰¹å¾´é‡åˆ—
    feature_cols = [col for col in df.columns if col not in ['Date', 'Code'] + target_cols]
    print(f"ğŸ“Š Feature columns: {len(feature_cols)}")

    # 1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨åŒã˜åå‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ç‰¹å¾´é‡ã‚’æ¢ã™
    print("\n" + "=" * 60)
    print("1ï¸âƒ£ Checking for suspicious feature names...")
    print("=" * 60)

    suspicious_features = []
    for col in feature_cols:
        # returnã‚„retãªã©ç–‘ã‚ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³
        if any(pattern in col.lower() for pattern in ['return', 'ret', 'target']):
            suspicious_features.append(col)

    if suspicious_features:
        print(f"\nâš ï¸ Found {len(suspicious_features)} suspicious features:")
        for col in suspicious_features[:20]:  # æœ€åˆã®20å€‹ã‚’è¡¨ç¤º
            print(f"   - {col}")
    else:
        print("âœ… No suspicious feature names found")

    # 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨å®Œå…¨ç›¸é–¢ã™ã‚‹ç‰¹å¾´é‡ã‚’æ¢ã™
    print("\n" + "=" * 60)
    print("2ï¸âƒ£ Checking for perfect correlation with targets...")
    print("=" * 60)

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ç›¸é–¢ã‚’è¨ˆç®—ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
    sample_df = df.sample(min(100000, len(df)))
    numeric_features = [col for col in feature_cols if sample_df[col].dtype in ['float64', 'float32', 'int64', 'int32']]

    leaked_features = {}
    for target_col in target_cols:
        if target_col not in sample_df.columns:
            continue

        print(f"\nğŸ¯ Checking {target_col}...")

        high_corr_features = []
        target_values = sample_df[target_col].fillna(0)

        for feature_col in numeric_features:
            try:
                feature_values = sample_df[feature_col].fillna(0)
                # ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—
                corr = np.corrcoef(target_values, feature_values)[0, 1]

                # ç›¸é–¢ãŒç•°å¸¸ã«é«˜ã„ï¼ˆ0.99ä»¥ä¸Šï¼‰å ´åˆ
                if abs(corr) > 0.99:
                    high_corr_features.append((feature_col, corr))
            except:
                continue

        if high_corr_features:
            leaked_features[target_col] = high_corr_features
            print(f"   ğŸ”´ CRITICAL: Found {len(high_corr_features)} features with correlation > 0.99!")
            for feat, corr in high_corr_features[:5]:  # æœ€åˆã®5å€‹ã‚’è¡¨ç¤º
                print(f"      - {feat}: correlation = {corr:.4f}")

    # 3. åŒã˜å€¤ã‚’æŒã¤åˆ—ã®ãƒšã‚¢ã‚’æ¢ã™
    print("\n" + "=" * 60)
    print("3ï¸âƒ£ Checking for duplicate/identical columns...")
    print("=" * 60)

    # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ç‰¹å¾´é‡ã®å€¤ãŒå®Œå…¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
    for target_col in target_cols[:2]:  # æœ€åˆã®2ã¤ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã¿ãƒã‚§ãƒƒã‚¯ï¼ˆæ™‚é–“çŸ­ç¸®ï¼‰
        if target_col not in sample_df.columns:
            continue

        target_vals = sample_df[target_col].values
        for feature_col in numeric_features[:50]:  # æœ€åˆã®50ç‰¹å¾´é‡ã‚’ãƒã‚§ãƒƒã‚¯
            feature_vals = sample_df[feature_col].values

            # NaNã‚’é™¤ã„ã¦æ¯”è¼ƒ
            mask = ~(np.isnan(target_vals) | np.isnan(feature_vals))
            if mask.sum() > 0:
                if np.array_equal(target_vals[mask], feature_vals[mask]):
                    print(f"   ğŸ”´ IDENTICAL: {target_col} == {feature_col}")

    # 4. ç‰¹å¾´é‡é–“ã®ç›¸é–¢è¡Œåˆ—ã‚’ç¢ºèª
    print("\n" + "=" * 60)
    print("4ï¸âƒ£ Feature correlation matrix check...")
    print("=" * 60)

    # returnã‚„retã‚’å«ã‚€ç‰¹å¾´é‡ã®ç›¸é–¢ã‚’ç¢ºèª
    ret_features = [col for col in numeric_features if 'ret' in col.lower() or 'return' in col.lower()]

    if len(ret_features) > 0:
        print(f"\nFound {len(ret_features)} features with 'ret' or 'return' in name")
        print("Sample features:")
        for feat in ret_features[:10]:
            print(f"   - {feat}")

        # ã“ã‚Œã‚‰ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ç›¸é–¢ã‚’ç¢ºèª
        if len(ret_features) > 0 and len(target_cols) > 0:
            sample_size = min(10000, len(df))
            sample_df = df.sample(sample_size)

            for target_col in target_cols[:2]:  # æœ€åˆã®2ã¤ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
                if target_col in sample_df.columns:
                    target_vals = sample_df[target_col].fillna(0)

                    print(f"\nğŸ¯ Correlation with {target_col}:")
                    high_corr_count = 0

                    for feat in ret_features[:20]:  # æœ€åˆã®20å€‹
                        if feat in sample_df.columns:
                            feat_vals = sample_df[feat].fillna(0)
                            try:
                                corr = np.corrcoef(target_vals, feat_vals)[0, 1]
                                if abs(corr) > 0.5:  # ç›¸é–¢0.5ä»¥ä¸Šã‚’è¡¨ç¤º
                                    print(f"   {feat}: {corr:.4f}")
                                    if abs(corr) > 0.9:
                                        high_corr_count += 1
                            except:
                                pass

                    if high_corr_count > 0:
                        print(f"   ğŸ”´ WARNING: {high_corr_count} features have correlation > 0.9!")

    # 5. Normalization statistics check (newly added v3.0)
    check_normalization_stats(df, feature_cols, verbose=True)

    # 6. æœ€çµ‚è¨ºæ–­
    print("\n" + "=" * 60)
    print("ğŸ DIAGNOSIS")
    print("=" * 60)

    if leaked_features:
        print("\nğŸ”´ CRITICAL DATA LEAKAGE DETECTED!")
        print("\nåŸå› :")
        print("  1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã¨åŒã˜å€¤ã‚’æŒã¤ç‰¹å¾´é‡ãŒå­˜åœ¨")
        print("  2. returns_*d ãŒç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§")
        print("\nå¯¾ç­–:")
        print("  1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆæ™‚ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’é™¤å¤–")
        print("  2. ATFTç”¨ãƒ‡ãƒ¼ã‚¿å¤‰æ›æ™‚ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’åˆ†é›¢")
        print("  3. ç‰¹å¾´é‡é¸æŠã‚’è¦‹ç›´ã—")
        print("\nå½±éŸ¿:")
        print("  â†’ ã“ã‚ŒãŒåŸå› ã§ATFT-GAT-FANãŒå­¦ç¿’ã§ãã¦ã„ãªã„")
        print("  â†’ Val RankIC = 0.0719 ã®å›ºå®šå€¤ã«ãªã£ã¦ã„ã‚‹")
    else:
        print("\nğŸŸ¢ No obvious data leakage detected")
        print("But the high baseline RankIC (0.99+) suggests hidden leakage")
        print("Recommend manual review of feature engineering pipeline")

if __name__ == "__main__":
    detect_data_leakage()
