#!/usr/bin/env python
"""
å®Œå…¨ä»•æ§˜æº–æ‹ E2Eãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å…¨ä»•æ§˜è¦ä»¶ã‚’æº€ãŸã™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆâ†’MLå­¦ç¿’
"""

import sys
import os
import polars as pl
import numpy as np
from datetime import datetime, timedelta, date
import pytz
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), "./"))

# Import all specification-compliant modules
from src.gogooku3.features.ta_core import TechnicalIndicators, CrossSectionalNormalizer
from src.gogooku3.features.cross_features import CrossFeatures, MarketRegimeFeatures
from src.gogooku3.features.flow_features import FlowFeatures
from src.gogooku3.features.financial_features import FinancialFeatures
from src.gogooku3.utils.calendar_utils import JPXCalendar
from src.features.section_mapper import SectionMapper
from src.features.market_features import MarketFeaturesGenerator

# JST timezone
JST = pytz.timezone('Asia/Tokyo')

print("=" * 80)
print("å®Œå…¨ä»•æ§˜æº–æ‹  ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¤œè¨¼")
print("=" * 80)
print()

# ===============================
# STEP 1: ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
# ===============================
print("STEP 1: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ")
print("-" * 40)

def generate_sample_quotes(n_stocks=10, n_days=500):
    """ä»•æ§˜æº–æ‹ ã®ã‚µãƒ³ãƒ—ãƒ«æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    np.random.seed(42)
    
    stocks = [f"{1300+i}" for i in range(n_stocks)]
    calendar = JPXCalendar()
    
    # Use business days only
    start_date = date(2023, 1, 4)
    end_date = date(2024, 12, 30)
    business_days = calendar.get_business_days(start_date, end_date)[:n_days]
    
    data = []
    for stock in stocks:
        base_price = 1000 + np.random.rand() * 2000
        prices = []
        
        for i, bday in enumerate(business_days):
            if i == 0:
                price = base_price
            else:
                returns = np.random.randn() * 0.02
                price = prices[-1] * (1 + returns)
            prices.append(price)
            
            # OHLCV
            high = price * (1 + abs(np.random.randn() * 0.01))
            low = price * (1 - abs(np.random.randn() * 0.01))
            open_price = (high + low) / 2 + np.random.randn() * (high - low) * 0.1
            volume = int(1000000 * (1 + abs(np.random.randn())))
            
            data.append({
                "Code": stock,
                "Date": bday,
                "Open": open_price,
                "High": high,
                "Low": low,
                "Close": price,
                "Volume": volume,
                "TurnoverValue": price * volume,
                "SharesOutstanding": 100000000
            })
    
    return pl.DataFrame(data)

df_quotes = generate_sample_quotes(n_stocks=10, n_days=500)
print(f"âœ… ç”Ÿæˆå®Œäº†: {len(df_quotes)}è¡Œ")
print(f"   éŠ˜æŸ„æ•°: {df_quotes['Code'].n_unique()}")
print(f"   æœŸé–“: {df_quotes['Date'].min()} ã€œ {df_quotes['Date'].max()}")
print()

# ===============================
# STEP 2: Sectionä»˜ä¸ï¼ˆä»•æ§˜1æº–æ‹ ï¼‰
# ===============================
print("STEP 2: Sectionä»˜ä¸ï¼ˆå¸‚å ´åŒºåˆ†ï¼‰")
print("-" * 40)

# Add meta columns and section
df_quotes = df_quotes.with_columns([
    pl.col("Code").alias("meta_code"),
    pl.col("Date").alias("meta_date")
])

# Simulate section assignment (Prime/Standard/Growth)
sections = ["Prime", "Standard", "Growth"]
section_map = {}
for i, code in enumerate(df_quotes["Code"].unique()):
    section_map[code] = sections[i % 3]

df_quotes = df_quotes.with_columns(
    pl.col("Code").map_dict(section_map).alias("meta_section")
)

print(f"âœ… Sectionä»˜ä¸å®Œäº†")
print(f"   åŒºåˆ†: {df_quotes['meta_section'].value_counts()}")
print()

# ===============================
# STEP 3: ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ï¼ˆä»•æ§˜2æº–æ‹ ï¼‰
# ===============================
print("STEP 3: ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ç”Ÿæˆï¼ˆä»•æ§˜æº–æ‹ ï¼‰")
print("-" * 40)

result_dfs = []
for code in df_quotes["Code"].unique():
    code_df = df_quotes.filter(pl.col("Code") == code).sort("Date")
    
    # Apply all technical indicators (spec-compliant naming)
    code_df = TechnicalIndicators.add_returns(code_df, [1, 5, 10, 20, 60, 120])
    code_df = TechnicalIndicators.add_volatility(code_df, [5, 10, 20, 60])
    code_df = TechnicalIndicators.add_realized_volatility(code_df, 20, "parkinson")
    code_df = TechnicalIndicators.add_moving_averages(code_df, [5, 10, 20, 60, 200])
    code_df = TechnicalIndicators.add_price_ratios(code_df, [5, 20, 60])
    code_df = TechnicalIndicators.add_volume_indicators(code_df, [5, 20])
    code_df = TechnicalIndicators.add_rsi(code_df, [2, 14])
    code_df = TechnicalIndicators.add_macd(code_df)
    code_df = TechnicalIndicators.add_bollinger_bands(code_df)
    code_df = TechnicalIndicators.add_atr(code_df)
    code_df = TechnicalIndicators.add_adx(code_df)
    code_df = TechnicalIndicators.add_stochastic(code_df)
    code_df = TechnicalIndicators.add_targets(code_df, [1, 5, 10, 20])
    
    result_dfs.append(code_df)

df_with_ta = pl.concat(result_dfs)

# Remove px_ prefix per specification
rename_map = {}
for col in df_with_ta.columns:
    if col.startswith("px_"):
        # Remove px_ prefix for spec compliance
        new_name = col[3:]  # Remove "px_"
        rename_map[col] = new_name

df_with_ta = df_with_ta.rename(rename_map)

print(f"âœ… ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ç”Ÿæˆå®Œäº†: {len(df_with_ta.columns)}åˆ—")
print()

# ===============================
# STEP 4: å¸‚å ´ç‰¹å¾´é‡ï¼ˆä»•æ§˜3æº–æ‹ ï¼‰
# ===============================
print("STEP 4: TOPIXå¸‚å ´ç‰¹å¾´é‡ç”Ÿæˆ")
print("-" * 40)

# Create synthetic TOPIX data
market_df = df_quotes.group_by("Date").agg([
    pl.col("Close").mean().alias("Close"),
    pl.col("High").mean().alias("High"),
    pl.col("Low").mean().alias("Low"),
    pl.col("Volume").sum().alias("Volume")
]).sort("Date")

# Generate market features manually (MarketFeaturesGenerator has no generate_features method)
# Add market returns
for horizon in [1, 5, 10, 20]:
    market_df = market_df.with_columns(
        (pl.col("Close") / pl.col("Close").shift(horizon) - 1).alias(f"mkt_ret_{horizon}d")
    )

# Add market volatility
market_df = market_df.with_columns(
    (pl.col("mkt_ret_1d").rolling_std(20, min_periods=20) * np.sqrt(252)).alias("mkt_vol_20d")
)

# Add market regime indicators
market_df = market_df.with_columns([
    (pl.col("mkt_ret_20d") > 0).cast(pl.Int8).alias("mkt_bull_200"),
    (pl.col("mkt_vol_20d") > pl.col("mkt_vol_20d").median()).cast(pl.Int8).alias("mkt_high_vol")
])

market_features = market_df

print(f"âœ… å¸‚å ´ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(market_features.columns)}åˆ—")
print()

# ===============================
# STEP 5: ã‚¯ãƒ­ã‚¹ç‰¹å¾´é‡ï¼ˆä»•æ§˜4æº–æ‹ ï¼‰
# ===============================
print("STEP 5: ã‚¯ãƒ­ã‚¹ç‰¹å¾´é‡ç”Ÿæˆï¼ˆx_*ï¼‰")
print("-" * 40)

# Join market features first
df_panel = df_with_ta.join(
    market_features,
    left_on="meta_date",
    right_on="Date",
    how="left"
)

# Add cross features with beta lag
df_panel = CrossFeatures.add_cross_features(
    df_panel,
    beta_windows=[60],
    alpha_horizons=[1, 5, 10, 20]
)

# Add market regime features
df_panel = MarketRegimeFeatures.add_regime_features(df_panel)

# Add sector relative features
df_panel = CrossFeatures.add_sector_relative_features(df_panel)

print(f"âœ… ã‚¯ãƒ­ã‚¹ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
print(f"   x_* features: {len([c for c in df_panel.columns if c.startswith('x_')])}åˆ—")
print()

# ===============================
# STEP 6: ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ï¼ˆä»•æ§˜5æº–æ‹ ï¼‰
# ===============================
print("STEP 6: æŠ•è³‡ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ç”Ÿæˆï¼ˆflow_*ï¼‰")
print("-" * 40)

# Generate flow features (simulated for testing)
df_panel = FlowFeatures.generate_flow_features(df_panel)
df_panel = FlowFeatures.calculate_flow_impact_features(df_panel)

# Add calendar features first for seasonality
calendar = JPXCalendar()
df_panel = calendar.add_business_day_features(df_panel, "meta_date")
df_panel = FlowFeatures.add_flow_seasonality_features(df_panel)

print(f"âœ… ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
print(f"   flow_* features: {len([c for c in df_panel.columns if c.startswith('flow_')])}åˆ—")
print()

# ===============================
# STEP 7: è²¡å‹™ç‰¹å¾´é‡ï¼ˆä»•æ§˜6æº–æ‹ ï¼‰
# ===============================
print("STEP 7: è²¡å‹™è«¸è¡¨ç‰¹å¾´é‡ç”Ÿæˆï¼ˆstmt_*ï¼‰")
print("-" * 40)

# Generate financial features (simulated for testing)
df_panel = FinancialFeatures.generate_financial_features(df_panel)
df_panel = FinancialFeatures.add_financial_momentum_features(df_panel)

print(f"âœ… è²¡å‹™ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
print(f"   stmt_* features: {len([c for c in df_panel.columns if c.startswith('stmt_')])}åˆ—")
print()

# ===============================
# STEP 8: æ–­é¢æ­£è¦åŒ–ï¼ˆä»•æ§˜7æº–æ‹ ï¼‰
# ===============================
print("STEP 8: ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒŠãƒ«æ­£è¦åŒ–")
print("-" * 40)

# Select features for normalization (exclude meta, target, binary)
feature_cols = []
for col in df_panel.columns:
    if (not col.startswith(("meta_", "y_", "is_", "cal_")) and 
        col not in ["Code", "Date", "meta_section"] and
        df_panel[col].dtype in [pl.Float32, pl.Float64]):
        feature_cols.append(col)

print(f"æ­£è¦åŒ–å¯¾è±¡: {len(feature_cols)}åˆ—")

# Apply cross-sectional normalization with clip[-10,10]
df_panel = CrossSectionalNormalizer.normalize_daily(
    df_panel,
    feature_cols[:50],  # Limit for performance in test
    method="zscore",
    robust=True,
    winsorize_pct=0.01
)

print(f"âœ… æ­£è¦åŒ–å®Œäº†ï¼ˆZã‚¹ã‚³ã‚¢ã¯clip[-10,10]é©ç”¨æ¸ˆã¿ï¼‰")
print()

# ===============================
# STEP 9: æœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼ˆä»•æ§˜8æº–æ‹ ï¼‰
# ===============================
print("STEP 9: ãƒ‡ãƒ¼ã‚¿å“è³ªæœ€çµ‚ãƒã‚§ãƒƒã‚¯")
print("-" * 40)

# Check (Code, Date) uniqueness
duplicate_count = df_panel.group_by(["meta_code", "meta_date"]).count().filter(
    pl.col("count") > 1
).shape[0]

print(f"âœ… (Code, Date) ãƒ¦ãƒ‹ãƒ¼ã‚¯æ€§: {duplicate_count == 0} (é‡è¤‡: {duplicate_count})")

# Check for data leakage indicators
leakage_checks = {
    "Beta lag": "x_beta_60d" in df_panel.columns,
    "Flow days_since": any("days_since" in c for c in df_panel.columns),
    "As-of financial": "is_financial_data_valid" in df_panel.columns
}

for check, result in leakage_checks.items():
    print(f"âœ… {check}: {'å®Ÿè£…æ¸ˆã¿' if result else 'N/Aï¼ˆãƒ†ã‚¹ãƒˆï¼‰'}")

print()

# ===============================
# STEP 10: MLå­¦ç¿’ï¼ˆLightGBMï¼‰
# ===============================
print("STEP 10: MLå­¦ç¿’")
print("-" * 40)

# Prepare data
df_train = df_panel.filter(pl.col("y_5d").is_not_null())

# Select features (normalized + original important ones)
feature_cols = []
for col in df_train.columns:
    if (col.endswith("_z") or  # Normalized features
        col.startswith(("x_", "flow_", "stmt_", "mkt_")) or
        col in ["returns_1d", "volatility_20d", "volume_ratio_20d"]):
        if col not in ["meta_code", "meta_date", "meta_section"]:
            feature_cols.append(col)

# Remove features with too many NaNs
valid_features = []
for col in feature_cols:
    null_rate = df_train[col].null_count() / len(df_train)
    if null_rate < 0.5:
        valid_features.append(col)

print(f"æœ‰åŠ¹ç‰¹å¾´é‡: {len(valid_features)}/{len(feature_cols)}")

# Fill NaN
df_train = df_train.fill_null(0)

# Convert to numpy
X = df_train.select(valid_features[:100]).to_numpy()  # Limit features for test
y = df_train["y_5d"].to_numpy()

# Time-based split
dates_sorted = df_train["meta_date"].sort().unique()
split_idx = int(len(dates_sorted) * 0.8)
split_date = dates_sorted[split_idx]
train_mask = df_train["meta_date"] <= split_date
test_mask = df_train["meta_date"] > split_date

X_train = X[train_mask]
y_train = y[train_mask]
X_test = X[test_mask]
y_test = y[test_mask]

print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {X_train.shape}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}")

# Train LightGBM
print("\nLightGBMå­¦ç¿’ä¸­...")
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'verbose': -1,
    'seed': 42
}

train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

model = lgb.train(
    params,
    train_data,
    valid_sets=[valid_data],
    num_boost_round=50,
    callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
)

print(f"âœ… å­¦ç¿’å®Œäº†: {model.num_trees()}ãƒ©ã‚¦ãƒ³ãƒ‰")
print()

# ===============================
# STEP 11: è©•ä¾¡
# ===============================
print("STEP 11: ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
print("-" * 40)

y_pred = model.predict(X_test, num_iteration=model.best_iteration)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
corr = np.corrcoef(y_test, y_pred)[0, 1]

# Calculate IC
test_df = df_train.filter(test_mask).select(["meta_date", "meta_code"])
test_df = test_df.with_columns([
    pl.Series("y_true", y_test),
    pl.Series("y_pred", y_pred)
])

ic_per_date = []
for date in test_df["meta_date"].unique():
    date_df = test_df.filter(pl.col("meta_date") == date)
    if len(date_df) > 1:
        date_ic = np.corrcoef(date_df["y_true"], date_df["y_pred"])[0, 1]
        if not np.isnan(date_ic):
            ic_per_date.append(date_ic)

mean_ic = np.mean(ic_per_date) if ic_per_date else 0

print("ğŸ“Š è©•ä¾¡çµæœ:")
print(f"   RMSE: {rmse:.6f}")
print(f"   MAE:  {mae:.6f}")
print(f"   ç›¸é–¢: {corr:.4f}")
print(f"   å¹³å‡IC: {mean_ic:.4f}")
print()

# Feature importance
importance = model.feature_importance(importance_type='gain')
feature_imp = sorted(zip(valid_features[:100], importance), key=lambda x: x[1], reverse=True)

print("ğŸ“ˆ ä¸Šä½10ç‰¹å¾´é‡:")
for i, (feat, imp) in enumerate(feature_imp[:10], 1):
    print(f"   {i:2d}. {feat:40s}: {imp:10.2f}")

print()
print("=" * 80)
print("âœ… å®Œå…¨ä»•æ§˜æº–æ‹ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¤œè¨¼å®Œäº†!")
print("=" * 80)

# Summary
print("\nä»•æ§˜æº–æ‹ ã‚µãƒãƒªãƒ¼:")
print("  âœ… Sectionä»˜ä¸ãƒ»å¸‚å ´åŒºåˆ†")
print("  âœ… ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ï¼ˆpx_prefixå‰Šé™¤ï¼‰")
print("  âœ… TOPIXå¸‚å ´ç‰¹å¾´é‡ï¼ˆmkt_*ï¼‰")
print("  âœ… ã‚¯ãƒ­ã‚¹ç‰¹å¾´é‡ï¼ˆx_*, beta t-1 lagï¼‰")
print("  âœ… ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ï¼ˆflow_*ï¼‰")
print("  âœ… è²¡å‹™ç‰¹å¾´é‡ï¼ˆstmt_*, YoYå³å¯†å®Ÿè£…ï¼‰")
print("  âœ… æ–­é¢æ­£è¦åŒ–ï¼ˆZã‚¹ã‚³ã‚¢clip[-10,10]ï¼‰")
print("  âœ… æœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯æ€§ãƒ»ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰")
print(f"  âœ… MLå­¦ç¿’: IC={mean_ic:.4f}")

# Save results
output_dir = "output/full_spec_test"
os.makedirs(output_dir, exist_ok=True)

df_panel.write_parquet(f"{output_dir}/ml_panel_full_spec.parquet")
print(f"\nãƒ‡ãƒ¼ã‚¿ä¿å­˜: {output_dir}/ml_panel_full_spec.parquet")

model.save_model(f"{output_dir}/model_full_spec.txt")
print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {output_dir}/model_full_spec.txt")