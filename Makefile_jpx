# Makefile for Gogooku3 ML Pipeline
.PHONY: help setup data dataset train eval smoke clean docker-up docker-down

# Variables
PYTHON := python
PIP := pip
DAGSTER := dagster
START_DATE ?= 2024-01-01
END_DATE ?= 2024-12-31
CONFIG ?= configs/features/market.yaml
HORIZONS ?= 1,5,10,20

# Colors for output
RED := \033[0;31m
GREEN := \033[0;32m
YELLOW := \033[1;33m
NC := \033[0m # No Color

help: ## Show this help message
	@echo "$(GREEN)Gogooku3 ML Pipeline - Make Commands$(NC)"
	@echo ""
	@echo "Usage: make [target] [VARIABLE=value]"
	@echo ""
	@echo "Targets:"
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "  $(YELLOW)%-15s$(NC) %s\n", $$1, $$2}'
	@echo ""
	@echo "Variables:"
	@echo "  START_DATE    Start date for data processing (default: $(START_DATE))"
	@echo "  END_DATE      End date for data processing (default: $(END_DATE))"
	@echo "  CONFIG        Configuration file path (default: $(CONFIG))"
	@echo "  HORIZONS      Prediction horizons (default: $(HORIZONS))"
	@echo "  JQ=1         Use JQuants in wrappers (optional)"
	@echo "  TRADES_SPEC  Path to trades_spec parquet (optional)"
	@echo "  STATEMENTS   Path to statements parquet (optional)"

setup: ## Install dependencies and setup environment
	@echo "$(GREEN)Setting up environment...$(NC)"
	$(PIP) install -r requirements.txt
	$(PIP) install -e .
	@echo "$(GREEN)Environment setup complete!$(NC)"

data: ## Run Dagster pipeline to generate data assets
	@echo "$(GREEN)Running Dagster data pipeline...$(NC)"
	@echo "Date range: $(START_DATE) to $(END_DATE)"
	$(DAGSTER) job execute -j generate_ml_data \
		--config '{"ops": {"daily_quotes": {"config": {"start_date": "$(START_DATE)", "end_date": "$(END_DATE)"}}}}'

dataset: ## Build ML dataset using script
    @echo "$(GREEN)Building ML dataset (enhanced builder)...$(NC)"
    $(PYTHON) scripts/data/ml_dataset_builder.py

train: ## Train model with specified configuration
	@echo "$(GREEN)Training model...$(NC)"
	@echo "Config: $(CONFIG)"
	@echo "Horizons: $(HORIZONS)"
	$(PYTHON) pipelines/train.py \
		--config $(CONFIG) \
		--horizons $(HORIZONS) \
		--start-date $(START_DATE) \
		--end-date $(END_DATE)

eval: ## Evaluate trained model
	@echo "$(GREEN)Evaluating model...$(NC)"
	$(PYTHON) pipelines/evaluate.py \
		--model-path output/models/latest \
		--test-start $(START_DATE) \
		--test-end $(END_DATE)

smoke: ## Run smoke test
    @echo "$(YELLOW)Running smoke test...$(NC)"
    $(PYTHON) scripts/smoke_test.py
    @echo "$(GREEN)Smoke test complete!$(NC)"

test: ## Run unit tests
	@echo "$(GREEN)Running tests...$(NC)"
	$(PYTHON) -m pytest tests/ -v --cov=src

lint: ## Run code linting
    @echo "$(GREEN)Running linters...$(NC)"
    ruff check src/ scripts/orchestration/
    mypy src/ --ignore-missing-imports

format: ## Format code
    @echo "$(GREEN)Formatting code...$(NC)"
    ruff format src/ scripts/orchestration/
    isort src/ scripts/orchestration/

clean: ## Clean temporary files and caches
	@echo "$(YELLOW)Cleaning temporary files...$(NC)"
	find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
	find . -type f -name "*.pyc" -delete
	find . -type f -name ".DS_Store" -delete
	rm -rf .pytest_cache/
	rm -rf .mypy_cache/
	rm -rf .ruff_cache/
	@echo "$(GREEN)Clean complete!$(NC)"

docker-up: ## Start Docker services (MinIO, ClickHouse, etc.)
	@echo "$(GREEN)Starting Docker services...$(NC)"
	docker-compose up -d
	@echo "$(GREEN)Services started!$(NC)"

docker-down: ## Stop Docker services
	@echo "$(YELLOW)Stopping Docker services...$(NC)"
	docker-compose down
	@echo "$(GREEN)Services stopped!$(NC)"

dagster-dev: ## Start Dagster development server
	@echo "$(GREEN)Starting Dagster UI...$(NC)"
	$(DAGSTER) dev -f scripts/orchestration/repository.py

dataset-with-topix: ## Run base pipeline then add TOPIX features
	@echo "$(GREEN)Running base pipeline + TOPIX enrichment...$(NC)"
	$(PYTHON) scripts/pipelines/run_pipeline_with_topix.py \
		$(if $(JQ),--jquants,) \
		$(if $(START_DATE),--start-date $(START_DATE),) \
		$(if $(END_DATE),--end-date $(END_DATE),)
	@echo "$(GREEN)Done. See: output/ml_dataset_latest_with_topix.parquet$(NC)"

dataset-with-enrichment: ## Base + TOPIX + trade spec + statements
	@echo "$(GREEN)Running base pipeline + TOPIX + flow + statements...$(NC)"
	$(PYTHON) scripts/pipelines/run_pipeline_with_enrichment.py \
		$(if $(JQ),--jquants,) \
		$(if $(START_DATE),--start-date $(START_DATE),) \
		$(if $(END_DATE),--end-date $(END_DATE),) \
		$(if $(TRADES_SPEC),--trades-spec $(TRADES_SPEC),) \
		$(if $(STATEMENTS),--statements $(STATEMENTS),)
	@echo "$(GREEN)Done. See: output/ml_dataset_latest_enriched.parquet$(NC)"

notebook: ## Start Jupyter notebook for exploration
	@echo "$(GREEN)Starting Jupyter notebook...$(NC)"
	jupyter notebook --notebook-dir=notebooks/

profile: ## Profile dataset creation
    @echo "$(GREEN)Profiling dataset creation...$(NC)"
    $(PYTHON) -m cProfile -o output/profile.stats scripts/data/ml_dataset_builder.py
    $(PYTHON) -m pstats output/profile.stats

benchmark: ## Run performance benchmark
	@echo "$(GREEN)Running performance benchmark...$(NC)"
	@echo "Testing data loading speed..."
	@time $(PYTHON) -c "import polars as pl; df = pl.scan_parquet('output/datasets/*.parquet').collect(); print(f'Loaded {len(df)} rows')"
	@echo ""
	@echo "Testing feature calculation speed..."
	@time $(PYTHON) -c "from src.gogooku3.features.ta_core import TechnicalIndicators; import polars as pl; df = pl.DataFrame({'Close': [100.0]*10000, 'High': [105.0]*10000, 'Low': [95.0]*10000, 'Volume': [1000000]*10000}); df = TechnicalIndicators.add_all_indicators(df); print(f'Generated {len(df.columns)} features')"

# Advanced targets
parallel-train: ## Train multiple models in parallel
	@echo "$(GREEN)Training models in parallel...$(NC)"
	parallel -j 4 $(PYTHON) pipelines/train.py --horizon {} ::: 1 5 10 20

backtest: ## Run backtesting simulation
	@echo "$(GREEN)Running backtest...$(NC)"
	$(PYTHON) pipelines/backtest.py \
		--strategy momentum \
		--start-date $(START_DATE) \
		--end-date $(END_DATE) \
		--initial-capital 10000000

report: ## Generate analysis report
	@echo "$(GREEN)Generating report...$(NC)"
	$(PYTHON) pipelines/generate_report.py \
		--output output/reports/analysis_$(shell date +%Y%m%d).html

# Installation targets
install-dev: ## Install development dependencies
	$(PIP) install -r requirements-dev.txt
	pre-commit install

install-gpu: ## Install GPU dependencies
	$(PIP) install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Monitoring targets
monitor: ## Start monitoring dashboard
	@echo "$(GREEN)Starting monitoring dashboard...$(NC)"
	streamlit run dashboards/monitoring.py

tensorboard: ## Start TensorBoard
	@echo "$(GREEN)Starting TensorBoard...$(NC)"
	tensorboard --logdir=output/logs

# Deployment targets
build-docker: ## Build Docker image
	@echo "$(GREEN)Building Docker image...$(NC)"
	docker build -t gogooku3-ml:latest .

deploy-staging: ## Deploy to staging environment
	@echo "$(YELLOW)Deploying to staging...$(NC)"
	@echo "Not implemented yet"

deploy-prod: ## Deploy to production environment
	@echo "$(RED)Deploying to production...$(NC)"
	@echo "Not implemented yet"
