project:
  name: ATFT-GAT-FAN
  version: 1.0.0
  seed: 42
  deterministic: true
mode: train
debug:
  enabled: false
  fast_dev_run: 10
  profiler: false
  detect_anomaly: false
paths:
  root: ${hydra:runtime.cwd}
  data: ${paths.root}/data
  data_dir: ${paths.data}
  models: ${paths.root}/models
  logs: ${paths.root}/logs
  cache: ${paths.root}/cache
  experiments: ${paths.root}/experiments
logging:
  level: INFO
  mlflow:
    tracking_uri: ${oc.env:MLFLOW_TRACKING_URI,file://${paths.experiments}}
    experiment_name: ${project.name}
  wandb:
    enabled: ${oc.env:WANDB_ENABLED,false}
    project: ${project.name}
    entity: ${oc.env:WANDB_ENTITY,null}
data:
  data_source:
    type: jpx_parquet
    format: parquet
  source:
    data_dir: output/atft_data_sample_50000
    data_format: parquet
    use_cache: false
    cache_dir: ${paths.cache}/atft_data
  schema:
    date_column: date
    code_column: code
    target_column: target
    feature_columns: null
  time_series:
    sequence_length: 10
    prediction_horizons:
    - 1
    - 2
    - 3
    - 5
    - 10
    drop_historical_columns: false
    use_tensor_sequences: true
  split:
    method: test_only
    train_ratio: 0.7
    val_ratio: 0.2
    test_ratio: 1.0
    embargo_days: 5
  validation:
    check_missing: true
    missing_threshold: 0.5
    check_outliers: false
    check_target_distribution: false
  loader:
    shuffle: true
    drop_last: false
  normalization:
    online_normalization:
      enabled: true
      per_batch: true
    cross_sectional:
      enabled: false
    batch_norm:
      enabled: false
  graph_builder:
    enabled: true
    method: pearson
    k: 20
    threshold: 0.2
  use_day_batch_sampler: false
model:
  name: ATFT_GAT_FAN_v1
  architecture: atft_gat_fan
  hidden_size: 64
  input_projection:
    use_layer_norm: true
    dropout: 0.1
  input_dims:
    basic_features: 46
    historical_features: 260
    total_features: 306
  tft:
    lstm:
      layers: 1
      hidden_size: ${model.hidden_size}
      dropout: 0.1
      bidirectional: false
    attention:
      heads: 4
      dropout: 0.1
      use_scale: true
    variable_selection:
      hidden_size: ${model.hidden_size}
      dropout: 0.1
      use_sigmoid: true
      sparsity_coefficient: 0.01
    grn:
      hidden_size: ${model.hidden_size}
      dropout: 0.1
      use_layer_norm: true
    temporal:
      use_positional_encoding: true
      max_sequence_length: 20
  adaptive_normalization:
    fan:
      enabled: true
      window_sizes:
      - 5
      - 10
      - 20
      aggregation: weighted_mean
      learn_weights: true
    san:
      enabled: true
      num_slices: 3
      overlap: 0.5
      slice_aggregation: learned
  gat:
    enabled: true
    architecture:
      num_layers: 2
      hidden_channels:
      - ${model.hidden_size}
      - ${model.hidden_size}
      heads:
      - 4
      - 2
      concat:
      - true
      - false
    layer_config:
      dropout: 0.2
      edge_dropout: 0.1
      negative_slope: 0.2
      add_self_loops: false
      bias: true
    edge_features:
      use_edge_attr: true
      edge_dim: 3
      edge_projection: linear
    regularization:
      edge_weight_penalty: 0.01
      attention_entropy_penalty: 0.001
  prediction_head:
    architecture:
      hidden_layers:
      - 32
      activation: relu
      dropout: 0.2
      use_batch_norm: false
    output:
      point_prediction: true
      quantile_prediction:
        enabled: true
        quantiles:
        - 0.1
        - 0.25
        - 0.5
        - 0.75
        - 0.9
      distribution_prediction:
        enabled: false
        type: normal
      student_t: true
  optimization:
    compression:
      gradient_checkpointing: false
      mixed_precision: true
      channels_last: true
    compile:
      enabled: false
      mode: default
      fullgraph: false
      dynamic: false
    inference:
      use_torch_script: false
      use_onnx: false
      optimize_for_inference: true
  initialization:
    method: xavier_uniform
    gain: 1.0
  regularization:
    weight_decay: 0.0001
    gradient_clip_val: 1.0
    gradient_clip_algorithm: norm
    l2_lambda: 0.0001
  load_strict: false
  eval_mode: true
train:
  name: adaptive_training
  trainer:
    max_epochs: 1
    min_epochs: 10
    check_val_every_n_epoch: 1
    val_check_interval: 1.0
    gradient_clip_val: ${model.regularization.gradient_clip_val}
    gradient_clip_algorithm: ${model.regularization.gradient_clip_algorithm}
    accumulate_grad_batches: 2
    precision: 32
    enable_progress_bar: true
    deterministic: ${project.deterministic}
    benchmark: false
    profiler: ${debug.profiler}
    detect_anomaly: ${debug.detect_anomaly}
    accelerator: cpu
    devices: 1
  batch:
    train_batch_size: 512
    val_batch_size: 128
    test_batch_size: 128
    num_workers: 0
    pin_memory: false
    persistent_workers: false
    prefetch_factor: null
  optimizer:
    type: AdamW
    lr: 5.0e-05
    weight_decay: ${model.regularization.weight_decay}
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    warmup:
      enabled: true
      steps: 500
  scheduler:
    type: CosineAnnealingWarmRestarts
    T_0: 10
    T_mult: 2
    eta_min: 1.0e-06
  loss:
    main:
      type: quantile
      quantiles: ${model.prediction_head.output.quantile_prediction.quantiles}
    horizon_weights:
      1: 1.0
      2: 0.9
      3: 0.8
      5: 0.6
      10: 0.4
    auxiliary:
      sharpe_loss:
        enabled: true
        weight: 0.1
        min_periods: 10
      correlation_penalty:
        enabled: true
        weight: 0.05
        target_correlation: 0.3
      ranking_loss:
        enabled: true
        weight: 0.1
        margin: 0.1
  early_stopping:
    monitor: val/sharpe_ratio
    mode: max
    patience: 10
    min_delta: 0.001
  checkpoint:
    monitor: ${early_stopping.monitor}
    mode: ${early_stopping.mode}
    save_top_k: 3
    save_last: true
    filename: '{epoch:03d}-{val_sharpe_ratio:.4f}'
  logging:
    log_every_n_steps: 20
    log_grad_norm: true
    log_learning_rate: true
    metrics:
    - accuracy
    - precision
    - recall
    - sharpe_ratio
    - information_ratio
    - max_drawdown
    - hit_rate
    - profit_factor
    visualize:
      attention_weights: true
      feature_importance: true
      prediction_distribution: true
      portfolio_weights: true
  phase_training:
    enabled: true
    phases:
      baseline:
        epochs: 10
        freeze: []
        config_overrides:
          model.adaptive_normalization.fan.enabled: false
          model.adaptive_normalization.san.enabled: false
          model.gat.enabled: false
          data.wavelet.enabled: false
          data.augmentation.enabled: false
      adaptive_norm:
        epochs: 10
        freeze: []
        load_from: baseline
        config_overrides:
          model.adaptive_normalization.fan.enabled: true
          model.adaptive_normalization.san.enabled: true
          data.wavelet.enabled: true
      gat:
        epochs: 20
        freeze: []
        load_from: adaptive_norm
        config_overrides:
          model.gat.enabled: true
          data.graph.enabled: true
      augmentation:
        epochs: 10
        freeze: []
        load_from: gat
        config_overrides:
          data.augmentation.enabled: true
          optimizer.lr: 0.0001
experiment:
  name: atft_gat_fan_baseline
  seed: 42
  output_dir: output/experiments/atft_gat_fan
  expected_sharpe: 0.849
  enable_checkpointing: true
  checkpoint_interval: 5
  log_interval: 10
  tracking:
    mlflow:
      enabled: true
      experiment_name: ${experiment.name}
    wandb:
      enabled: ${oc.env:WANDB_ENABLED,false}
      project: ${experiment.name}
inference:
  batch_size: 2048
  num_workers: 8
  prefetch_factor: null
  pin_memory: true
  checkpoint_path: null
  use_ema: true
  output_dir: output/predictions
  save_raw_predictions: true
  save_aggregated: true
  use_amp: true
  amp_dtype: bf16
  calculate_metrics: true
  metrics:
  - IC
  - RankIC
  - Sharpe
  - Calmar
  - MaxDrawdown
  prediction_horizons:
  - 1
  - 5
  - 10
  - 20
  apply_ranking: true
  apply_sigmoid: false
  clip_predictions: true
  clip_range:
  - -3.0
  - 3.0
data_module:
  use_gpu_acceleration: true
hardware:
  device: ${oc.env:DEVICE, cuda}
  precision: bf16-mixed
  name: default
  gpu_id: ${oc.env:GPU_ID, 0}
  num_workers: ${oc.env:NUM_WORKERS, 8}
  pin_memory: ${oc.env:PIN_MEMORY, true}
  compile_model: ${oc.env:COMPILE_MODEL, false}
  mixed_precision: ${oc.env:MIXED_PRECISION, true}
  channels_last: ${oc.env:CHANNELS_LAST, true}
  a100_optimizations:
    tf32: true
    cudnn_benchmark: true
    gradient_checkpointing: false
  memory:
    max_memory_allocated: ${oc.env:MAX_MEMORY_ALLOCATED, 0.9}
    memory_fraction: ${oc.env:MEMORY_FRACTION, 0.8}
    allow_growth: ${oc.env:ALLOW_GROWTH, true}
  performance:
    prefetch_factor: ${oc.env:PREFETCH_FACTOR, 4}
    persistent_workers: ${oc.env:PERSISTENT_WORKERS, true}
    drop_last: ${oc.env:DROP_LAST, false}
