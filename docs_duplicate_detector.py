#!/usr/bin/env python3
"""
Markdownãƒ•ã‚¡ã‚¤ãƒ«é‡è¤‡æ¤œå‡ºãƒ„ãƒ¼ãƒ« - exact/near/topicé‡è¤‡ã‚’æ¤œå‡º
"""

import csv
import re
import hashlib
from pathlib import Path
from difflib import SequenceMatcher
from collections import defaultdict
import argparse
import json


def normalize_text(content):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ï¼ˆè¦‹å‡ºã—/ã‚³ãƒ¼ãƒ‰/ãƒªãƒ³ã‚¯é™¤å¤–ï¼‰"""
    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é™¤å»
    content = re.sub(r'```.*?```', '', content, flags=re.DOTALL)
    # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰é™¤å»  
    content = re.sub(r'`[^`]*`', '', content)
    # Markdownãƒªãƒ³ã‚¯é™¤å»
    content = re.sub(r'\[([^\]]*)\]\([^\)]*\)', r'\1', content)
    # è¦‹å‡ºã—è¨˜å·é™¤å»
    content = re.sub(r'^#+\s*', '', content, flags=re.MULTILINE)
    # æ”¹è¡Œãƒ»ç©ºç™½æ­£è¦åŒ–
    content = ' '.join(content.split())
    return content.lower()


def extract_keywords(content, title):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»è¦‹å‡ºã—ãƒ»é‡è¦èªï¼‰"""
    keywords = set()
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰
    keywords.update(re.findall(r'\b[a-zA-Z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]{3,}\b', title.lower()))
    
    # è¦‹å‡ºã—ã‹ã‚‰
    headings = re.findall(r'^#+\s*(.+)$', content, re.MULTILINE)
    for heading in headings:
        keywords.update(re.findall(r'\b[a-zA-Z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]{3,}\b', heading.lower()))
    
    # é‡è¦ãã†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæŠ€è¡“ç”¨èªï¼‰
    tech_words = re.findall(r'\b(gogooku|atft|gat|fan|jquants|dagster|mlflow|feast|clickhouse|minio|redis|postgresql|docker|make|training|pipeline|model)\b', content.lower())
    keywords.update(tech_words)
    
    return keywords


def detect_exact_duplicates(file_data):
    """å®Œå…¨é‡è¤‡æ¤œå‡ºï¼ˆSHA256ãƒ™ãƒ¼ã‚¹ï¼‰"""
    duplicates = []
    hash_groups = defaultdict(list)
    
    for item in file_data:
        hash_groups[item['sha256']].append(item)
    
    group_id = 1
    for hash_val, files in hash_groups.items():
        if len(files) > 1:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã§å„ªå…ˆåº¦æ±ºå®šï¼ˆå¤§ãã„æ–¹ã‚’å„ªå…ˆï¼‰
            files.sort(key=lambda x: x['size'], reverse=True)
            preferred = files[0]['path']
            
            for i in range(len(files)):
                for j in range(i + 1, len(files)):
                    duplicates.append({
                        'group_id': f'exact_{group_id}',
                        'type': 'exact-duplicate',
                        'path_a': files[i]['path'],
                        'path_b': files[j]['path'],
                        'similarity': 1.0,
                        'preferred': preferred
                    })
            group_id += 1
    
    return duplicates


def detect_near_duplicates(file_data, threshold=0.88):
    """è¿‘ä¼¼é‡è¤‡æ¤œå‡ºï¼ˆæ­£è¦åŒ–ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰"""
    duplicates = []
    group_id = 1
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã‚“ã§æ­£è¦åŒ–
    normalized_content = {}
    for item in file_data:
        try:
            with open(item['path'], 'r', encoding='utf-8') as f:
                content = f.read()
                normalized_content[item['path']] = normalize_text(content)
        except:
            normalized_content[item['path']] = ""
    
    paths = list(file_data)
    for i in range(len(paths)):
        for j in range(i + 1, len(paths)):
            path_a, path_b = paths[i]['path'], paths[j]['path']
            content_a = normalized_content.get(path_a, "")
            content_b = normalized_content.get(path_b, "")
            
            if content_a and content_b and len(content_a) > 100 and len(content_b) > 100:
                similarity = SequenceMatcher(None, content_a, content_b).ratio()
                
                if similarity >= threshold:
                    # ã‚µã‚¤ã‚ºã§å„ªå…ˆåº¦æ±ºå®š
                    preferred = path_a if paths[i]['size'] >= paths[j]['size'] else path_b
                    
                    duplicates.append({
                        'group_id': f'near_{group_id}',
                        'type': 'near-duplicate',
                        'path_a': path_a,
                        'path_b': path_b,
                        'similarity': round(similarity, 3),
                        'preferred': preferred
                    })
                    group_id += 1
    
    return duplicates


def detect_topic_duplicates(file_data, threshold=0.8):
    """ãƒˆãƒ”ãƒƒã‚¯é‡è¤‡æ¤œå‡ºï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»è¦‹å‡ºã—ãƒ™ãƒ¼ã‚¹ï¼‰"""
    duplicates = []
    group_id = 1
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    file_keywords = {}
    for item in file_data:
        try:
            with open(item['path'], 'r', encoding='utf-8') as f:
                content = f.read()
                keywords = extract_keywords(content, item['title'])
                file_keywords[item['path']] = keywords
        except:
            file_keywords[item['path']] = set()
    
    paths = list(file_data)
    for i in range(len(paths)):
        for j in range(i + 1, len(paths)):
            path_a, path_b = paths[i]['path'], paths[j]['path']
            keywords_a = file_keywords.get(path_a, set())
            keywords_b = file_keywords.get(path_b, set())
            
            if keywords_a and keywords_b:
                intersection = keywords_a & keywords_b
                union = keywords_a | keywords_b
                
                if union:
                    similarity = len(intersection) / len(union)
                    
                    if similarity >= threshold:
                        # æœ€è¿‘æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆ
                        preferred = path_a if paths[i]['last_commit_date'] >= paths[j]['last_commit_date'] else path_b
                        
                        duplicates.append({
                            'group_id': f'topic_{group_id}',
                            'type': 'topic-duplicate',
                            'path_a': path_a,
                            'path_b': path_b,
                            'similarity': round(similarity, 3),
                            'preferred': preferred
                        })
                        group_id += 1
    
    return duplicates


def detect_multilingual_pairs(file_data):
    """å¤šè¨€èªãƒšã‚¢æ¤œå‡ºï¼ˆja/enãƒšã‚¢ï¼‰"""
    pairs = []
    group_id = 1
    
    # è¨€èªåˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    ja_files = [item for item in file_data if item['lang'] == 'ja']
    en_files = [item for item in file_data if item['lang'] == 'en']
    
    for ja_file in ja_files:
        for en_file in en_files:
            # ãƒ‘ã‚¹é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
            ja_path = Path(ja_file['path'])
            en_path = Path(en_file['path'])
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åé¡ä¼¼åº¦
            ja_stem = ja_path.stem.lower()
            en_stem = en_path.stem.lower()
            
            # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ï¼ˆè‹±æ•°å­—ã®ã¿ã§æ¯”è¼ƒï¼‰
            ja_title_norm = re.sub(r'[^\w\s]', '', ja_file['title']).lower()
            en_title_norm = re.sub(r'[^\w\s]', '', en_file['title']).lower()
            
            # é¡ä¼¼åº¦è¨ˆç®—
            name_sim = SequenceMatcher(None, ja_stem, en_stem).ratio()
            title_sim = SequenceMatcher(None, ja_title_norm, en_title_norm).ratio()
            
            if name_sim > 0.7 or title_sim > 0.6:
                pairs.append({
                    'group_id': f'multilang_{group_id}',
                    'type': 'multilang-pair',
                    'path_a': ja_file['path'],
                    'path_b': en_file['path'],
                    'similarity': round(max(name_sim, title_sim), 3),
                    'preferred': 'both'  # å¤šè¨€èªãƒšã‚¢ã¯ä¸¡æ–¹ä¿æŒ
                })
                group_id += 1
    
    return pairs


def load_inventory(csv_path):
    """æ£šå¸ã—CSVã‚’èª­ã¿è¾¼ã¿"""
    file_data = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            row['size'] = int(row['size'])
            file_data.append(row)
    return file_data


def save_duplicates_csv(duplicates, output_path):
    """é‡è¤‡æ¤œå‡ºçµæœã‚’CSVã«ä¿å­˜"""
    fieldnames = ['group_id', 'type', 'path_a', 'path_b', 'similarity', 'preferred']
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(duplicates)
    
    print(f"ğŸ“‹ é‡è¤‡æ¤œå‡ºçµæœã‚’ {output_path} ã«ä¿å­˜")


def main():
    parser = argparse.ArgumentParser(description='Markdowné‡è¤‡æ¤œå‡ºãƒ„ãƒ¼ãƒ«')
    parser.add_argument('--inventory', default='docs_inventory.csv', help='æ£šå¸ã—CSVãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--out', default='md_duplicates.csv', help='é‡è¤‡æ¤œå‡ºçµæœCSV')
    
    args = parser.parse_args()
    
    print(f"ğŸ“‚ æ£šå¸ã—çµæœ {args.inventory} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    file_data = load_inventory(args.inventory)
    
    print("ğŸ” é‡è¤‡æ¤œå‡ºã‚’å®Ÿè¡Œä¸­...")
    all_duplicates = []
    
    # å®Œå…¨é‡è¤‡æ¤œå‡º
    exact_dups = detect_exact_duplicates(file_data)
    all_duplicates.extend(exact_dups)
    print(f"   å®Œå…¨é‡è¤‡: {len(exact_dups)} ãƒšã‚¢")
    
    # è¿‘ä¼¼é‡è¤‡æ¤œå‡º
    near_dups = detect_near_duplicates(file_data)
    all_duplicates.extend(near_dups)
    print(f"   è¿‘ä¼¼é‡è¤‡: {len(near_dups)} ãƒšã‚¢")
    
    # ãƒˆãƒ”ãƒƒã‚¯é‡è¤‡æ¤œå‡º
    topic_dups = detect_topic_duplicates(file_data)
    all_duplicates.extend(topic_dups)
    print(f"   ãƒˆãƒ”ãƒƒã‚¯é‡è¤‡: {len(topic_dups)} ãƒšã‚¢")
    
    # å¤šè¨€èªãƒšã‚¢æ¤œå‡º
    multilang_pairs = detect_multilingual_pairs(file_data)
    all_duplicates.extend(multilang_pairs)
    print(f"   å¤šè¨€èªãƒšã‚¢: {len(multilang_pairs)} ãƒšã‚¢")
    
    save_duplicates_csv(all_duplicates, args.out)
    
    print(f"\nğŸ“Š é‡è¤‡æ¤œå‡ºã‚µãƒãƒªãƒ¼:")
    print(f"   ç·æ¤œå‡ºãƒšã‚¢æ•°: {len(all_duplicates)}")
    type_counts = defaultdict(int)
    for dup in all_duplicates:
        type_counts[dup['type']] += 1
    for dup_type, count in type_counts.items():
        print(f"   {dup_type}: {count}")


if __name__ == '__main__':
    main()