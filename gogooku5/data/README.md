# gogooku5 Dataset Builder (Skeleton)

This package will host the standalone dataset generation pipeline described in `../MIGRATION_PLAN.md`.

## Roadmap
1. Implement configuration and API clients under `src/builder/api/`.
2. Migrate feature engineering modules (`core`, `legacy`, `macro`).
3. Introduce full and optimized pipelines under `src/builder/pipelines/` with caching support.
4. Deliver integration tests that rebuild one month of data and compare with `gogooku3` outputs.

## Usage
```bash
# Ensure environment variables are set
cp .env.example .env

# Build 30-day dataset (START/END default to last 30 days)
make build

# Override date range
make build START=2024-01-01 END=2024-01-31

# Warm caches without writing parquet output
make build-optimized START=2024-01-01 END=2024-01-31 CACHE_ONLY=1
```

## Output Artifacts
- Timestamped parquet + metadata pairs are written under `output/` using the pattern
  `ml_dataset_{YYYYMMDDYYYYMMDD}_{timestamp}_full.parquet`.
- Symlinks `ml_dataset_latest.parquet` and `ml_dataset_latest_full.parquet` always point to the newest dataset.
- Metadata is persisted alongside the parquet (`*_metadata.json`) with `ml_dataset_latest_metadata.json` tracking the latest snapshot.
- Retention (default: keep 3 snapshots) and compression (`zstd` by default) are configurable via `.env`.

## Testing
```bash
# Run data package tests (requires using the package source path)
PYTHONPATH=gogooku5/data/src pytest gogooku5/data/tests -q
```

## Data Fetchers
- `builder.api.advanced_fetcher.AdvancedJQuantsFetcher` wraps legacy async clients so gogooku5 can download TOPIX, indices, trades spec, margin (daily/weekly), futures, options, short sellingã€dividendsã€earning statements, etc.
- `data/tools/clean_empty_cache.py` can purge stale cache files (0-row Parquet/IPC) before a new build. Run `python data/tools/clean_empty_cache.py --dry-run` to inspect and without `--dry-run` to delete.
- Use `AdvancedJQuantsFetcher` together with `builder.utils.asyncio.run_sync` when integrating new pipelines to avoid manual event loop handling.
- `builder.features.core.flow.enhanced.FlowFeatureEngineer` converts cached trades-spec data into flow metrics (`foreign_sentiment`, `smart_flow_indicator`, etc.), driven by `DataSourceManager.trades_spec()`.
- Parity check CLI: `python scripts/compare_parity.py <gogooku3 parquet> <gogooku5 parquet> [--output-json report.json]` to inspect schema and numeric differences. For automated runs, set `PARITY_BASELINE_PATH=/path/to/gogooku3_parquet` (and optionally `PARITY_CANDIDATE_PATH=/path/to/gogooku5_parquet`) before `python tools/project-health-check.sh`.
- DatasetBuilder now materialises a fullå–¶æ¥­æ—¥Ã—éŠ˜æŸ„ã‚°ãƒªãƒƒãƒ‰ï¼ˆæ—¥æœ¬ã®ç¥æ—¥ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ï¼‹å®Ÿè¦³æ¸¬æ—¥ä»˜ï¼‰ã‚’ãƒ™ãƒ¼ã‚¹ã«å„ç‰¹å¾´é‡ã‚’ä»˜ä¸ã—ã€æ¬ ææ—¥ã®å¯è¦–åŒ–ã¨ gogooku3 ã¨ã®ãƒ‘ãƒªãƒ†ã‚£ç¢ºèªã‚’å®¹æ˜“ã«ã—ã¦ã„ã¾ã™ã€‚

### Source cache controls

APIã‚½ãƒ¼ã‚¹ï¼ˆè²¡å‹™ãƒ»é…å½“ãƒ»ç©ºå£²ã‚Šãƒ»ãƒãƒ¼ã‚¸ãƒ³ãƒ»æ±ºç®—ãªã©ï¼‰ã¯ `output/cache` ã«ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã•ã‚Œã¾ã™ã€‚ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•° or Dagster resource è¨­å®šã§æŒ™å‹•ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¾ã™ã€‚

| è¨­å®š | èª¬æ˜ |
| --- | --- |
| `SOURCE_CACHE_MODE` | `read_write` (æ—¢å®š) / `read` / `off` |
| `SOURCE_CACHE_FORCE_REFRESH` | `true` ã§ TTL ã‚’ç„¡è¦–ã—ã¦å¸¸ã« API ã‹ã‚‰å†å–å¾— |
| `SOURCE_CACHE_ASOF` | `YYYY-MM-DD` ã‚„ `today`ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã« `asof-<date>` ãŒä»˜ä¸ã•ã‚Œã€åŒä¸€ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å†åˆ©ç”¨å¯èƒ½ |
| `SOURCE_CACHE_TAG` | ä»»æ„ã‚¿ã‚°ï¼ˆä¾‹ `backfill`ï¼‰ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã¨ãƒ¡ã‚¿æƒ…å ±ã«è¨˜éŒ² |
| `SOURCE_CACHE_TTL_OVERRIDE_DAYS` | ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ã”ã¨ã® TTL ã‚’ã¾ã¨ã‚ã¦ä¸Šæ›¸ã |

Dagster ã§ã¯ `dataset_builder` resource ã« `source_cache_*` ã‚’æ¸¡ã™ã“ã¨ã§ run å˜ä½ã§ã“ã‚Œã‚‰ã‚’æŒ‡å®šã§ãã¾ã™ã€‚

Detailed pipeline behavior, feature coverage, and validation routines will be documented as implementation progresses through the migration milestones.

## Dagster Integration
`gogooku5/data/src/dagster_gogooku5` ships reusable Dagster assets that wrap the dataset builder:

```bash
# Launch Dagster UI with the gogooku5 definitions
export DAGSTER_HOME=/workspace/gogooku3/gogooku5   # use absolute path
PYTHONPATH=gogooku5/data/src dagster dev -m dagster_gogooku5.defs
```

- `g5_dataset_chunks`: builds DatasetBuilder chunks for a configurable date range. Configure `start`, `end`, `chunk_months`, etc. directly in Dagster.
- `g5_dataset_full`: merges the latest completed chunks by invoking the existing `data/tools/merge_chunks.py` helper.
- `dataset_builder_resource`: initializes `DatasetBuilder` with optional overrides (output dir, dataset tag, refresh behavior).

These assets allow you to schedule recurring dataset builds via Dagster jobs or run adâ€‘hoc chunk builds/merges from the UI with full observability.

> ğŸ•’ **Timezone**
> Export `TZ=Asia/Tokyo` together with `DAGSTER_HOME=/absolute/path/to/gogooku5` ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰ before running `dagster dev` / `dagster job â€¦` to keep Dagster timestamps in JST.
