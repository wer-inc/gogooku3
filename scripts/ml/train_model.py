"""
é‡‘èMLãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å¤§è¦æ¨¡æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ3,977éŠ˜æŸ„Ã—5å¹´ï¼‰ã‚’ä½¿ç”¨ã—ãŸLightGBMãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’
"""

import logging
import warnings
from datetime import datetime
from pathlib import Path

import lightgbm as lgb
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import RobustScaler

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FinancialMLTrainer:
    """é‡‘èæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å‘ã‘MLå­¦ç¿’ã‚¯ãƒ©ã‚¹"""

    def __init__(self, data_path: str):
        """åˆæœŸåŒ–"""
        self.data_path = data_path
        self.df = None
        self.model = None
        self.scaler = None
        self.feature_cols = None
        self.results = {}

    def load_and_preprocess_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        logger.info("ğŸ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚’é–‹å§‹...")

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.df = pd.read_parquet(self.data_path)
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df):,}è¡Œ Ã— {len(self.df.columns)}åˆ—")

        # æ—¥ä»˜ã‚½ãƒ¼ãƒˆ
        self.df = self.df.sort_values(['date', 'Code']).reset_index(drop=True)

        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ å®šç¾©
        self.feature_cols = [col for col in self.df.columns
                           if col not in ['Code', 'date', 'target', 'year', 'month']]

        logger.info(f"ğŸ”§ ç‰¹å¾´é‡æ•°: {len(self.feature_cols)}")
        logger.info(f"ğŸ“‹ ç‰¹å¾´é‡: {self.feature_cols}")

        # å‰å‡¦ç†
        self._preprocess_features()

        # ç›®çš„å¤‰æ•°ã®å¤–ã‚Œå€¤å‡¦ç†
        self._process_target_outliers()

        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†")
        return self.df

    def _preprocess_features(self):
        """ç‰¹å¾´é‡ã®å‰å‡¦ç†"""
        logger.info("ğŸ”§ ç‰¹å¾´é‡å‰å‡¦ç†ã‚’é–‹å§‹...")

        # æ¬ æå€¤è£œå®Œï¼ˆå‰å€¤è£œå®Œï¼‰
        for col in self.feature_cols:
            if self.df[col].isnull().any():
                null_count = self.df[col].isnull().sum()
                self.df[col] = self.df[col].fillna(method='ffill')
                logger.info(f"   {col}: {null_count}ä»¶ã®æ¬ æå€¤ã‚’è£œå®Œ")

        # ç„¡é™å€¤ã®å‡¦ç†
        for col in self.feature_cols:
            inf_mask = np.isinf(self.df[col])
            if inf_mask.any():
                inf_count = inf_mask.sum()
                self.df.loc[inf_mask, col] = np.nan
                self.df[col] = self.df[col].fillna(method='ffill')
                logger.info(f"   {col}: {inf_count}ä»¶ã®ç„¡é™å€¤ã‚’è£œå®Œ")

        # ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆéŠ˜æŸ„ã”ã¨ã«ï¼‰
        logger.info("ğŸ“Š ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’é–‹å§‹...")
        self.scaler = RobustScaler()  # å¤–ã‚Œå€¤ã«å¼·ã„ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼

        # éŠ˜æŸ„ã”ã¨ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaled_features = []
        for code in self.df['Code'].unique():
            stock_data = self.df[self.df['Code'] == code].copy()
            if len(stock_data) > 1:
                scaled_values = self.scaler.fit_transform(stock_data[self.feature_cols])
                scaled_df = pd.DataFrame(scaled_values, columns=self.feature_cols,
                                       index=stock_data.index)
                scaled_features.append(scaled_df)

        if scaled_features:
            scaled_features_df = pd.concat(scaled_features)
            self.df[self.feature_cols] = scaled_features_df

        logger.info("âœ… ç‰¹å¾´é‡å‰å‡¦ç†å®Œäº†")

    def _process_target_outliers(self):
        """ç›®çš„å¤‰æ•°ã®å¤–ã‚Œå€¤å‡¦ç†"""
        logger.info("ğŸ¯ ç›®çš„å¤‰æ•°å¤–ã‚Œå€¤å‡¦ç†ã‚’é–‹å§‹...")

        # æ¥µç«¯ãªå€¤ã®çµ±è¨ˆ
        original_mean = self.df['target'].mean()
        original_std = self.df['target'].std()

        # 3Ïƒä»¥ä¸Šã®å€¤ã‚’ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆãŸã ã—Â±20%ä»¥å†…ã«åˆ¶é™ï¼‰
        upper_limit = min(0.20, original_mean + 3 * original_std)
        lower_limit = max(-0.20, original_mean - 3 * original_std)

        outlier_mask = (self.df['target'] > upper_limit) | (self.df['target'] < lower_limit)
        outlier_count = outlier_mask.sum()

        if outlier_count > 0:
            logger.info(f"   å¤–ã‚Œå€¤æ¤œå‡º: {outlier_count:,}ä»¶ ({outlier_count/len(self.df)*100:.2f}%)")
            self.df.loc[outlier_mask, 'target'] = np.clip(
                self.df.loc[outlier_mask, 'target'], lower_limit, upper_limit
            )

        processed_mean = self.df['target'].mean()
        processed_std = self.df['target'].std()

        logger.info(f"   å‡¦ç†å¾Œ - å¹³å‡: {processed_mean:.6f}, æ¨™æº–åå·®: {processed_std:.6f}")
        logger.info("âœ… ç›®çš„å¤‰æ•°å‡¦ç†å®Œäº†")

    def create_time_series_splits(self, n_splits=5):
        """æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²"""
        logger.info("ğŸ“… æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å‰²ã‚’ä½œæˆ...")

        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        self.df = self.df.sort_values('date').reset_index(drop=True)

        # æ™‚ç³»åˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=n_splits)

        splits = []
        dates = self.df['date'].unique()

        for i, (train_index, test_index) in enumerate(tscv.split(dates)):
            train_dates = dates[train_index]
            test_dates = dates[test_index]

            train_mask = self.df['date'].isin(train_dates)
            test_mask = self.df['date'].isin(test_dates)

            X_train = self.df.loc[train_mask, self.feature_cols]
            y_train = self.df.loc[train_mask, 'target']
            X_test = self.df.loc[test_mask, self.feature_cols]
            y_test = self.df.loc[test_mask, 'target']

            splits.append({
                'fold': i + 1,
                'X_train': X_train,
                'y_train': y_train,
                'X_test': X_test,
                'y_test': y_test,
                'train_dates': (train_dates.min(), train_dates.max()),
                'test_dates': (test_dates.min(), test_dates.max())
            })

            logger.info(f"   Fold {i+1}: å­¦ç¿’={len(X_train):,}, ãƒ†ã‚¹ãƒˆ={len(X_test):,}")

        return splits

    def train_lightgbm_model(self, splits):
        """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
        logger.info("ğŸš€ LightGBMãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’é–‹å§‹...")

        # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé‡‘èæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å‘ã‘ï¼‰
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': 42
        }

        fold_results = []

        for split in splits:
            logger.info(f"ğŸ“Š Fold {split['fold']} å­¦ç¿’é–‹å§‹...")

            # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            train_data = lgb.Dataset(split['X_train'], label=split['y_train'])
            valid_data = lgb.Dataset(split['X_test'], label=split['y_test'], reference=train_data)

            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆLightGBM 4.xå¯¾å¿œï¼‰
            model = lgb.train(
                params,
                train_data,
                num_boost_round=1000,
                valid_sets=[train_data, valid_data],
                valid_names=['train', 'valid'],
                callbacks=[
                    lgb.early_stopping(stopping_rounds=50),
                    lgb.log_evaluation(period=0)  # verbose_eval=Falseç›¸å½“
                ]
            )

            # äºˆæ¸¬
            y_pred = model.predict(split['X_test'], num_iteration=model.best_iteration)

            # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
            metrics = self._calculate_metrics(split['y_test'], y_pred)

            fold_result = {
                'fold': split['fold'],
                'model': model,
                'metrics': metrics,
                'train_dates': split['train_dates'],
                'test_dates': split['test_dates']
            }

            fold_results.append(fold_result)

            logger.info(f"   Fold {split['fold']} å®Œäº† - RMSE: {metrics['rmse']:.6f}")

        # æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆæœ€å¾Œã®Foldã‚’ä½¿ç”¨ï¼‰
        self.model = fold_results[-1]['model']

        return fold_results

    def _calculate_metrics(self, y_true, y_pred):
        """è©•ä¾¡æŒ‡æ¨™è¨ˆç®—"""
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)

        # é‡‘èç‰¹åŒ–æŒ‡æ¨™
        returns = y_true

        # Sharpeæ¯”ï¼ˆå˜ç´”åŒ–ï¼‰
        if len(returns) > 1 and returns.std() > 0:
            sharpe = returns.mean() / returns.std() * np.sqrt(252)  # å¹´é–“åŒ–
        else:
            sharpe = 0

        # æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³
        cumulative_returns = (1 + returns).cumprod()
        running_max = cumulative_returns.expanding().max()
        drawdown = (cumulative_returns - running_max) / running_max
        max_drawdown = drawdown.min()

        # å‹ç‡
        win_rate = (np.sign(y_pred) == np.sign(y_true)).mean()

        return {
            'mse': mse,
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'sharpe': sharpe,
            'max_drawdown': max_drawdown,
            'win_rate': win_rate,
            'sample_size': len(y_true)
        }

    def save_model_and_results(self, fold_results):
        """ãƒ¢ãƒ‡ãƒ«ã¨çµæœã®ä¿å­˜"""
        logger.info("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜...")

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_dir = Path('models') / f'ml_model_{timestamp}'
        model_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = model_dir / 'lightgbm_model.txt'
        self.model.save_model(str(model_path))
        logger.info(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜
        scaler_path = model_dir / 'scaler.pkl'
        import joblib
        joblib.dump(self.scaler, scaler_path)
        logger.info(f"   ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜: {scaler_path}")

        # ç‰¹å¾´é‡æƒ…å ±ä¿å­˜
        feature_info = {
            'feature_columns': self.feature_cols,
            'n_features': len(self.feature_cols),
            'n_stocks': self.df['Code'].nunique(),
            'date_range': (self.df['date'].min(), self.df['date'].max()),
            'total_samples': len(self.df)
        }

        feature_path = model_dir / 'feature_info.json'
        import json
        with open(feature_path, 'w', encoding='utf-8') as f:
            json.dump(feature_info, f, indent=2, default=str)
        logger.info(f"   ç‰¹å¾´é‡æƒ…å ±ä¿å­˜: {feature_path}")

        # è©•ä¾¡çµæœä¿å­˜
        results_df = pd.DataFrame([{
            'fold': r['fold'],
            'rmse': r['metrics']['rmse'],
            'mae': r['metrics']['mae'],
            'r2': r['metrics']['r2'],
            'sharpe': r['metrics']['sharpe'],
            'max_drawdown': r['metrics']['max_drawdown'],
            'win_rate': r['metrics']['win_rate'],
            'train_start': r['train_dates'][0],
            'train_end': r['train_dates'][1],
            'test_start': r['test_dates'][0],
            'test_end': r['test_dates'][1]
        } for r in fold_results])

        results_path = model_dir / 'evaluation_results.csv'
        results_df.to_csv(results_path, index=False)
        logger.info(f"   è©•ä¾¡çµæœä¿å­˜: {results_path}")

        # é›†è¨ˆçµæœè¡¨ç¤º
        print("\nğŸ¯ å­¦ç¿’çµæœã‚µãƒãƒªãƒ¼:")
        print(f"   å¹³å‡RMSE: {results_df['rmse'].mean():.6f}")
        print(f"   å¹³å‡MAE: {results_df['mae'].mean():.6f}")
        print(f"   å¹³å‡RÂ²: {results_df['r2'].mean():.6f}")
        print(f"   å¹³å‡å‹ç‡: {results_df['win_rate'].mean():.4f}")
        print(f"   å¹³å‡Sharpeæ¯”: {results_df['sharpe'].mean():.4f}")

        return model_dir

    def plot_feature_importance(self, model_dir):
        """ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–"""
        logger.info("ğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ã‚’å¯è¦–åŒ–...")

        # ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
        importance = self.model.feature_importance(importance_type='gain')
        feature_names = self.model.feature_name()

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)

        # ãƒ—ãƒ­ãƒƒãƒˆ
        plt.figure(figsize=(12, 8))
        sns.barplot(data=importance_df.head(20), x='importance', y='feature')
        plt.title('Top 20 Feature Importance (LightGBM)')
        plt.xlabel('Importance (Gain)')
        plt.ylabel('Features')
        plt.tight_layout()

        # ä¿å­˜
        plot_path = model_dir / 'feature_importance.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"   ç‰¹å¾´é‡é‡è¦åº¦ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜: {plot_path}")

        return importance_df

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 80)
    print("ğŸš€ é‡‘èMLãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 80)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹
    data_path = 'data/processed/ml_dataset_20250829_135002.parquet'

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
    trainer = FinancialMLTrainer(data_path)

    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
        print("\\nğŸ“ Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†")
        trainer.load_and_preprocess_data()

        # 2. æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å‰²
        print("\\nğŸ“… Step 2: æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
        splits = trainer.create_time_series_splits(n_splits=3)  # è¨ˆç®—æ™‚é–“ã‚’è€ƒæ…®ã—ã¦3åˆ†å‰²

        # 3. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        print("\\nğŸš€ Step 3: LightGBMãƒ¢ãƒ‡ãƒ«å­¦ç¿’")
        fold_results = trainer.train_lightgbm_model(splits)

        # 4. çµæœä¿å­˜ã¨å¯è¦–åŒ–
        print("\\nğŸ’¾ Step 4: çµæœä¿å­˜")
        model_dir = trainer.save_model_and_results(fold_results)

        # 5. ç‰¹å¾´é‡é‡è¦åº¦
        print("\\nğŸ“Š Step 5: ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ")
        importance_df = trainer.plot_feature_importance(model_dir)

        # çµæœè¡¨ç¤º
        print("\\n" + "=" * 80)
        print("ğŸ‰ å­¦ç¿’å®Œäº†ï¼")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {model_dir}")
        print(f"ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {trainer.df['Code'].nunique():,}éŠ˜æŸ„ Ã— {len(trainer.df):,}ã‚µãƒ³ãƒ—ãƒ«")
        print(f"ğŸ”§ ç‰¹å¾´é‡æ•°: {len(trainer.feature_cols)}")
        print("=" * 80)

        # ãƒˆãƒƒãƒ—ç‰¹å¾´é‡è¡¨ç¤º
        print("\\nğŸ† ãƒˆãƒƒãƒ—10ç‰¹å¾´é‡:")
        for i, row in importance_df.head(10).iterrows():
            print(f"   {i+1:2d}. {row['feature']:<15} {row['importance']:.4f}")
    except Exception as e:
        logger.error(f"âŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
