
# ============================================================================
# Dataset Generation Makefile
# Specialized for Japanese Stock ML Dataset Generation
# ============================================================================

# This file is included by the main Makefile
# For standalone usage: make -f Makefile.dataset <target>

SHELL := /bin/bash

# ============================================================================
# Variables
# ============================================================================

# Date defaults (5 years of data)
DEFAULT_END   ?= $(shell date -u -d "yesterday" +%F)
DEFAULT_START ?= $(shell date -u -d "yesterday -5 years +1 day" +%F)

# Graph feature parameters
GRAPH_WINDOW    ?= 60
GRAPH_THRESHOLD ?= 0.5
GRAPH_MAX_K     ?= 4
CACHE_TTL_DAYS  ?= 120

# Cache directory (monthly sharding)
CACHE_SHARD ?= $(shell date -u -d "$(END)" +%Y%m 2>/dev/null || date -u +%Y%m)
CACHE_DIR   ?= output/graph_cache/$(CACHE_SHARD)/w$(GRAPH_WINDOW)-t$(GRAPH_THRESHOLD)-k$(GRAPH_MAX_K)

# GPU environment (safe settings for dataset generation)
SAFE_GPU_ENV ?= REQUIRE_GPU=1 USE_GPU_ETL=1 \
	RMM_ALLOCATOR=cuda_async RMM_POOL_SIZE=40GB CUDF_SPILL=1 \
	CUDA_VISIBLE_DEVICES=$${CUDA_VISIBLE_DEVICES:-0} PYTHONPATH=src

# ============================================================================
# Help
# ============================================================================

.PHONY: help-dataset
help-dataset:
	@echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
	@echo "  ğŸ“Š Dataset Generation Commands"
	@echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
	@echo ""
	@echo "ğŸš€ Layer 1: User-Friendly (RECOMMENDED)"
	@echo "  make dataset-bg     SSH-safe background build (5 years)"
	@echo "  make go             Alias for dataset-bg"
	@echo "  make dataset        Interactive all-in-one build"
	@echo ""
	@echo "âš™ï¸  Layer 2: Detailed Control"
	@echo "  make dataset-gpu START=... END=...         GPU-accelerated (auto-reuses cache)"
	@echo "  make dataset-gpu-refresh START=... END=... Force fresh API fetch (ignore cache)"
	@echo "  make dataset-cpu START=... END=...         CPU fallback"
	@echo "  make dataset-prod START=... END=...        Production config"
	@echo "  make dataset-research START=... END=...    Research features"
	@echo "  make build-chunks START=... END=...        Chunk builder (--resume/--force, CHUNK_MONTHS=1 for monthly safe mode)"
	@echo ""
	@echo "ğŸ› ï¸  Layer 3: Utilities"
	@echo "  make dataset-check           Environment check (relaxed)"
	@echo "  make dataset-check-strict    Environment check (strict GPU)"
	@echo "  make dataset-clean           Clean artifacts (keep raw/cache)"
	@echo "  make dataset-rebuild         Clean + rebuild with defaults"
	@echo "  make cache-stats             Show cache statistics"
	@echo "  make cache-prune             Prune old cache ($(CACHE_TTL_DAYS)d)"
	@echo "  make datasets-prune          Prune old ML datasets (keep latest 1)"
	@echo "  make raw-prune               Prune duplicate raw data files"
	@echo "  make output-clean            Full cleanup (datasets+raw+cache)"
	@echo "  make merge-chunks            Merge chunk outputs into latest dataset"
	@echo ""
	@echo "ğŸ›¡ï¸  Memory-Safe Options (NEW)"
	@echo "  make dataset-safe            Chunked generation with checkpointing"
	@echo "  make dataset-safe-resume     Resume from checkpoint"
	@echo "  make dataset-monitored       Generation with memory monitoring"
	@echo "  make cache-cleanup           Interactive cache cleanup"
	@echo "  make cache-monitor           Detailed cache statistics"
	@echo "  make memory-monitor PID=...  Monitor process memory"
	@echo ""
	@echo "ğŸ“‹ Defaults:"
	@echo "  Period: $(DEFAULT_START) â†’ $(DEFAULT_END)"
	@echo "  Graph:  window=$(GRAPH_WINDOW) threshold=$(GRAPH_THRESHOLD) k=$(GRAPH_MAX_K)"
	@echo "  Cache:  $(CACHE_DIR)"
	@echo ""
	@echo "ğŸ’¡ Examples:"
	@echo "  make dataset-bg                                   # Background, last 5 years"
	@echo "  make dataset-gpu START=2020-01-01 END=2024-12-31  # With smart cache"
	@echo "  make dataset-gpu-refresh START=... END=...        # Force API refresh"
	@echo "  make build-chunks START=2024-01-01 END=2024-12-31 RESUME=1  # Chunked build"
	@echo "  make dataset-rebuild                              # Clean + rebuild defaults"
	@echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# ============================================================================
# Layer 1: User-Friendly Commands (RECOMMENDED)
# ============================================================================

.PHONY: dataset-bg go dataset

# Background dataset builder (MOST RECOMMENDED)
# - GPU-accelerated with safe settings
# - Runs in background (SSH-safe)
# - Includes: preflight â†’ clean â†’ build â†’ stats
# Usage: make dataset-bg [START=YYYY-MM-DD END=YYYY-MM-DD]
dataset-bg:
	@echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
	@echo "â•‘  ğŸš€ Background Dataset Builder (GPU + SSH-safe)                  â•‘"
	@echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@mkdir -p _logs/dataset
	@START_VAL="$(START)"; END_VAL="$(END)"; \
	if [ -z "$$START_VAL" ]; then START_VAL="$(DEFAULT_START)"; fi; \
	if [ -z "$$END_VAL" ]; then END_VAL="$(DEFAULT_END)"; fi; \
	ts=$$(date +%Y%m%d_%H%M%S); \
	log=_logs/dataset/dataset_bg_$$ts.log; \
	pid_file=_logs/dataset/dataset_bg_$$ts.pid; \
	pgid_file=_logs/dataset/dataset_bg_$$ts.pgid; \
	echo "ğŸ“… Period: $$START_VAL â†’ $$END_VAL"; \
	echo "ğŸ“ Log: $$log"; \
	echo ""; \
	echo "ğŸ©º Preflight: checking credentials and GPU (STRICT - GPU required)..."; \
	if ! $(MAKE) -f Makefile.dataset dataset-check-strict; then \
	  echo ""; \
	  echo "âŒ Preflight failed. GPU is REQUIRED for background dataset generation."; \
	  echo "ğŸ’¡ If GPU is not available, use 'make dataset-cpu' instead."; \
	  exit 1; \
	fi; \
	echo ""; \
	if command -v setsid >/dev/null 2>&1; then \
	  nohup setsid env $(SAFE_GPU_ENV) $(MAKE) -f Makefile.dataset dataset START="$$START_VAL" END="$$END_VAL" > "$$log" 2>&1 & \
	else \
	  nohup env $(SAFE_GPU_ENV) $(MAKE) -f Makefile.dataset dataset START="$$START_VAL" END="$$END_VAL" > "$$log" 2>&1 & \
	fi; \
	pid=$$!; \
	pgid=$$(ps -o pgid= -p $$pid 2>/dev/null | tr -d ' ' || true); \
	echo "$$pid" > "$$pid_file"; \
	if [ -n "$$pgid" ]; then echo "$$pgid" > "$$pgid_file"; fi; \
	echo "âœ… Started in background (PID: $$pid$${pgid:+, PGID: $$pgid})"; \
	echo "ğŸ“Š Monitor: tail -f $$log"; \
	echo "ğŸ›‘ Stop (PID):   kill $$pid"; \
	if [ -n "$$pgid" ]; then echo "ğŸ›‘ Stop (group): kill -TERM -$$pgid"; fi; \
	echo "ğŸ—‚ï¸  PID file:  $$pid_file"; \
	if [ -n "$$pgid" ]; then echo "ğŸ—‚ï¸  PGID file: $$pgid_file"; fi

# Ultra-simple alias - points to SSH-safe background builder
go: dataset-bg

# All-in-one dataset builder: preflight â†’ clean â†’ build â†’ stats
# Usage: make dataset [START=YYYY-MM-DD END=YYYY-MM-DD]
# Default: Last ~5 years of data
dataset:
	@echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
	@echo "â•‘  ğŸš€ ALL-IN-ONE Dataset Builder (å®Œå…¨è‡ªå‹•)                        â•‘"
	@echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "ğŸ“‹ Steps:"
	@echo "  1ï¸âƒ£  Preflight check (credentials + GPU/CPU detection)"
	@echo "  2ï¸âƒ£  Clean old artifacts (keep raw data + caches)"
	@echo "  3ï¸âƒ£  Build full dataset (up to 395 features, ~307 active, GPU-accelerated)"
	@echo "  4ï¸âƒ£  Show cache statistics"
	@echo "  5ï¸âƒ£  Prune old dataset generations (keep latest only)"
	@echo "  6ï¸âƒ£  Sync to GCS (if enabled)"
	@echo ""
	@echo "â±ï¸  Estimated time:"
	@echo "  â€¢ Initial run: 30-60 min (GPU required)"
	@echo "  â€¢ Subsequent runs: <3 seconds (cache hit)"
	@echo ""
	@$(MAKE) -f Makefile.dataset dataset-check-strict || { \
	  echo ""; \
	  echo "âŒ Preflight check failed. GPU is REQUIRED for this command."; \
	  echo "ğŸ’¡ If GPU is not available, use 'make dataset-cpu START=... END=...' instead."; \
	  exit 1; \
	}
	@echo ""
	@echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
	@echo "ğŸ§¹ Step 2/4: Cleaning old artifacts..."
	@$(MAKE) -f Makefile.dataset dataset-clean
	@echo ""
	@echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
	@echo "ğŸ“Š Step 3/4: Building dataset..."
	@START_VAL="$(START)"; END_VAL="$(END)"; \
	if [ -z "$$START_VAL" ]; then START_VAL="$(DEFAULT_START)"; fi; \
	if [ -z "$$END_VAL" ]; then END_VAL="$(DEFAULT_END)"; fi; \
	echo "ğŸ“… Period: $$START_VAL â†’ $$END_VAL"; \
	echo ""; \
	$(MAKE) -f Makefile.dataset dataset-gpu START=$$START_VAL END=$$END_VAL
	@echo ""
	@echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
	@echo "ğŸ“¦ Step 4/5: Cache statistics..."
	@$(MAKE) -f Makefile.dataset cache-stats
	@echo ""
	@echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
	@echo "ğŸ—‘ï¸  Step 5/6: Pruning old dataset generations..."
	@$(MAKE) -f Makefile.dataset datasets-prune
	@echo ""
	@echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
	@echo "â˜ï¸  Step 6/6: Syncing to GCS..."
	@if [ "$${GCS_ENABLED}" = "1" ]; then \
		$(MAKE) gcs-sync; \
	else \
		echo "â­ï¸  GCS sync skipped (GCS_ENABLED != 1)"; \
	fi
	@echo ""
	@echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
	@echo "â•‘  âœ… Dataset build complete!                                      â•‘"
	@echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "ğŸ“„ Output: output/ml_dataset_latest_full.parquet"
	@echo "ğŸ”— Symlink: output/datasets/ml_dataset_latest_full.parquet"
	@echo ""
	@echo "Next steps:"
	@echo "  â€¢ Train model: make train"
	@echo "  â€¢ Quick test:  make smoke"
	@echo "  â€¢ Research:    make research-plus"

# ============================================================================
# Layer 2: Detailed Control (Custom Parameters)
# ============================================================================

.PHONY: dataset-gpu dataset-cpu dataset-prod dataset-research dataset-fetch build-chunks merge-chunks

# GPU-accelerated dataset generation (requires START/END parameters)
# Usage: make dataset-gpu START=YYYY-MM-DD END=YYYY-MM-DD
# Note: Automatically reuses cached data from previous runs (within 7 days)
dataset-gpu:
	@echo "ğŸš€ Running dataset generation with GPU-ETL enabled (up to 395 features; ~307 active)"
	@echo "âœ… Graph: cuGraph/CuPy with optimized memory config"
	@echo "âœ… Sector cross-sectional and daily margin features enabled"
	@echo "âœ… Smart cache: Reuses local data if available (use dataset-gpu-refresh to force API fetch)"
	@echo "âš ï¸  Note: Futures features (88-92 columns) disabled due to API unavailability"
	@[ -n "$(START)" ] && [ -n "$(END)" ] || { \
	  echo "âŒ Error: START and END dates required"; \
	  echo "Usage: make dataset-gpu START=YYYY-MM-DD END=YYYY-MM-DD"; \
	  exit 1; }
	@echo ""
	@echo "ğŸ” Verifying cache configuration..."
	@if ! grep -q "^USE_CACHE=1" .env 2>/dev/null; then \
	  echo "âš ï¸  WARNING: USE_CACHE not enabled in .env"; \
	  echo "âš ï¸  This will cause multi-GB price data to be re-fetched every time (~45-60s waste)"; \
	  echo "âš ï¸  Run 'make cache-verify' for more details"; \
	  echo "âš ï¸  Continuing in 5 seconds (Ctrl+C to abort and fix)..."; \
	  sleep 5; \
	else \
	  echo "âœ… Cache enabled (USE_CACHE=1)"; \
	fi
	@echo ""
	@env $(SAFE_GPU_ENV) \
	./venv/bin/python scripts/pipelines/run_full_dataset.py \
	  --jquants --start-date $(START) --end-date $(END) \
	  --gpu-etl --enable-graph-features \
	  --graph-window $(GRAPH_WINDOW) \
	  --graph-cache-dir $(CACHE_DIR) \
	  --graph-threshold $(GRAPH_THRESHOLD) --graph-max-k $(GRAPH_MAX_K) \
	  --futures-continuous \
	  --attach-nk225-option-market \
	  --sector-onehot33 \
	  --enable-sector-cs \
	  --enable-daily-margin \
	  --enable-short-selling \
	  --enable-sector-short-selling

# Force refresh from API (ignore cache)
# Usage: make dataset-gpu-refresh START=YYYY-MM-DD END=YYYY-MM-DD
dataset-gpu-refresh:
	@echo "ğŸ”„ Forcing fresh data fetch from API (ignoring cache)"
	@[ -n "$(START)" ] && [ -n "$(END)" ] || { \
	  echo "âŒ Error: START and END dates required"; \
	  echo "Usage: make dataset-gpu-refresh START=YYYY-MM-DD END=YYYY-MM-DD"; \
	  exit 1; }
	@env $(SAFE_GPU_ENV) \
	./venv/bin/python scripts/pipelines/run_full_dataset.py \
	  --jquants --force-refresh --start-date $(START) --end-date $(END) \
	  --gpu-etl --enable-graph-features \
	  --enable-short-selling \
	  --enable-sector-short-selling \
	  --graph-window $(GRAPH_WINDOW) \
	  --graph-cache-dir $(CACHE_DIR) \
	  --graph-threshold $(GRAPH_THRESHOLD) --graph-max-k $(GRAPH_MAX_K) \
	  --futures-continuous \
	  --attach-nk225-option-market \
	  --sector-onehot33 \
	  --enable-sector-cs \
	  --enable-daily-margin

# Chunk builder (default quarterly, set CHUNK_MONTHS=1 for monthly safe mode)
# Usage: make build-chunks START=YYYY-MM-DD END=YYYY-MM-DD [RESUME=1] [FORCE=1] [DRY_RUN=1] [LATEST=1] [JOBS=N] [CHUNK_MONTHS=M]
build-chunks:
	@[ -n "$(START)" ] && [ -n "$(END)" ] || { \
	  echo "âŒ Error: START and END dates required"; \
	  echo "Usage: make build-chunks START=YYYY-MM-DD END=YYYY-MM-DD [RESUME=1] [FORCE=1] [DRY_RUN=1]"; \
	  exit 1; }
	@cmd="./venv/bin/python gogooku5/data/scripts/build_chunks.py --start $(START) --end $(END)"; \
	if [ "$(RESUME)" = "1" ]; then cmd="$$cmd --resume"; fi; \
	if [ "$(FORCE)" = "1" ]; then cmd="$$cmd --force"; fi; \
	if [ "$(DRY_RUN)" = "1" ]; then cmd="$$cmd --dry-run"; fi; \
	if [ "$(LATEST)" = "1" ]; then cmd="$$cmd --latest-only"; fi; \
	if [ -n "$(CHUNK_MONTHS)" ]; then cmd="$$cmd --chunk-months $(CHUNK_MONTHS)"; fi; \
	if [ -n "$(JOBS)" ]; then cmd="$$cmd --jobs $(JOBS)"; elif [ -n "$(CHUNK_JOBS)" ]; then cmd="$$cmd --jobs $(CHUNK_JOBS)"; fi; \
	echo "ğŸš€ Chunk build command: $$cmd"; \
	env $(SAFE_GPU_ENV) $$cmd

# Merge completed chunks into latest dataset artifacts
merge-chunks:
	@cmd="./venv/bin/python gogooku5/data/tools/merge_chunks.py"; \
	if [ -n "$(CHUNKS_DIR)" ]; then cmd="$$cmd --chunks-dir $(CHUNKS_DIR)"; fi; \
	if [ -n "$(OUTPUT_DIR)" ]; then cmd="$$cmd --output-dir $(OUTPUT_DIR)"; fi; \
	if [ "$(STRICT)" = "1" ]; then cmd="$$cmd --strict"; fi; \
	if [ "$(ALLOW_PARTIAL)" = "1" ]; then cmd="$$cmd --allow-partial"; fi; \
	echo "ğŸ”— Merge command: $$cmd"; \
	env $(SAFE_GPU_ENV) $$cmd

# CPU-only dataset generation (fallback)
dataset-cpu:
	@[ -n "$(START)" ] && [ -n "$(END)" ] || { \
	  echo "âŒ Error: START and END dates required"; \
	  echo "Usage: make dataset-cpu START=YYYY-MM-DD END=YYYY-MM-DD"; \
	  exit 1; }
	@echo "ğŸ–¥ï¸  Running CPU-only dataset generation"
	@echo ""
	@echo "ğŸ” Verifying cache configuration..."
	@if ! grep -q "^USE_CACHE=1" .env 2>/dev/null; then \
	  echo "âš ï¸  WARNING: USE_CACHE not enabled in .env"; \
	  echo "âš ï¸  This will cause multi-GB price data to be re-fetched every time (~45-60s waste)"; \
	  echo "âš ï¸  Run 'make cache-verify' for more details"; \
	  echo "âš ï¸  Continuing in 5 seconds (Ctrl+C to abort and fix)..."; \
	  sleep 5; \
	else \
	  echo "âœ… Cache enabled (USE_CACHE=1)"; \
	fi
	@echo ""
	./venv/bin/python scripts/pipelines/run_full_dataset.py \
	  --jquants --start-date $(START) --end-date $(END)

# Production dataset with custom config
dataset-prod:
	@[ -n "$(START)" ] && [ -n "$(END)" ] || { \
	  echo "âŒ Error: START and END dates required"; \
	  echo "Usage: make dataset-prod START=YYYY-MM-DD END=YYYY-MM-DD"; \
	  exit 1; }
	@echo "ğŸ­ Running production dataset generation"
	@echo ""
	@echo "ğŸ” Verifying cache configuration..."
	@if ! grep -q "^USE_CACHE=1" .env 2>/dev/null; then \
	  echo "âš ï¸  WARNING: USE_CACHE not enabled in .env"; \
	  echo "âš ï¸  This will cause multi-GB price data to be re-fetched every time (~45-60s waste)"; \
	  echo "âš ï¸  Run 'make cache-verify' for more details"; \
	  echo "âš ï¸  Continuing in 5 seconds (Ctrl+C to abort and fix)..."; \
	  sleep 5; \
	else \
	  echo "âœ… Cache enabled (USE_CACHE=1)"; \
	fi
	@echo ""
	./venv/bin/python scripts/pipelines/run_full_dataset.py \
	  --jquants --start-date $(START) --end-date $(END) \
	  --config configs/pipeline/full_dataset.yaml

# Research dataset with indices features
dataset-research:
	@[ -n "$(START)" ] && [ -n "$(END)" ] || { \
	  echo "âŒ Error: START and END dates required"; \
	  echo "Usage: make dataset-research START=YYYY-MM-DD END=YYYY-MM-DD"; \
	  exit 1; }
	@echo "ğŸ”¬ Running research dataset generation"
	@echo ""
	@echo "ğŸ” Verifying cache configuration..."
	@if ! grep -q "^USE_CACHE=1" .env 2>/dev/null; then \
	  echo "âš ï¸  WARNING: USE_CACHE not enabled in .env"; \
	  echo "âš ï¸  This will cause multi-GB price data to be re-fetched every time (~45-60s waste)"; \
	  echo "âš ï¸  Run 'make cache-verify' for more details"; \
	  echo "âš ï¸  Continuing in 5 seconds (Ctrl+C to abort and fix)..."; \
	  sleep 5; \
	else \
	  echo "âœ… Cache enabled (USE_CACHE=1)"; \
	fi
	@echo ""
	./venv/bin/python scripts/pipelines/run_full_dataset.py \
	  --jquants --start-date $(START) --end-date $(END) \
	  --config configs/pipeline/research_full_indices.yaml

# Fetch all raw components (no ML dataset build)
dataset-fetch:
	@[ -n "$(START)" ] && [ -n "$(END)" ] || { \
	  echo "âŒ Error: START and END dates required"; \
	  echo "Usage: make dataset-fetch START=YYYY-MM-DD END=YYYY-MM-DD"; \
	  exit 1; }
	@echo "ğŸ“¥ Fetching raw data only (no dataset build)"
	./venv/bin/python scripts/data/fetch_jquants_history.py \
	  --jquants --all --start-date $(START) --end-date $(END)

# ============================================================================
# Layer 3: Utilities
# ============================================================================

.PHONY: dataset-check dataset-check-strict dataset-clean dataset-rebuild
.PHONY: cache-stats cache-prune datasets-prune raw-prune output-clean

# Preflight check (relaxed, allows CPU fallback)
dataset-check:
	@echo "ğŸ©º Running preflight check (credentials + basic GPU)"
	@echo "   (GPU fallback to CPU allowed)"
	@env $(SAFE_GPU_ENV) \
	./venv/bin/python scripts/pipelines/run_full_dataset.py --jquants --check-env-only

# Strict check: requires fully functional GPU graph features
dataset-check-strict:
	@echo "ğŸ©º Running STRICT preflight check (GPU graph required)"
	@echo "   (This check will fail if cuDF/cuGraph cannot be imported)"
	@env $(SAFE_GPU_ENV) \
	./venv/bin/python scripts/pipelines/run_full_dataset.py \
	  --jquants --check-env-only --require-gpu-graph

# Clean dataset artifacts (keep raw data and caches)
dataset-clean:
	@echo "ğŸ§¹ Removing dataset artifacts (keeping raw/* and caches)"
	@set -e; \
	rm -f output/ml_dataset_*.parquet output/ml_dataset_*_metadata.json 2>/dev/null || true; \
	rm -f output/performance_report_*.json 2>/dev/null || true; \
	rm -f output/datasets/ml_dataset_*_full.parquet output/datasets/ml_dataset_*_full_metadata.json 2>/dev/null || true; \
	for link in \
	  output/ml_dataset_latest_full.parquet \
	  output/ml_dataset_latest_full_metadata.json \
	  output/datasets/ml_dataset_latest_full.parquet \
	  output/datasets/ml_dataset_latest_full_metadata.json; do \
	  [ -L "$$link" ] && unlink "$$link" || true; \
	done; \
	echo "âœ… Cleanup complete."

# Clean + rebuild with default date range
dataset-rebuild:
	@set -euo pipefail; \
	$(MAKE) -f Makefile.dataset dataset-clean; \
	START_VAL="$(START)"; END_VAL="$(END)"; \
	if [ -z "$$START_VAL" ]; then START_VAL="$(DEFAULT_START)"; fi; \
	if [ -z "$$END_VAL" ]; then END_VAL="$(DEFAULT_END)"; fi; \
	echo "ğŸš€ Rebuilding dataset with START=$$START_VAL END=$$END_VAL"; \
	$(MAKE) -f Makefile.dataset dataset-gpu START=$$START_VAL END=$$END_VAL

# Show graph cache statistics
cache-stats:
	@echo "ğŸ“¦ Graph cache layout under output/graph_cache";
	@if [ -d output/graph_cache ]; then \
	  find output/graph_cache -maxdepth 2 -type d -print | sort; \
	  echo ""; \
	  echo "Total files:"; find output/graph_cache -type f -name '*.pkl' | wc -l; \
	  echo "Total size:"; du -sh output/graph_cache 2>/dev/null || true; \
	else \
	  echo "(no cache yet)"; \
	fi

# Prune old cache files (default: 120 days)
cache-prune:
	@echo "ğŸ§¹ Pruning graph cache older than $(CACHE_TTL_DAYS) days";
	@if [ -d output/graph_cache ]; then \
	  find output/graph_cache -type f -name '*.pkl' -mtime +$(CACHE_TTL_DAYS) -print -delete; \
	  echo "After prune size:"; du -sh output/graph_cache 2>/dev/null || true; \
	else \
	  echo "output/graph_cache not found"; \
	fi

# Prune old ML dataset generations (keep latest N)
# Usage: make datasets-prune [DATASET_KEEP_GENERATIONS=1]
datasets-prune:
	@echo "ğŸ§¹ Pruning old ML dataset generations"
	@bash scripts/maintenance/cleanup_datasets.sh --force

# Prune duplicate raw data files (keep widest date range)
raw-prune:
	@echo "ğŸ§¹ Pruning duplicate raw data files"
	@bash scripts/maintenance/cleanup_raw_data.sh --force

# Full output directory cleanup (datasets + raw + cache)
# Usage: make output-clean
output-clean:
	@echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
	@echo "â•‘  ğŸ—‘ï¸  Full Output Directory Cleanup                              â•‘"
	@echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "This will:"
	@echo "  1. Prune old ML dataset generations"
	@echo "  2. Prune duplicate raw data files"
	@echo "  3. Prune old graph cache ($(CACHE_TTL_DAYS)+ days)"
	@echo ""
	@$(MAKE) -f Makefile.dataset datasets-prune
	@echo ""
	@$(MAKE) -f Makefile.dataset raw-prune
	@echo ""
	@$(MAKE) -f Makefile.dataset cache-prune
	@echo ""
	@echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
	@echo "â•‘  âœ… Full cleanup complete!                                       â•‘"
	@echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@du -sh output/ 2>/dev/null || true

# ============================================================================
# Memory-Safe Dataset Generation (NEW)
# ============================================================================

.PHONY: dataset-safe dataset-safe-resume dataset-monitored
.PHONY: cache-cleanup cache-monitor memory-monitor

# Memory-safe dataset generation with chunking and checkpointing
# Usage: make dataset-safe [START=YYYY-MM-DD END=YYYY-MM-DD] [CHUNK_YEARS=1]
dataset-safe:
	@echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
	@echo "â•‘  ğŸ›¡ï¸  Memory-Safe Dataset Generator                              â•‘"
	@echo "â•‘  â€¢ Chunked processing (1 year per chunk by default)             â•‘"
	@echo "â•‘  â€¢ Automatic checkpointing                                       â•‘"
	@echo "â•‘  â€¢ Resume from interruptions                                     â•‘"
	@echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@START_VAL="$(START)"; END_VAL="$(END)"; \
	if [ -z "$$START_VAL" ]; then START_VAL="$(DEFAULT_START)"; fi; \
	if [ -z "$$END_VAL" ]; then END_VAL="$(DEFAULT_END)"; fi; \
	CHUNK_YEARS=$${CHUNK_YEARS:-1}; \
	echo "ğŸ“… Period: $$START_VAL â†’ $$END_VAL"; \
	echo "ğŸ“¦ Chunk size: $$CHUNK_YEARS year(s)"; \
	echo ""; \
	./venv/bin/python scripts/data/dataset_generator_safe.py \
	  --start-date "$$START_VAL" \
	  --end-date "$$END_VAL" \
	  --chunk-years $$CHUNK_YEARS

# Resume interrupted dataset generation from checkpoint
dataset-safe-resume:
	@echo "ğŸ”„ Resuming from checkpoint..."
	@START_VAL="$(START)"; END_VAL="$(END)"; \
	if [ -z "$$START_VAL" ]; then START_VAL="$(DEFAULT_START)"; fi; \
	if [ -z "$$END_VAL" ]; then END_VAL="$(DEFAULT_END)"; fi; \
	./venv/bin/python scripts/data/dataset_generator_safe.py \
	  --start-date "$$START_VAL" \
	  --end-date "$$END_VAL" \
	  --resume

# Dataset generation with real-time memory monitoring
# Automatically terminates if memory exceeds threshold
dataset-monitored:
	@echo "ğŸ¯ Starting dataset generation with memory monitoring..."
	@START_VAL="$(START)"; END_VAL="$(END)"; \
	if [ -z "$$START_VAL" ]; then START_VAL="$(DEFAULT_START)"; fi; \
	if [ -z "$$END_VAL" ]; then END_VAL="$(DEFAULT_END)"; fi; \
	bash scripts/monitoring/watch_dataset.sh \
	  $(MAKE) -f Makefile.dataset dataset-gpu START="$$START_VAL" END="$$END_VAL"

# Clean up old cache with confirmation
cache-cleanup:
	@echo "ğŸ—‘ï¸  Cache cleanup utility"
	@bash scripts/maintenance/cleanup_cache.sh

# Show detailed cache statistics and recommendations
cache-monitor:
	@echo "ğŸ“Š Cache monitoring report"
	@./venv/bin/python scripts/maintenance/cache_monitor.py --verbose

# Monitor memory usage of a running process
# Usage: make memory-monitor PID=<process_id>
memory-monitor:
	@if [ -z "$(PID)" ]; then \
	  echo "âŒ Error: PID required"; \
	  echo "Usage: make memory-monitor PID=<process_id>"; \
	  exit 1; \
	fi
	@echo "ğŸ‘ï¸  Monitoring process $(PID)..."
	@./venv/bin/python scripts/monitoring/memory_monitor.py --pid $(PID) --gpu
