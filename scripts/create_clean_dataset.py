#!/usr/bin/env python3
"""
Create clean dataset without data leakage
ç›®çš„: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é™¤å»ã—ãŸã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
"""

from pathlib import Path

import numpy as np
import pandas as pd


def create_clean_dataset():
    """ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é™¤å»ã—ãŸã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""

    print("=" * 60)
    print("ğŸ§¹ CREATING CLEAN DATASET WITHOUT DATA LEAKAGE")
    print("=" * 60)

    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    input_path = Path("/home/ubuntu/gogooku3-standalone/output/ml_dataset_latest_full.parquet")
    output_path = Path("/home/ubuntu/gogooku3-standalone/output/ml_dataset_clean.parquet")

    print(f"\nğŸ“‚ Loading data from: {input_path}")
    df = pd.read_parquet(input_path)
    print(f"âœ… Original data shape: {df.shape}")

    # 2. é™¤å¤–ã™ã¹ãåˆ—ã®ç‰¹å®š
    print("\nğŸ” Identifying columns to exclude...")

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ï¼ˆä¿æŒã™ã‚‹ï¼‰
    target_cols = ['returns_1d', 'returns_5d', 'returns_10d', 'returns_20d', 'returns_60d', 'returns_120d']

    # ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’å¼•ãèµ·ã“ã™åˆ—ï¼ˆé™¤å¤–ã™ã‚‹ï¼‰
    leakage_columns = [
        # å¯¾æ•°ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã»ã¼åŒã˜ï¼‰
        'log_returns_1d', 'log_returns_5d', 'log_returns_10d', 'log_returns_20d',

        # target_*ã¨ã„ã†åå‰ã®åˆ—ï¼ˆæ˜ã‚‰ã‹ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
        'target_1d', 'target_5d', 'target_10d', 'target_20d',
        'target_1d_binary', 'target_5d_binary', 'target_10d_binary',

        # feat_ret_*ï¼ˆç‰¹å¾´é‡ãƒªã‚¿ãƒ¼ãƒ³ã ãŒã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨é‡è¤‡ã®å¯èƒ½æ€§ï¼‰
        'feat_ret_1d', 'feat_ret_5d', 'feat_ret_10d', 'feat_ret_20d',

        # æœªæ¥ã®æƒ…å ±ã‚’å«ã‚€å¯èƒ½æ€§ã®ã‚ã‚‹åˆ—
        'target', 'label', 'y', 'Y'
    ]

    # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’é™¤å¤–ãƒªã‚¹ãƒˆã«å«ã‚ã‚‹
    columns_to_exclude = [col for col in leakage_columns if col in df.columns]

    print(f"\nğŸ—‘ï¸ Columns to exclude ({len(columns_to_exclude)}):")
    for col in columns_to_exclude:
        print(f"   - {col}")

    # 3. ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    print("\nğŸ§¹ Creating clean dataset...")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ—
    meta_cols = ['Code', 'Date']

    # ä¿æŒã™ã‚‹åˆ—ã‚’æ±ºå®š
    all_cols = df.columns.tolist()
    keep_cols = []

    for col in all_cols:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ—ã¯ä¿æŒ
        if col in meta_cols:
            keep_cols.append(col)
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã¯ä¿æŒ
        elif col in target_cols:
            keep_cols.append(col)
        # é™¤å¤–ãƒªã‚¹ãƒˆã«ãªã„åˆ—ã¯ä¿æŒ
        elif col not in columns_to_exclude:
            keep_cols.append(col)

    # ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    clean_df = df[keep_cols].copy()

    print(f"\nâœ… Clean data shape: {clean_df.shape}")
    print(f"   Original features: {len([c for c in all_cols if c not in meta_cols + target_cols])}")
    print(f"   Clean features: {len([c for c in keep_cols if c not in meta_cols + target_cols])}")
    print(f"   Removed features: {len(columns_to_exclude)}")

    # 4. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    print("\nğŸ” Verifying data quality...")

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã®ç¢ºèª
    for target in target_cols:
        if target in clean_df.columns:
            non_null = clean_df[target].notna().sum()
            print(f"   {target}: {non_null:,} non-null values")

    # ç‰¹å¾´é‡ã®å‹ã‚’ç¢ºèª
    numeric_features = [col for col in clean_df.columns
                       if col not in meta_cols + target_cols
                       and clean_df[col].dtype in ['float64', 'float32', 'int64', 'int32']]

    print("\nğŸ“Š Feature types:")
    print(f"   Numeric features: {len(numeric_features)}")
    print(f"   Non-numeric features: {len([c for c in clean_df.columns if c not in meta_cols + target_cols]) - len(numeric_features)}")

    # 5. ä¿å­˜
    print(f"\nğŸ’¾ Saving clean dataset to: {output_path}")
    clean_df.to_parquet(output_path, index=False)
    print("âœ… Clean dataset saved successfully!")

    # 6. ç°¡å˜ãªæ¤œè¨¼
    print("\n" + "=" * 60)
    print("ğŸ“Š Quick Validation")
    print("=" * 60)

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ç›¸é–¢ã‚’å†ãƒã‚§ãƒƒã‚¯
    sample_df = clean_df.sample(min(10000, len(clean_df)))
    numeric_features_sample = [col for col in numeric_features[:50] if col in sample_df.columns]

    print("\nğŸ” Checking for remaining high correlations with targets...")
    high_corr_found = False

    for target in ['returns_1d', 'returns_5d']:
        if target not in sample_df.columns:
            continue

        target_vals = sample_df[target].fillna(0)
        max_corr = 0
        max_corr_feature = None

        for feat in numeric_features_sample:
            feat_vals = sample_df[feat].fillna(0)
            try:
                corr = np.corrcoef(target_vals, feat_vals)[0, 1]
                if abs(corr) > abs(max_corr):
                    max_corr = corr
                    max_corr_feature = feat
                if abs(corr) > 0.9:
                    print(f"   âš ï¸ High correlation: {target} vs {feat} = {corr:.4f}")
                    high_corr_found = True
            except:
                pass

        if max_corr_feature:
            print(f"   {target}: max correlation = {max_corr:.4f} (with {max_corr_feature})")

    if not high_corr_found:
        print("\nâœ… No obvious data leakage remaining!")
    else:
        print("\nâš ï¸ Some high correlations remain - manual review recommended")

    # 7. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
    print("\n" + "=" * 60)
    print("ğŸš€ Next Steps")
    print("=" * 60)
    print("\n1. Convert clean dataset to ATFT format:")
    print("   python scripts/convert_to_atft_data.py --input output/ml_dataset_clean.parquet")
    print("\n2. Re-run baseline test to verify:")
    print("   python scripts/test_baseline_rankic.py --use-clean")
    print("\n3. Train ATFT with clean data:")
    print("   make train-clean")

    return output_path

if __name__ == "__main__":
    output_path = create_clean_dataset()
    print(f"\nâœ¨ Clean dataset ready at: {output_path}")
