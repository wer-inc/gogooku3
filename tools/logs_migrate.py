#!/usr/bin/env python3
"""
Gogooku3 ãƒ­ã‚°ç§»è¨­ãƒ„ãƒ¼ãƒ«

æ•£åœ¨ã—ã¦ã„ã‚‹ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±ä¸€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€  (_logs/) ã«å®‰å…¨ã«ç§»è¨­ã™ã‚‹
æ—¢å­˜ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼å¤‰æ›ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ä¸Žãƒ»JSTæ­£è¦åŒ–ã‚‚å®Ÿè¡Œ
"""

import argparse
import hashlib
import json
import os
import re
import shutil
import socket
import subprocess
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging
import gzip

# JST timezone
JST = timezone(timedelta(hours=9))

class LogMigrator:
    """ãƒ­ã‚°ç§»è¨­ãƒ»å¤‰æ›ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, repo_root: Path, dry_run: bool = False):
        self.repo_root = Path(repo_root)
        self.dry_run = dry_run
        self.hostname = socket.gethostname()
        
        # Git SHAå–å¾—
        try:
            self.git_sha = subprocess.check_output(
                ["git", "rev-parse", "--short", "HEAD"],
                cwd=repo_root,
                stderr=subprocess.DEVNULL
            ).decode().strip()
        except:
            self.git_sha = "unknown"
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'files_found': 0,
            'files_migrated': 0,
            'files_converted': 0,
            'files_skipped': 0,
            'errors': []
        }
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def find_log_files(self) -> List[Path]:
        """æ•£åœ¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        log_patterns = ['*.log', '*.out', '*.err']
        exclude_dirs = {'.git', '_logs', '__pycache__', '.vscode', '.idea', 'node_modules'}
        
        found_files = []
        
        for pattern in log_patterns:
            for path in self.repo_root.rglob(pattern):
                # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¹ã‚­ãƒƒãƒ—
                if any(part in exclude_dirs for part in path.parts):
                    continue
                
                found_files.append(path)
                self.stats['files_found'] += 1
        
        return sorted(found_files)
    
    def classify_service(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’æŽ¨å®š"""
        path_str = str(file_path).lower()
        
        if any(x in path_str for x in ['dagster', 'orchestration']):
            return 'dagster'
        elif any(x in path_str for x in ['mlflow', 'ml_training', 'atft']):
            return 'mlflow'
        elif any(x in path_str for x in ['feast', 'feature_store']):
            return 'feast'
        elif any(x in path_str for x in ['clickhouse', 'db']):
            return 'clickhouse'
        elif any(x in path_str for x in ['redis', 'cache']):
            return 'redis'
        elif any(x in path_str for x in ['postgres', 'sql']):
            return 'postgres'
        elif 'docker' in path_str:
            return 'docker'
        else:
            return 'app'
    
    def get_file_date(self, file_path: Path) -> datetime:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥ä»˜ã‚’å–å¾—ï¼ˆåå‰ã‹ã‚‰æŽ¨å®š or mtimeä½¿ç”¨ï¼‰"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜æŠ½å‡ºã‚’è©¦è¡Œ
        date_patterns = [
            r'(\d{4}-\d{2}-\d{2})',          # YYYY-MM-DD
            r'(\d{8})',                       # YYYYMMDD
            r'(\d{4})(\d{2})(\d{2})',        # YYYYMMDD (åˆ†å‰²)
        ]
        
        filename = file_path.name
        for pattern in date_patterns:
            match = re.search(pattern, filename)
            if match:
                try:
                    if len(match.groups()) == 1:
                        date_str = match.group(1)
                        if len(date_str) == 8:  # YYYYMMDD
                            return datetime.strptime(date_str, '%Y%m%d')
                        else:  # YYYY-MM-DD
                            return datetime.strptime(date_str, '%Y-%m-%d')
                    else:  # (YYYY)(MM)(DD)
                        year, month, day = match.groups()
                        return datetime(int(year), int(month), int(day))
                except ValueError:
                    continue
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ—¥æ™‚ã‚’ä½¿ç”¨
        return datetime.fromtimestamp(file_path.stat().st_mtime)
    
    def create_target_path(self, file_path: Path, service: str, file_date: datetime) -> Path:
        """ç§»è¨­å…ˆãƒ‘ã‚¹ã‚’ç”Ÿæˆ"""
        env = 'dev'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç’°å¢ƒ
        
        target_dir = (
            self.repo_root / '_logs' / env / service / 
            f"{file_date.year}" / f"{file_date.month:02d}" / f"{file_date.day:02d}"
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å: hostname_service_original.jsonl.gz
        original_stem = file_path.stem
        if file_path.suffix in ['.log', '.out', '.err']:
            new_name = f"{self.hostname}_{service}_{original_stem}.jsonl.gz"
        else:
            new_name = f"{self.hostname}_{service}_{file_path.name}.gz"
            
        return target_dir / new_name
    
    def convert_to_jsonl(self, source_path: Path, target_path: Path, service: str) -> bool:
        """ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ­ã‚°ã‚’JSON Lineså½¢å¼ã«å¤‰æ›"""
        
        try:
            with open(source_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            with gzip.open(target_path, 'wt', encoding='utf-8') as f:
                for line_num, line in enumerate(lines, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«æŽ¨å®š
                    level = 'INFO'
                    if any(keyword in line.upper() for keyword in ['ERROR', 'CRITICAL', 'FATAL']):
                        level = 'ERROR'
                    elif any(keyword in line.upper() for keyword in ['WARN', 'WARNING']):
                        level = 'WARNING'
                    elif any(keyword in line.upper() for keyword in ['DEBUG']):
                        level = 'DEBUG'
                    
                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡ºè©¦è¡Œ
                    timestamp = datetime.now(JST).isoformat()
                    ts_patterns = [
                        r'(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2})',
                        r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})',
                    ]
                    
                    for pattern in ts_patterns:
                        match = re.search(pattern, line)
                        if match:
                            try:
                                dt = datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S')
                                timestamp = dt.replace(tzinfo=JST).isoformat()
                                break
                            except ValueError:
                                try:
                                    dt = datetime.strptime(match.group(1), '%Y-%m-%dT%H:%M:%S')
                                    timestamp = dt.replace(tzinfo=JST).isoformat()
                                    break
                                except ValueError:
                                    continue
                    
                    # JSON Lines ã‚¨ãƒ³ãƒˆãƒªä½œæˆ
                    log_entry = {
                        "ts": timestamp,
                        "lvl": level,
                        "msg": line,
                        "svc": service,
                        "mod": "migrated",
                        "file": source_path.name,
                        "line": line_num,
                        "host": self.hostname,
                        "git_sha": self.git_sha,
                        "migrated_from": str(source_path.relative_to(self.repo_root)),
                        "migration_ts": datetime.now(JST).isoformat()
                    }
                    
                    f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
            
            return True
            
        except Exception as e:
            self.stats['errors'].append(f"Conversion failed for {source_path}: {e}")
            self.logger.error(f"Failed to convert {source_path}: {e}")
            return False
    
    def copy_binary_file(self, source_path: Path, target_path: Path) -> bool:
        """ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONç­‰ï¼‰ã®ã‚³ãƒ”ãƒ¼ãƒ»åœ§ç¸®"""
        
        try:
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(source_path, 'rb') as f_in:
                with gzip.open(target_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            return True
            
        except Exception as e:
            self.stats['errors'].append(f"Copy failed for {source_path}: {e}")
            self.logger.error(f"Failed to copy {source_path}: {e}")
            return False
    
    def migrate_file(self, source_path: Path) -> bool:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ç§»è¨­å‡¦ç†"""
        
        # ã‚µãƒ¼ãƒ“ã‚¹åˆ†é¡ž
        service = self.classify_service(source_path)
        
        # æ—¥ä»˜å–å¾—
        file_date = self.get_file_date(source_path)
        
        # ç§»è¨­å…ˆãƒ‘ã‚¹ç”Ÿæˆ
        target_path = self.create_target_path(source_path, service, file_date)
        
        self.logger.info(f"Migrating: {source_path} -> {target_path}")
        
        if self.dry_run:
            self.logger.info(f"[DRY RUN] Would migrate {source_path} to {target_path}")
            return True
        
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¹ã‚­ãƒƒãƒ—
        if target_path.exists():
            self.logger.warning(f"Target exists, skipping: {target_path}")
            self.stats['files_skipped'] += 1
            return False
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ãŸå‡¦ç†
        if source_path.suffix in ['.log', '.out', '.err'] and source_path.stat().st_size > 0:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ­ã‚°ã®å¤‰æ›
            success = self.convert_to_jsonl(source_path, target_path, service)
            if success:
                self.stats['files_converted'] += 1
        else:
            # ãã®ä»–ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ”ãƒ¼ãƒ»åœ§ç¸®
            success = self.copy_binary_file(source_path, target_path)
        
        if success:
            self.stats['files_migrated'] += 1
            
            # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç§»å‹•ï¼ˆå‰Šé™¤ã¯ã—ãªã„ï¼‰
            backup_dir = self.repo_root / '_logs_backup'
            backup_dir.mkdir(exist_ok=True)
            
            backup_path = backup_dir / source_path.name
            if not backup_path.exists():
                shutil.copy2(source_path, backup_path)
            
        return success
    
    def migrate_all(self) -> Dict:
        """å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ç§»è¨­å®Ÿè¡Œ"""
        
        self.logger.info("ðŸš€ Starting log migration...")
        self.logger.info(f"Repository: {self.repo_root}")
        self.logger.info(f"Dry run: {self.dry_run}")
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        log_files = self.find_log_files()
        self.logger.info(f"Found {len(log_files)} log files")
        
        if not log_files:
            self.logger.info("No log files found to migrate")
            return self.stats
        
        # ç§»è¨­å®Ÿè¡Œ
        for file_path in log_files:
            try:
                self.migrate_file(file_path)
            except Exception as e:
                error_msg = f"Migration error for {file_path}: {e}"
                self.stats['errors'].append(error_msg)
                self.logger.error(error_msg)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_report()
        
        return self.stats
    
    def generate_report(self):
        """ç§»è¨­ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        report_path = self.repo_root / 'logs_migration_report.md'
        
        report_content = f"""# Gogooku3 ãƒ­ã‚°ç§»è¨­ãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now(JST).strftime('%Y-%m-%d %H:%M:%S JST')}  
**å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰**: {'DRY RUN' if self.dry_run else 'ACTUAL MIGRATION'}  
**Git SHA**: {self.git_sha}

## ðŸ“Š ç§»è¨­çµ±è¨ˆ

- **ç™ºè¦‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {self.stats['files_found']}
- **ç§»è¨­æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {self.stats['files_migrated']}
- **å¤‰æ›æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {self.stats['files_converted']}
- **ã‚¹ã‚­ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {self.stats['files_skipped']}
- **ã‚¨ãƒ©ãƒ¼æ•°**: {len(self.stats['errors'])}

## ðŸ“ ç§»è¨­å…ˆæ§‹é€ 

```
_logs/
â”œâ”€â”€ dev/
â”‚   â”œâ”€â”€ app/           # Python scripts
â”‚   â”œâ”€â”€ dagster/       # Orchestration
â”‚   â”œâ”€â”€ mlflow/        # ML lifecycle
â”‚   â”œâ”€â”€ feast/         # Feature store
â”‚   â”œâ”€â”€ clickhouse/    # OLAP database
â”‚   â”œâ”€â”€ redis/         # Cache
â”‚   â”œâ”€â”€ postgres/      # RDBMS
â”‚   â””â”€â”€ docker/        # Container logs
â””â”€â”€ prd/               # Production (same structure)
```

## ðŸ”§ å¤‰æ›ä»•æ§˜

- **å½¢å¼**: Plain text â†’ JSON Lines (gzipåœ§ç¸®)
- **ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³**: JST (Asia/Tokyo)
- **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**: git_sha, hostname, migration_timestamp ä»˜ä¸Ž
- **å‘½å**: `{{hostname}}_{{service}}_{{original}}.jsonl.gz`

## âŒ ã‚¨ãƒ©ãƒ¼ä¸€è¦§

"""

        if self.stats['errors']:
            for i, error in enumerate(self.stats['errors'], 1):
                report_content += f"{i}. {error}\n"
        else:
            report_content += "ã‚¨ãƒ©ãƒ¼ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"

        report_content += f"""

## ðŸŽ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **å‹•ä½œç¢ºèª**: ç§»è¨­ã•ã‚ŒãŸãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ç¢ºèª
2. **çµ±ä¸€ãƒ­ã‚¬ãƒ¼é©ç”¨**: scripts/*.py ã¸ã® setup_gogooku_logger é©ç”¨
3. **å…ƒãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤**: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç¢ºèªå¾Œã® cleanup
4. **Dockerè¨­å®š**: docker-compose.yml ã®ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š

## ðŸ“ž ãƒˆãƒ©ãƒ–ãƒ«æ™‚

- **ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯**: `_logs_backup/` ã‹ã‚‰ã®å¾©æ—§
- **å†å®Ÿè¡Œ**: `python tools/logs_migrate.py --force`
- **ç¢ºèª**: `find _logs -name "*.jsonl.gz" | head -10`

---

*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ tools/logs_migrate.py ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*
"""
        
        if not self.dry_run:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            self.logger.info(f"Report saved: {report_path}")
        else:
            self.logger.info("Report content (dry run):")
            print(report_content)


def main():
    parser = argparse.ArgumentParser(description="Gogooku3 ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç§»è¨­ãƒ„ãƒ¼ãƒ«")
    parser.add_argument(
        '--repo-root', 
        type=Path, 
        default=Path(__file__).parent.parent,
        help="ãƒªãƒã‚¸ãƒˆãƒªãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹"
    )
    parser.add_argument(
        '--dry-run', 
        action='store_true',
        help="å®Ÿéš›ã®ç§»è¨­ã‚’ã›ãšã€å¤‰æ›´äºˆå®šã‚’è¡¨ç¤º"
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help="æ—¢å­˜ã®ç§»è¨­å…ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã"
    )
    
    args = parser.parse_args()
    
    if not args.repo_root.exists():
        print(f"âŒ Repository root not found: {args.repo_root}")
        return 1
    
    migrator = LogMigrator(args.repo_root, dry_run=args.dry_run)
    stats = migrator.migrate_all()
    
    print("\nðŸŽ¯ Migration Summary:")
    print(f"  Files found: {stats['files_found']}")
    print(f"  Files migrated: {stats['files_migrated']}")
    print(f"  Files converted: {stats['files_converted']}")
    print(f"  Files skipped: {stats['files_skipped']}")
    print(f"  Errors: {len(stats['errors'])}")
    
    if stats['errors']:
        print("\nâŒ Errors occurred:")
        for error in stats['errors'][:5]:  # Show first 5 errors
            print(f"  - {error}")
        if len(stats['errors']) > 5:
            print(f"  ... and {len(stats['errors']) - 5} more")
    
    return 0 if len(stats['errors']) == 0 else 1


if __name__ == "__main__":
    exit(main())