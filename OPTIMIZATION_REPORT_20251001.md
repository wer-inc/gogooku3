# ğŸš€ å­¦ç¿’æ€§èƒ½æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ - 2025-10-01

## ğŸ“Š å•é¡Œåˆ†æ

### å®Ÿè¡Œå‰ã®çŠ¶æ…‹ï¼ˆ12:02é–‹å§‹ã€12:45æ™‚ç‚¹ï¼‰
- **GPUä½¿ç”¨ç‡**: 0% âš ï¸ **æœ€å¤§ã®å•é¡Œ**
- **CPUä½¿ç”¨ç‡**: 76.1%
- **Validationé€Ÿåº¦**: 2.1-2.2ç§’/iterationï¼ˆéå¸¸ã«é…ã„ï¼‰
- **å®Ÿè¡Œæ™‚é–“**: 45åˆ†21ç§’ã§ã¾ã 84%é€²æ—
- **æ¨å®šå®Œäº†æ™‚é–“**: 1ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Š3-4æ™‚é–“

### ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
1. **Validationä¸­ã«ã‚°ãƒ©ãƒ•æ§‹ç¯‰** - å„ãƒãƒƒãƒã§256ãƒãƒ¼ãƒ‰ã€2560ã‚¨ãƒƒã‚¸ã‚’å†æ§‹ç¯‰ï¼ˆç´„0.2ç§’/ãƒãƒƒãƒï¼‰
2. **CPUå‡¦ç†ã§GPUã‚¢ã‚¤ãƒ‰ãƒ«** - ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã¯CPUå‡¦ç†ã®ãŸã‚GPUãŒå¾…æ©Ÿ
3. **éå‰°ãªDataLoaderãƒ¯ãƒ¼ã‚«ãƒ¼** - 16 workersã§ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ç™ºç”Ÿ
4. **æº–æœ€é©ãªprecisionè¨­å®š** - FP16ã‚ˆã‚ŠBF16ã®æ–¹ãŒA100ã§ã¯é«˜é€Ÿ

---

## âœ… å®Ÿæ–½ã—ãŸæœ€é©åŒ–

### ğŸ”´ Priority 1: GPUä½¿ç”¨ç‡0%ã®è§£æ±º

**å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«**: `configs/atft/config_production_optimized.yaml`

```yaml
# BEFORE
graph_builder:
  use_in_training: true

# AFTER
graph_builder:
  use_in_training: false  # Disable graph rebuild during validation
```

**åŠ¹æœ**:
- Validationä¸­ã®ã‚°ãƒ©ãƒ•å†æ§‹ç¯‰ã‚’ç„¡åŠ¹åŒ–
- GPUå¾…æ©Ÿæ™‚é–“ã‚’ã‚¼ãƒ­ã«å‰Šæ¸›
- **æœŸå¾…GPUä½¿ç”¨ç‡**: 0% â†’ 80-90%

---

### ğŸŸ¡ Priority 2: DataLoaderæœ€é©åŒ–

**å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«**: `configs/atft/train/production.yaml`

```yaml
# BEFORE
batch:
  num_workers: 16
  prefetch_factor: 8
  gradient_accumulation_steps: 1

# AFTER
batch:
  num_workers: 8  # Optimal for A100
  prefetch_factor: 4  # Matches optimal ratio
  gradient_accumulation_steps: 2  # Effective batch = 8192
```

**åŠ¹æœ**:
- ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯è§£æ¶ˆ
- CPUä½¿ç”¨ç‡ã®åŠ¹ç‡åŒ–
- Effective batch size: 4096 â†’ 8192

---

### ğŸŸ¡ Priority 3: ãƒãƒƒãƒã‚µã‚¤ã‚ºæœ€é©åŒ–

**æ—¢å­˜è¨­å®šã‚’ç¢ºèª**: `train_batch_size: 4096` (æ—¢ã«æœ€é©)

**Gradient Accumulationè¿½åŠ **:
- Steps: 1 â†’ 2
- Effective batch: 4096 â†’ 8192
- A100 80GBãƒ¡ãƒ¢ãƒªã«æœ€é©åŒ–

---

### ğŸŸ¢ Priority 4: Mixed Precisionæœ€é©åŒ–

**å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«**: `configs/atft/train/production.yaml`

```yaml
# BEFORE
trainer:
  precision: 16-mixed  # FP16

# AFTER
trainer:
  precision: bf16-mixed  # BF16 (faster on A100)
```

**åŠ¹æœ**:
- BF16ã¯A100ã§10-20%é«˜é€Ÿ
- æ•°å€¤å®‰å®šæ€§ãŒå‘ä¸Š
- ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼/ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼å•é¡Œã®è»½æ¸›

---

### ğŸŸ¢ Priority 5: torch.compileæœ€é©åŒ–

**å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«**: `configs/atft/model/atft_gat_fan.yaml`

```yaml
# BEFORE
optimization:
  compile:
    mode: default

# AFTER
optimization:
  compile:
    mode: max-autotune  # Maximum A100 optimization
```

**è¿½åŠ **: `scripts/train_atft.py`ã«ãƒ­ã‚°æ©Ÿèƒ½è¿½åŠ 

```python
logger.info(f"ğŸ”§ torch.compile enabled: mode={compile_mode}, dynamic={compile_dynamic}")
model = torch.compile(model, mode=compile_mode, dynamic=compile_dynamic)
logger.info("âœ… torch.compile applied successfully")
```

**åŠ¹æœ**:
- æ¨è«–é€Ÿåº¦10-30%å‘ä¸Š
- A100å‘ã‘ã‚«ãƒ¼ãƒãƒ«è‡ªå‹•æœ€é©åŒ–
- å®Ÿè¡ŒçŠ¶æ…‹ã®å¯è¦–åŒ–

---

### ğŸš€ Bonus: A100å°‚ç”¨æœ€é©åŒ–

**è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«**: `scripts/train_atft.py`

```python
if torch.cuda.is_available():
    # Enable TF32 for faster matmul on A100
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    # Enable cuDNN benchmark
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    logger.info("ğŸš€ A100 optimizations enabled: TF32=True, cudnn_benchmark=True")
```

**åŠ¹æœ**:
- TF32ã§è¡Œåˆ—è¨ˆç®—ãŒé«˜é€ŸåŒ–
- cuDNNãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€é©ã‚«ãƒ¼ãƒãƒ«é¸æŠ
- ã•ã‚‰ã«5-10%ã®æ€§èƒ½å‘ä¸Š

---

## ğŸ“ˆ äºˆæƒ³ã•ã‚Œã‚‹æ€§èƒ½æ”¹å–„

| æŒ‡æ¨™ | å¤‰æ›´å‰ | å¤‰æ›´å¾Œï¼ˆäºˆæƒ³ï¼‰ | å‘ä¸Šç‡ |
|------|--------|----------------|--------|
| **GPUä½¿ç”¨ç‡** | 0% | 80-90% | **âˆ** |
| **Validationé€Ÿåº¦** | 2.1ç§’/iter | 0.2-0.3ç§’/iter | **7-10å€** |
| **Epochæ™‚é–“** | 3-4æ™‚é–“ | 20-30åˆ† | **6-8å€** |
| **120 epochså®Œäº†** | 15-20æ—¥ | **2-3æ—¥** | **5-7å€** |
| **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** | ~1,000 samples/sec | 10,000-15,000 samples/sec | **10å€** |

---

## ğŸ”§ å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

1. âœ… `configs/atft/config_production_optimized.yaml`
   - `graph_builder.use_in_training: false`

2. âœ… `configs/atft/train/production.yaml`
   - `num_workers: 8`
   - `prefetch_factor: 4`
   - `gradient_accumulation_steps: 2`
   - `precision: bf16-mixed`

3. âœ… `configs/atft/model/atft_gat_fan.yaml`
   - `optimization.compile.mode: max-autotune`

4. âœ… `scripts/train_atft.py`
   - torch.compileãƒ­ã‚°è¿½åŠ 
   - A100æœ€é©åŒ–ï¼ˆTF32, cuDNN benchmarkï¼‰
   - GPUæƒ…å ±ãƒ­ã‚°è¿½åŠ 

---

## ğŸ¯ æ¬¡å›å­¦ç¿’å®Ÿè¡Œæ™‚ã®ç¢ºèªãƒã‚¤ãƒ³ãƒˆ

### ãƒ­ã‚°ã§ç¢ºèªã™ã¹ãé …ç›®

1. **A100æœ€é©åŒ–ã®èµ·å‹•ãƒ­ã‚°**:
   ```
   ğŸš€ A100 optimizations enabled: TF32=True, cudnn_benchmark=True
   ğŸ® GPU: NVIDIA A100-PCIE-80GB (80.0GB)
   ```

2. **torch.compileã®èµ·å‹•ãƒ­ã‚°**:
   ```
   ğŸ”§ torch.compile enabled: mode=max-autotune, dynamic=False, fullgraph=False
   âœ… torch.compile applied successfully
   ```

3. **GPUä½¿ç”¨ç‡**:
   ```bash
   nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits
   # æœŸå¾…å€¤: 80-90
   ```

4. **Validationé€Ÿåº¦**:
   ```
   Validation:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5320/6291 [00:01:10<00:00:11, 85.23 it/s]
   # æœŸå¾…å€¤: ç´„80-100 it/sï¼ˆç¾åœ¨ã¯ç´„0.5 it/sï¼‰
   ```

---

## ğŸš¨ é‡è¦ãªæ³¨æ„äº‹é …

### ç¾åœ¨å®Ÿè¡Œä¸­ã®ãƒ—ãƒ­ã‚»ã‚¹

**PID 433782**ã§å­¦ç¿’ãŒå®Ÿè¡Œä¸­ã§ã™ã€‚

**æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**:
```bash
# ç¾åœ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢
kill 433782

# æ•°ç§’å¾…æ©Ÿ
sleep 5

# æ–°ã—ã„æœ€é©åŒ–è¨­å®šã§å†å®Ÿè¡Œ
make train-optimized
```

### æ—¢å­˜ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿

- wandb run: `run-20251001_120252-cwdm7329`
- é€²æ—: 84% (5320/6291 iterations)
- **çµè«–**: æœ€é©åŒ–åŠ¹æœãŒåœ§å€’çš„ãªãŸã‚ã€å†é–‹ã‚ˆã‚Šã‚‚å†ã‚¹ã‚¿ãƒ¼ãƒˆã‚’æ¨å¥¨

---

## ğŸ“Š æœ€é©åŒ–ã®ç†è«–çš„æ ¹æ‹ 

### 1. ã‚°ãƒ©ãƒ•ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ï¼ˆæœ€å¤§ã®åŠ¹æœï¼‰

**å•é¡Œ**: æ¯ãƒãƒƒãƒã§ã‚°ãƒ©ãƒ•æ§‹ç¯‰ï¼ˆ0.2ç§’ Ã— 6291 iterations = 21åˆ†ã®ç„¡é§„ï¼‰

**è§£æ±º**: äº‹å‰æ§‹ç¯‰ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã‚’ä½¿ç”¨
- Validationãƒ•ã‚§ãƒ¼ã‚ºã§ã¯ã‚°ãƒ©ãƒ•æ§‹é€ ã¯å¤‰åŒ–ã—ãªã„
- äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§æ¯å›ã®å†æ§‹ç¯‰ã‚’å›é¿

**ç†è«–speedup**: 2.1ç§’ â†’ 0.2ç§’ = **10å€**

### 2. BF16 on A100

**FP16ã®å•é¡Œ**:
- Dynamic range ãŒç‹­ã„ï¼ˆæŒ‡æ•°éƒ¨5bitï¼‰
- ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼/ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼ãŒç™ºç”Ÿã—ã‚„ã™ã„
- GradScalerãŒå¿…è¦

**BF16ã®åˆ©ç‚¹**:
- Dynamic range ãŒåºƒã„ï¼ˆæŒ‡æ•°éƒ¨8bitã€FP32ã¨åŒã˜ï¼‰
- A100ã®Tensor CoreãŒBF16ã‚’æœ€é©åŒ–
- GradScalerãŒä¸è¦ã§å®‰å®š

**ç†è«–speedup**: 10-20%

### 3. torch.compile max-autotune

**defaultãƒ¢ãƒ¼ãƒ‰**: æ±ç”¨çš„ãªæœ€é©åŒ–
**max-autotuneãƒ¢ãƒ¼ãƒ‰**:
- A100å‘ã‘ã‚«ãƒ¼ãƒãƒ«è‡ªå‹•æ¢ç´¢
- ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æœ€é©åŒ–
- Fusionã®ç©æ¥µçš„é©ç”¨

**ç†è«–speedup**: 10-30%

### 4. TF32

**FP32ã®å•é¡Œ**: é«˜ç²¾åº¦ã ãŒé…ã„
**TF32ã®åˆ©ç‚¹**:
- 10bit mantissaï¼ˆFP32ã®23bitã‚ˆã‚Šä½ã„ï¼‰
- FP32ã®ç²¾åº¦ã‚’ã»ã¼ç¶­æŒ
- **8å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**ï¼ˆA100 Tensor Coreï¼‰

**ç†è«–speedup**: è¡Œåˆ—è¨ˆç®—ã§5-8å€

---

## ğŸ“ å­¦ã‚“ã ã“ã¨

### ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã®é‡è¦æ€§

1. **GPUä½¿ç”¨ç‡0%ã¯ç•°å¸¸** - å³åº§ã«èª¿æŸ»ãŒå¿…è¦
2. **ãƒ­ã‚°åˆ†æ**: ã‚°ãƒ©ãƒ•æ§‹ç¯‰ãƒ­ã‚°ã‹ã‚‰åŸå› ç‰¹å®š
3. **ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°**: nvidia-smiã€wandbãƒ­ã‚°ã®æ´»ç”¨

### A100æœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1. **BF16 > FP16** - å¸¸ã«BF16ã‚’ä½¿ç”¨
2. **TF32æœ‰åŠ¹åŒ–** - ç„¡æ–™ã®8å€speedup
3. **torch.compile max-autotune** - A100å‘ã‘æœ€é©åŒ–
4. **é©åˆ‡ãªbatch size** - A100 80GBãªã‚‰4096-8192

### è¨­å®šã®é€£é–

1. ã‚°ãƒ©ãƒ•æ§‹ç¯‰ â†’ GPUä½¿ç”¨ç‡
2. DataLoaderãƒ¯ãƒ¼ã‚«ãƒ¼ â†’ ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯
3. Precisionè¨­å®š â†’ è¨ˆç®—é€Ÿåº¦ã¨å®‰å®šæ€§

---

## ğŸ“ ä»Šå¾Œã®æ”¹å–„æ¡ˆ

### ã•ã‚‰ãªã‚‹æœ€é©åŒ–ï¼ˆOptionalï¼‰

1. **Flash Attention 2**: Attentionãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é«˜é€ŸåŒ–
2. **Gradient Checkpointing**: ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼ˆbatch sizeå¢—åŠ å¯èƒ½ï¼‰
3. **Multi-GPU**: è¤‡æ•°GPUä½¿ç”¨ã§ç·šå½¢speedup
4. **Async DataLoading**: CPUä¸¦åˆ—å‡¦ç†ã®æœ€å¤§åŒ–

### ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–

1. **GPUä½¿ç”¨ç‡ã®è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆ**: <70%ã§è­¦å‘Š
2. **Throughput tracking**: samples/secã‚’ç¶™ç¶šè¨˜éŒ²
3. **Per-layer profiling**: ã©ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒé…ã„ã‹ç‰¹å®š

---

## âœ… ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [x] GPUä½¿ç”¨ç‡0%ã®åŸå› ç‰¹å®š
- [x] ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã®æœ€é©åŒ–
- [x] DataLoaderè¨­å®šã®æœ€é©åŒ–
- [x] Batch size & Gradient Accumulation
- [x] Mixed Precision (BF16)
- [x] torch.compile max-autotune
- [x] A100å°‚ç”¨æœ€é©åŒ–ï¼ˆTF32, cuDNNï¼‰
- [x] ãƒ­ã‚°æ©Ÿèƒ½è¿½åŠ 
- [x] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
- [ ] **æ–°ã—ã„å­¦ç¿’ã®å®Ÿè¡Œã¨æ¤œè¨¼**

---

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

```bash
# 1. ç¾åœ¨ã®å­¦ç¿’ã‚’åœæ­¢
kill 433782

# 2. æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šã§å†å®Ÿè¡Œ
cd /home/ubuntu/gogooku3-standalone
make train-optimized

# 3. GPUä½¿ç”¨ç‡ã‚’ç¢ºèªï¼ˆåˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ï¼‰
watch -n 1 nvidia-smi

# æœŸå¾…ã•ã‚Œã‚‹çµæœ:
# - GPUä½¿ç”¨ç‡: 80-90%
# - Validation: 80-100 it/s
# - Epochæ™‚é–“: 20-30åˆ†
```

---

## ğŸ“š å‚è€ƒè³‡æ–™

- PyTorch Mixed Precision Training: https://pytorch.org/docs/stable/amp.html
- torch.compile: https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html
- A100 TF32 Performance: https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/
- DataLoader Best Practices: https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading

---

**ä½œæˆæ—¥**: 2025-10-01 12:45
**ä½œæˆè€…**: Claude Code Optimization
**æ¨å®šåŠ¹æœ**: å­¦ç¿’é€Ÿåº¦ **5-7å€å‘ä¸Š**ã€120 epochså®Œäº†æ™‚é–“ **15-20æ—¥ â†’ 2-3æ—¥**
