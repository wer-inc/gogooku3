#!/usr/bin/env python3
"""
APEX-Ranker v0 Prediction Monitoring & Logging

Simple monitoring utility for production inference.
Logs predictions and generates daily summary reports.

Usage:
    # Log a single prediction
    python scripts/monitor_predictions.py log \\
        --predictions predictions_20251029.csv \\
        --log-dir logs/predictions

    # Generate daily summary
    python scripts/monitor_predictions.py summary \\
        --log-dir logs/predictions \\
        --date 2025-10-29

Author: Generated by Claude Code
Date: 2025-10-29
"""
from __future__ import annotations

import argparse
import json
from datetime import datetime
from pathlib import Path

import polars as pl


def log_predictions(
    predictions_path: str | Path,
    log_dir: str | Path,
    metadata: dict | None = None,
):
    """
    Log predictions to structured storage.

    Args:
        predictions_path: Path to predictions CSV file
        log_dir: Directory to store logs
        metadata: Optional metadata (model version, config, etc.)
    """
    log_dir = Path(log_dir)
    log_dir.mkdir(parents=True, exist_ok=True)

    # Load predictions
    predictions = pl.read_csv(predictions_path)

    if "Date" not in predictions.columns:
        raise ValueError("Predictions CSV must have 'Date' column")

    date_str = predictions["Date"][0]
    timestamp = datetime.now().isoformat()

    # Create log entry
    log_entry = {
        "timestamp": timestamp,
        "date": date_str,
        "num_predictions": len(predictions),
        "metadata": metadata or {},
        "statistics": {
            "score_mean": float(predictions["Score"].mean()),
            "score_std": float(predictions["Score"].std()),
            "score_min": float(predictions["Score"].min()),
            "score_max": float(predictions["Score"].max()),
        },
        "top_10_codes": predictions["Code"][:10].to_list(),
    }

    # Save log entry
    log_file = log_dir / f"predictions_{date_str.replace('-', '')}_{timestamp[:19].replace(':', '')}.json"
    with open(log_file, "w") as f:
        json.dump(log_entry, f, indent=2)

    # Also save predictions CSV
    pred_file = log_dir / f"predictions_{date_str.replace('-', '')}.csv"
    predictions.write_csv(pred_file)

    print(f"[Monitor] Logged predictions to: {log_dir}")
    print(f"  - Log: {log_file.name}")
    print(f"  - Data: {pred_file.name}")
    print(f"  - Stocks: {len(predictions)}")
    print(f"  - Score range: [{log_entry['statistics']['score_min']:.4f}, {log_entry['statistics']['score_max']:.4f}]")


def generate_summary(
    log_dir: str | Path,
    target_date: str | None = None,
    output_file: str | Path | None = None,
):
    """
    Generate summary report from prediction logs.

    Args:
        log_dir: Directory containing prediction logs
        target_date: Optional date to summarize (YYYY-MM-DD). If None, summarizes all.
        output_file: Optional output file path. If None, prints to stdout.
    """
    log_dir = Path(log_dir)
    if not log_dir.exists():
        raise FileNotFoundError(f"Log directory not found: {log_dir}")

    # Find log files
    if target_date:
        pattern = f"predictions_{target_date.replace('-', '')}*.json"
    else:
        pattern = "predictions_*.json"

    log_files = sorted(log_dir.glob(pattern))

    if not log_files:
        print(f"[Monitor] No log files found matching: {pattern}")
        return

    # Load all logs
    logs = []
    for log_file in log_files:
        with open(log_file) as f:
            logs.append(json.load(f))

    # Generate summary
    summary = {
        "generated_at": datetime.now().isoformat(),
        "log_directory": str(log_dir),
        "num_logs": len(logs),
        "date_range": {
            "start": min(log["date"] for log in logs),
            "end": max(log["date"] for log in logs),
        },
        "statistics": {
            "total_predictions": sum(log["num_predictions"] for log in logs),
            "avg_predictions_per_day": sum(log["num_predictions"] for log in logs) / len(logs),
            "score_stats": {
                "mean_of_means": sum(log["statistics"]["score_mean"] for log in logs) / len(logs),
                "overall_min": min(log["statistics"]["score_min"] for log in logs),
                "overall_max": max(log["statistics"]["score_max"] for log in logs),
            },
        },
        "daily_logs": [
            {
                "date": log["date"],
                "timestamp": log["timestamp"],
                "num_predictions": log["num_predictions"],
                "score_mean": log["statistics"]["score_mean"],
                "top_3_codes": log["top_10_codes"][:3],
            }
            for log in logs
        ],
    }

    # Output
    if output_file:
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w") as f:
            json.dump(summary, f, indent=2)
        print(f"[Monitor] Summary saved to: {output_path}")
    else:
        print("\n" + "=" * 80)
        print("APEX-Ranker v0 - Prediction Summary")
        print("=" * 80)
        print(f"Period: {summary['date_range']['start']} to {summary['date_range']['end']}")
        print(f"Total logs: {summary['num_logs']}")
        print(f"Total predictions: {summary['statistics']['total_predictions']}")
        print(f"Avg predictions/day: {summary['statistics']['avg_predictions_per_day']:.1f}")
        print(f"\nScore statistics:")
        print(f"  Mean: {summary['statistics']['score_stats']['mean_of_means']:.4f}")
        print(f"  Range: [{summary['statistics']['score_stats']['overall_min']:.4f}, {summary['statistics']['score_stats']['overall_max']:.4f}]")
        print(f"\nDaily breakdown:")
        for daily in summary["daily_logs"]:
            print(f"  {daily['date']}: {daily['num_predictions']} stocks, score_mean={daily['score_mean']:.4f}, top3={daily['top_3_codes']}")


def parse_args():
    parser = argparse.ArgumentParser(
        description="Monitor and log APEX-Ranker predictions"
    )
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Log command
    log_parser = subparsers.add_parser("log", help="Log predictions")
    log_parser.add_argument(
        "--predictions",
        required=True,
        help="Path to predictions CSV file",
    )
    log_parser.add_argument(
        "--log-dir",
        default="logs/predictions",
        help="Directory to store logs (default: logs/predictions)",
    )
    log_parser.add_argument(
        "--model-version",
        default=None,
        help="Optional model version identifier",
    )

    # Summary command
    summary_parser = subparsers.add_parser("summary", help="Generate summary report")
    summary_parser.add_argument(
        "--log-dir",
        default="logs/predictions",
        help="Directory containing logs (default: logs/predictions)",
    )
    summary_parser.add_argument(
        "--date",
        default=None,
        help="Target date (YYYY-MM-DD). If not specified, summarizes all logs.",
    )
    summary_parser.add_argument(
        "--output",
        default=None,
        help="Output JSON file. If not specified, prints to stdout.",
    )

    return parser.parse_args()


def main():
    args = parse_args()

    if args.command == "log":
        metadata = {}
        if args.model_version:
            metadata["model_version"] = args.model_version

        log_predictions(
            args.predictions,
            args.log_dir,
            metadata=metadata,
        )

    elif args.command == "summary":
        generate_summary(
            args.log_dir,
            target_date=args.date,
            output_file=args.output,
        )


if __name__ == "__main__":
    main()
