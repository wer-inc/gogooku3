#!/usr/bin/env python3
"""
A+å®Ÿè£…ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‹ç¢ºèª
"""

import sys
from pathlib import Path
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# Add project root to path
sys.path.append(str(Path(__file__).parent))


def test_fan_san_enabled():
    """Test 1: FAN/SANãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    logger.info("=" * 60)
    logger.info("Test 1: FAN/SANæœ‰åŠ¹åŒ–ç¢ºèª")
    logger.info("=" * 60)
    
    try:
        from src.models.architectures.atft_gat_fan import ATFT_GAT_FAN
        from types import SimpleNamespace
        
        # æœ€å°è¨­å®šã§ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        config = SimpleNamespace(
            model=SimpleNamespace(
                hidden_size=64,
                input_projection=SimpleNamespace(dropout=0.1),
                # adaptive_normalizationã‚’æ„å›³çš„ã«çœç•¥
            ),
            data=SimpleNamespace(
                input_features=SimpleNamespace(
                    dynamic_features=10,
                    static_features=5,
                    use_static_features=False
                ),
                time_series=SimpleNamespace(
                    sequence_length=20,
                    prediction_horizons=[1, 5]
                )
            )
        )
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§FAN/SANãŒæœ‰åŠ¹ã«ãªã‚‹ã¯ãšï¼‰
        model = ATFT_GAT_FAN(config)
        
        # FAN/SANã®ãƒã‚§ãƒƒã‚¯
        import torch.nn as nn
        fan_enabled = not isinstance(model.fan, nn.Identity)
        san_enabled = not isinstance(model.san, nn.Identity)
        
        logger.info(f"âœ… FAN enabled: {fan_enabled}")
        logger.info(f"âœ… SAN enabled: {san_enabled}")
        
        if fan_enabled and san_enabled:
            logger.info("âœ… Test 1 PASSED: FAN/SAN are enabled by default")
            return True
        else:
            logger.error("âŒ Test 1 FAILED: FAN/SAN are not enabled")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Test 1 FAILED: {e}")
        return False


def test_phase_runner():
    """Test 2: Phaseãƒ©ãƒ³ãƒŠãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª"""
    logger.info("\n" + "=" * 60)
    logger.info("Test 2: Phaseãƒ©ãƒ³ãƒŠãƒ¼ç¢ºèª")
    logger.info("=" * 60)
    
    try:
        # train_atft.pyã‹ã‚‰run_phase_trainingé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        import importlib.util
        spec = importlib.util.spec_from_file_location(
            "train_atft",
            "scripts/train_atft.py"
        )
        train_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(train_module)
        
        # é–¢æ•°ã®å­˜åœ¨ç¢ºèª
        has_phase_runner = hasattr(train_module, 'run_phase_training')
        
        if has_phase_runner:
            logger.info("âœ… run_phase_training function found")
            
            # é–¢æ•°ã®ã‚·ã‚°ãƒãƒãƒ£ç¢ºèª
            import inspect
            sig = inspect.signature(train_module.run_phase_training)
            params = list(sig.parameters.keys())
            expected_params = ['model', 'train_loader', 'val_loader', 'config', 'device']
            
            if params == expected_params:
                logger.info(f"âœ… Function signature correct: {params}")
                logger.info("âœ… Test 2 PASSED: Phase runner implemented correctly")
                return True
            else:
                logger.warning(f"âš ï¸ Function signature mismatch: {params} vs {expected_params}")
                return False
        else:
            logger.error("âŒ Test 2 FAILED: run_phase_training not found")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Test 2 FAILED: {e}")
        return False


def test_correlation_edges():
    """Test 3: ç›¸é–¢ã‚¨ãƒƒã‚¸æ§‹ç¯‰ãŒå‹•ä½œã™ã‚‹ã‹ç¢ºèª"""
    logger.info("\n" + "=" * 60)
    logger.info("Test 3: ç›¸é–¢ã‚¨ãƒƒã‚¸æ§‹ç¯‰ç¢ºèª")
    logger.info("=" * 60)
    
    try:
        import torch
        from src.graph.graph_builder import GraphBuilder, GBConfig
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        batch_size = 30
        seq_len = 20
        n_features = 10
        
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®ç‰¹å¾´é‡ã‚’ãƒªã‚¿ãƒ¼ãƒ³ã¨ä»®å®šï¼‰
        features = torch.randn(batch_size, seq_len, n_features)
        
        # GraphBuilderåˆæœŸåŒ–
        config = GBConfig(max_nodes=100, edge_threshold=0.3)
        graph_builder = GraphBuilder(config)
        
        # ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        edge_index, edge_attr = graph_builder.build_graph(features)
        
        logger.info(f"âœ… Graph built: {edge_index.shape[1]} edges")
        logger.info(f"âœ… Edge attributes shape: {edge_attr.shape}")
        
        # ç›¸é–¢ã‚¨ãƒƒã‚¸ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if hasattr(graph_builder, 'build_correlation_edges'):
            logger.info("âœ… build_correlation_edges method exists")
            
            # ç›´æ¥ç›¸é–¢ã‚¨ãƒƒã‚¸ã‚’æ§‹ç¯‰
            edge_index_corr, edge_attr_corr = graph_builder.build_correlation_edges(features)
            logger.info(f"âœ… Correlation edges: {edge_index_corr.shape[1]} edges")
            
            # ã‚¨ãƒƒã‚¸å±æ€§ãŒ[0, 1]ç¯„å›²ã‹ç¢ºèª
            if edge_attr_corr.min() >= 0 and edge_attr_corr.max() <= 1:
                logger.info(f"âœ… Edge attributes in [0, 1] range: [{edge_attr_corr.min():.3f}, {edge_attr_corr.max():.3f}]")
                logger.info("âœ… Test 3 PASSED: Correlation edges working correctly")
                return True
            else:
                logger.warning(f"âš ï¸ Edge attributes out of range: [{edge_attr_corr.min():.3f}, {edge_attr_corr.max():.3f}]")
                return False
        else:
            logger.error("âŒ build_correlation_edges method not found")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Test 3 FAILED: {e}")
        return False


def test_wf_evaluation():
    """Test 4: WFè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå­˜åœ¨ã—å®Ÿè¡Œå¯èƒ½ã‹ç¢ºèª"""
    logger.info("\n" + "=" * 60)
    logger.info("Test 4: WFè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆç¢ºèª")
    logger.info("=" * 60)
    
    try:
        script_path = Path("scripts/evaluate_with_wf.py")
        
        if not script_path.exists():
            logger.error(f"âŒ Script not found: {script_path}")
            return False
        
        logger.info(f"âœ… Script exists: {script_path}")
        
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
        import importlib.util
        spec = importlib.util.spec_from_file_location("evaluate_wf", script_path)
        eval_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(eval_module)
        
        # å¿…è¦ãªé–¢æ•°ã®å­˜åœ¨ç¢ºèª
        required_functions = [
            'compute_hit_rate',
            'compute_max_drawdown',
            'compute_rank_ic',
            'load_model',
            'evaluate_fold',
            'main'
        ]
        
        missing = []
        for func in required_functions:
            if not hasattr(eval_module, func):
                missing.append(func)
        
        if missing:
            logger.warning(f"âš ï¸ Missing functions: {missing}")
            return False
        
        logger.info(f"âœ… All required functions found: {required_functions}")
        
        # WalkForwardSplitterV2ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
        from src.data.safety.walk_forward_v2 import WalkForwardSplitterV2
        logger.info("âœ… WalkForwardSplitterV2 imported successfully")
        
        logger.info("âœ… Test 4 PASSED: WF evaluation script ready")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Test 4 FAILED: {e}")
        return False


def main():
    """å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("ğŸš€ A+ Implementation Test Suite")
    logger.info("=" * 60)
    
    results = {
        "FAN/SANæœ‰åŠ¹åŒ–": test_fan_san_enabled(),
        "Phaseãƒ©ãƒ³ãƒŠãƒ¼": test_phase_runner(),
        "ç›¸é–¢ã‚¨ãƒƒã‚¸æ§‹ç¯‰": test_correlation_edges(),
        "WFè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ": test_wf_evaluation()
    }
    
    # çµæœã‚µãƒãƒªãƒ¼
    logger.info("\n" + "=" * 60)
    logger.info("Test Results Summary")
    logger.info("=" * 60)
    
    passed = 0
    for test_name, result in results.items():
        status = "âœ… PASSED" if result else "âŒ FAILED"
        logger.info(f"{test_name}: {status}")
        if result:
            passed += 1
    
    logger.info("-" * 60)
    logger.info(f"Total: {passed}/{len(results)} tests passed")
    
    if passed == len(results):
        logger.info("\nğŸ‰ All tests passed! A+ implementation is ready.")
        logger.info("\nğŸ“ Next steps:")
        logger.info("1. Run training with Phase Training:")
        logger.info("   python scripts/integrated_ml_training_pipeline.py \\")
        logger.info("     --data-path output/ml_dataset_*.parquet \\")
        logger.info("     --max-epochs 45 --batch-size 512")
        logger.info("\n2. Evaluate with Walk-Forward:")
        logger.info("   python scripts/evaluate_with_wf.py \\")
        logger.info("     --model-path output/checkpoints/best_model.pth \\")
        logger.info("     --data-path output/ml_dataset_*.parquet")
    else:
        logger.warning(f"\nâš ï¸ {len(results) - passed} tests failed. Please check the implementation.")
    
    return passed == len(results)


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)