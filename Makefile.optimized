# Optimized Training Targets

.PHONY: train-optimized-final
train-optimized-final: ## Run optimized training with all improvements
	@echo "üöÄ Starting optimized ATFT-GAT-FAN training"
	@echo "   ‚úÖ Based on log analysis results"
	@echo "   ‚úÖ All performance improvements applied"
	@bash train_optimized_final.sh

.PHONY: train-simple-optimized
train-simple-optimized: ## Simplified optimized training (recommended)
	@echo "üéØ SIMPLIFIED OPTIMIZED TRAINING"
	@echo "   Using integrated pipeline with optimal settings"
	@OUTPUT_BASE=/home/ubuntu/gogooku3-standalone/output \
	DATA_PATH=/home/ubuntu/gogooku3-standalone/output/ml_dataset_latest_full.parquet \
	NUM_WORKERS=0 \
	MIN_TRAINING_DATE="2018-01-01" \
	FEATURE_CLIP_VALUE=10.0 \
	USE_RANKIC=1 \
	RANKIC_WEIGHT=0.3 \
	SHARPE_WEIGHT=0.4 \
	python scripts/integrated_ml_training_pipeline.py \
		--data-path /home/ubuntu/gogooku3-standalone/output/ml_dataset_latest_full.parquet \
		--batch-size 1024 \
		--lr 1e-4 \
		--max-epochs 60 \
		--hidden-size 256 \
		--early-stopping \
		--patience 10

.PHONY: train-quick-test
train-quick-test: ## Quick test with optimized settings (5 epochs)
	@echo "üß™ Quick test run (5 epochs)"
	@FEATURE_CLIP_VALUE=10.0 \
	NUM_WORKERS=0 \
	MIN_TRAINING_DATE="2020-01-01" \
	python scripts/train_atft.py \
		--config-path ../configs/atft \
		--config-name config_production_optimized \
		data.source.data_dir=/home/ubuntu/gogooku3-standalone/output/atft_data \
		model.hidden_size=256 \
		train.trainer.max_epochs=5 \
		train.batch.batch_size=512 \
		train.optimizer.lr=1e-4

.PHONY: monitor-training
monitor-training: ## Monitor current training progress
	@echo "üìä Monitoring training progress..."
	@tail -f logs/ml_training.log | grep -E "(Epoch|Loss|RankIC|Sharpe|WARNING|ERROR)"

.PHONY: check-gpu
check-gpu: ## Check GPU status
	@echo "üñ•Ô∏è GPU Status:"
	@nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits
	@echo ""
	@echo "üìä GPU Memory Usage:"
	@nvidia-smi

.PHONY: stop-training
stop-training: ## Safely stop current training
	@echo "üõë Stopping training processes..."
	@pkill -f "train_atft.py" || true
	@pkill -f "integrated_ml_training_pipeline.py" || true
	@echo "‚úÖ Training stopped. Checkpoints are saved."

.PHONY: resume-training
resume-training: ## Resume from last checkpoint
	@echo "üîÑ Resuming from last checkpoint..."
	@CHECKPOINT=$$(ls -t output/checkpoints/*.ckpt 2>/dev/null | head -1); \
	if [ -n "$$CHECKPOINT" ]; then \
		echo "Found checkpoint: $$CHECKPOINT"; \
		python scripts/train_atft.py \
			--config-path ../configs/atft \
			--config-name config_production_optimized \
			train.resume_from_checkpoint="$$CHECKPOINT"; \
	else \
		echo "‚ùå No checkpoint found"; \
	fi

.PHONY: analyze-results
analyze-results: ## Analyze training results
	@echo "üìà Analyzing training results..."
	@python scripts/analyze_training_results.py

.PHONY: help-optimized
help-optimized: ## Show optimization help
	@echo "================================"
	@echo "OPTIMIZED TRAINING GUIDE"
	@echo "================================"
	@echo ""
	@echo "1. Stop current training (if running):"
	@echo "   make -f Makefile.optimized stop-training"
	@echo ""
	@echo "2. Start optimized training:"
	@echo "   make -f Makefile.optimized train-optimized-final"
	@echo ""
	@echo "3. Monitor progress:"
	@echo "   make -f Makefile.optimized monitor-training"
	@echo ""
	@echo "4. Check GPU usage:"
	@echo "   make -f Makefile.optimized check-gpu"
	@echo ""
	@echo "Key improvements applied:"
	@echo "  ‚úÖ Hidden size: 64 ‚Üí 256"
	@echo "  ‚úÖ Feature clipping: Enabled (10.0)"
	@echo "  ‚úÖ Learning rate: 1e-4 (optimized)"
	@echo "  ‚úÖ Batch size: 1024 (larger)"
	@echo "  ‚úÖ Early stopping: 10 epochs patience"
	@echo "  ‚úÖ RankIC weight: 0.3"
	@echo "  ‚úÖ Sharpe weight: 0.4"
	@echo ""
	@echo "Expected improvements:"
	@echo "  ‚Ä¢ Better convergence"
	@echo "  ‚Ä¢ Higher RankIC"
	@echo "  ‚Ä¢ Reduced overfitting"
	@echo "  ‚Ä¢ Faster training (60 epochs max)"