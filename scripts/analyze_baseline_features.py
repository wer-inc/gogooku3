#!/usr/bin/env python3
"""
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMï¼‰ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’åˆ†æã—ã¦ã€
ATFT-GAT-FANãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„ã«æ´»ç”¨
"""

import warnings

import numpy as np
import pandas as pd
from lightgbm import LGBMRegressor
from scipy.stats import spearmanr

warnings.filterwarnings("ignore")


def calculate_rankic(y_true, y_pred):
    """RankICè¨ˆç®—"""
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    if mask.sum() < 10:
        return np.nan
    corr, _ = spearmanr(y_true[mask], y_pred[mask])
    return corr


def analyze_feature_importance(data_path="output/ml_dataset_future_returns.parquet"):
    """ç‰¹å¾´é‡é‡è¦åº¦ã‚’åˆ†æã—ã¦ATFTã®æ”¹å–„ã«æ´»ç”¨"""

    print("=" * 60)
    print("ğŸ” Feature Importance Analysis for ATFT Improvement")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“‚ Loading data...")
    df = pd.read_parquet(data_path)
    print(f"âœ… Data shape: {df.shape}")

    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    feature_cols = [
        col
        for col in df.columns
        if not col.startswith("returns_") and col not in ["Date", "Code"]
    ]

    # æ•°å€¤ç‰¹å¾´é‡ã®ã¿
    numeric_features = [
        col
        for col in feature_cols
        if df[col].dtype in ["float64", "float32", "int64", "int32"]
    ]

    print(f"\nğŸ“Š Features: {len(numeric_features)}")

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    df = df.sort_values("Date")

    # Train/Valåˆ†å‰²
    split_date = df["Date"].quantile(0.8)
    train_df = df[df["Date"] < split_date]
    val_df = df[df["Date"] >= split_date]

    X_train = train_df[numeric_features].fillna(0).clip(-10, 10)
    X_val = val_df[numeric_features].fillna(0).clip(-10, 10)

    # è¤‡æ•°ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ç‰¹å¾´é‡é‡è¦åº¦ã‚’åé›†
    feature_importance_dict = {}
    performance_dict = {}

    for target_col in ["returns_1d", "returns_5d", "returns_10d", "returns_20d"]:
        print(f"\nğŸ¯ Analyzing {target_col}...")

        y_train = train_df[target_col].fillna(0)
        y_val = val_df[target_col].fillna(0)

        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™
        sample_size = min(100000, len(X_train))
        indices = np.random.choice(len(X_train), sample_size, replace=False)

        # LightGBMãƒ¢ãƒ‡ãƒ«ï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰
        lgb = LGBMRegressor(
            n_estimators=300,  # å¢—åŠ 
            max_depth=7,  # å¢—åŠ 
            learning_rate=0.05,  # èª¿æ•´
            num_leaves=63,  # å¢—åŠ 
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,  # L1æ­£å‰‡åŒ–
            reg_lambda=0.1,  # L2æ­£å‰‡åŒ–
            random_state=42,
            verbosity=-1,
            n_jobs=-1,
        )

        # å­¦ç¿’
        lgb.fit(
            X_train.iloc[indices],
            y_train.iloc[indices],
            eval_set=[(X_val.iloc[:10000], y_val.iloc[:10000])],
            callbacks=[lambda x: None],
        )

        # äºˆæ¸¬ã¨RankICè¨ˆç®—
        y_pred = lgb.predict(X_val)
        rankic = calculate_rankic(y_val.values, y_pred)

        print(f"   RankIC: {rankic:.6f}")
        performance_dict[target_col] = rankic

        # ç‰¹å¾´é‡é‡è¦åº¦
        importance = pd.DataFrame(
            {"feature": numeric_features, "importance": lgb.feature_importances_}
        ).sort_values("importance", ascending=False)

        feature_importance_dict[target_col] = importance

        # Top10ç‰¹å¾´é‡
        print("   Top 10 features:")
        for idx, row in importance.head(10).iterrows():
            print(f"     {row['feature']}: {row['importance']:.0f}")

    # å…¨ä½“ã®ç‰¹å¾´é‡é‡è¦åº¦é›†è¨ˆ
    print("\n" + "=" * 60)
    print("ğŸ“ˆ Aggregated Feature Importance")
    print("=" * 60)

    # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®é‡è¦åº¦ã‚’å¹³å‡
    all_features = set()
    for importance_df in feature_importance_dict.values():
        all_features.update(importance_df["feature"].values)

    aggregated_importance = []
    for feature in all_features:
        importances = []
        for target_col, importance_df in feature_importance_dict.items():
            imp = importance_df[importance_df["feature"] == feature][
                "importance"
            ].values
            if len(imp) > 0:
                importances.append(imp[0])
            else:
                importances.append(0)
        aggregated_importance.append(
            {
                "feature": feature,
                "avg_importance": np.mean(importances),
                "max_importance": np.max(importances),
                "std_importance": np.std(importances),
            }
        )

    aggregated_df = pd.DataFrame(aggregated_importance).sort_values(
        "avg_importance", ascending=False
    )

    print("\nTop 20 Most Important Features (Averaged):")
    for idx, row in aggregated_df.head(20).iterrows():
        print(
            f"  {row['feature']}: avg={row['avg_importance']:.1f}, max={row['max_importance']:.1f}"
        )

    # ç‰¹å¾´é‡ã‚«ãƒ†ã‚´ãƒªåˆ†æ
    print("\n" + "=" * 60)
    print("ğŸ·ï¸ Feature Category Analysis")
    print("=" * 60)

    # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
    categories = {
        "price": ["Open", "High", "Low", "Close", "VWAP"],
        "volume": ["Volume", "TurnoverValue"],
        "return": ["ret_", "return"],
        "technical": ["rsi", "bb_", "macd", "sma", "ema", "adx", "cci", "stoch"],
        "market_cap": ["MarketCap", "market_cap"],
        "fundamental": ["per", "pbr", "psr", "dividend"],
        "volatility": ["volatility", "std", "atr"],
        "momentum": ["momentum", "roc"],
        "liquidity": ["liquidity", "spread", "turnover"],
    }

    category_importance = {}
    for category, keywords in categories.items():
        category_features = []
        for feature in aggregated_df["feature"]:
            for keyword in keywords:
                if keyword.lower() in feature.lower():
                    category_features.append(feature)
                    break

        if category_features:
            cat_importance = aggregated_df[
                aggregated_df["feature"].isin(category_features)
            ]["avg_importance"].sum()
            category_importance[category] = cat_importance
            print(f"  {category}: {cat_importance:.1f} (n={len(category_features)})")

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("ğŸ¯ Performance Summary")
    print("=" * 60)

    for target, rankic in performance_dict.items():
        print(f"  {target}: RankIC = {rankic:.6f}")

    avg_rankic = np.mean(list(performance_dict.values()))
    print(f"\n  Average RankIC: {avg_rankic:.6f}")

    # æ”¹å–„ææ¡ˆ
    print("\n" + "=" * 60)
    print("ğŸ’¡ Recommendations for ATFT-GAT-FAN")
    print("=" * 60)

    top_features = aggregated_df.head(20)["feature"].tolist()

    print("\n1. **Feature Engineering**:")
    print("   é‡è¦åº¦ã®é«˜ã„ç‰¹å¾´é‡ã‚’å¼·èª¿:")
    for i, feat in enumerate(top_features[:5], 1):
        print(f"   {i}. {feat}")

    print("\n2. **Feature Groups for Attention**:")
    sorted_categories = sorted(
        category_importance.items(), key=lambda x: x[1], reverse=True
    )
    for cat, imp in sorted_categories[:3]:
        print(f"   - {cat.upper()}: é«˜é‡è¦åº¦ã‚«ãƒ†ã‚´ãƒª")

    print("\n3. **Model Architecture**:")
    if avg_rankic > 0.07:
        print("   âœ… ãƒ‡ãƒ¼ã‚¿ã«äºˆæ¸¬å¯èƒ½æ€§ã‚ã‚Š")
        print("   â†’ ã‚ˆã‚Šæ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆhidden_size=256ä»¥ä¸Šï¼‰")
        print("   â†’ Feature-wise attentionå¼·åŒ–")
    else:
        print("   âš ï¸ äºˆæ¸¬å¯èƒ½æ€§ãŒä½ã„")
        print("   â†’ æ­£å‰‡åŒ–å¼·åŒ–")
        print("   â†’ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•æ¤œè¨")

    print("\n4. **Training Strategy**:")
    print("   - é‡è¦ç‰¹å¾´é‡ã«åŸºã¥ãã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’")
    print("   - Feature dropoutã§éå­¦ç¿’é˜²æ­¢")
    print("   - Multi-task learningï¼ˆè¤‡æ•°horizonåŒæ™‚å­¦ç¿’ï¼‰")

    return aggregated_df, performance_dict


if __name__ == "__main__":
    import sys

    data_path = (
        sys.argv[1] if len(sys.argv) > 1 else "output/ml_dataset_future_returns.parquet"
    )

    feature_importance, performance = analyze_feature_importance(data_path)

    # CSVã«ä¿å­˜
    output_path = "output/feature_importance_analysis.csv"
    feature_importance.to_csv(output_path, index=False)
    print(f"\nğŸ“Š Feature importance saved to: {output_path}")
