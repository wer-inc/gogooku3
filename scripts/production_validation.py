#!/usr/bin/env python3
"""
ATFT-GAT-FAN Production Workload Validation
æœ¬ç•ªãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã®æ€§èƒ½æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import logging
import os
import sys
import time
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Any

import psutil
import torch

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    import GPUtil
    GPU_UTIL_AVAILABLE = True
except ImportError:
    GPU_UTIL_AVAILABLE = False

# PyTorchãƒ¡ãƒ¢ãƒªç®¡ç†
torch.backends.cudnn.benchmark = True


@contextmanager
def memory_monitor():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()

    start_time = time.time()
    start_memory = psutil.virtual_memory().used

    if torch.cuda.is_available():
        start_gpu_memory = torch.cuda.memory_allocated()

    try:
        yield
    finally:
        end_time = time.time()
        end_memory = psutil.virtual_memory().used

        if torch.cuda.is_available():
            end_gpu_memory = torch.cuda.memory_allocated()
            peak_gpu_memory = torch.cuda.max_memory_allocated()

        memory_info = {
            'duration_seconds': end_time - start_time,
            'cpu_memory_used_mb': (end_memory - start_memory) / (1024**2),
            'cpu_memory_peak_mb': psutil.virtual_memory().used / (1024**2),
        }

        if torch.cuda.is_available():
            memory_info.update({
                'gpu_memory_used_mb': (end_gpu_memory - start_gpu_memory) / (1024**2),
                'gpu_memory_peak_mb': peak_gpu_memory / (1024**2),
            })

        logger.info(f"Memory usage: {memory_info}")


class ProductionValidator:
    """æœ¬ç•ªãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, data_path: str, config_path: str | None = None):
        self.data_path = Path(data_path)
        self.config_path = config_path or (project_root / "configs" / "atft" / "config.yaml")
        self.validation_results_dir = project_root / "validation_results"
        self.validation_results_dir.mkdir(exist_ok=True)

        # æ¤œè¨¼ã‚·ãƒŠãƒªã‚ªï¼ˆé«˜ã‚¹ãƒšãƒƒã‚¯ãƒã‚·ãƒ³å‘ã‘ã«æœ€é©åŒ–ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰
        self.scenarios = {
            'tiny_scale': {'batch_size': 256, 'sequence_length': 60, 'n_stocks': 50},
            'small_scale': {'batch_size': 512, 'sequence_length': 60, 'n_stocks': 100},
            'medium_scale': {'batch_size': 1024, 'sequence_length': 60, 'n_stocks': 200},
            'large_scale': {'batch_size': 2048, 'sequence_length': 60, 'n_stocks': 500},
            'production_scale': {'batch_size': 4096, 'sequence_length': 60, 'n_stocks': 1000}
        }

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
        self.metrics = {
            'throughput_samples_per_sec': 0,
            'memory_efficiency': 0,
            'training_stability': 0,
            'inference_speed': 0,
            'scalability_score': 0
        }

    def create_synthetic_data(self, batch_size: int, seq_length: int, n_stocks: int) -> dict[str, torch.Tensor]:
        """åˆæˆãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆæœ¬ç•ªè¦æ¨¡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        logger.info(f"Creating synthetic data: batch_size={batch_size}, seq_length={seq_length}, n_stocks={n_stocks}")

        # ç‰¹å¾´é‡æ¬¡å…ƒï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åŸºã¥ãï¼‰
        n_features = 8  # Basic + Technical + MA-derived + Interaction + Flow + Returns

        # å‹•çš„ç‰¹å¾´é‡
        dynamic_features = torch.randn(batch_size, seq_length, n_stocks, n_features)

        # é™çš„ç‰¹å¾´é‡
        static_features = torch.randn(batch_size, n_stocks, 4)  # éŠ˜æŸ„å›ºæœ‰ç‰¹å¾´é‡

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆè¤‡æ•°ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼‰
        targets = {
            'h1': torch.randn(batch_size, n_stocks),
            'h5': torch.randn(batch_size, n_stocks),
            'h10': torch.randn(batch_size, n_stocks),
            'h20': torch.randn(batch_size, n_stocks)
        }

        return {
            'dynamic_features': dynamic_features,
            'static_features': static_features,
            'targets': targets,
            'batch_size': batch_size,
            'seq_length': seq_length,
            'n_stocks': n_stocks
        }

    def create_mock_model(self):
        """ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰"""
        class MockATFTGATFAN(torch.nn.Module):
            def __init__(self, input_size: int = 8, hidden_size: int = 32):  # éš ã‚Œå±¤ã‚µã‚¤ã‚ºã‚’åŠåˆ†ã«
                super().__init__()
                self.input_projection = torch.nn.Linear(input_size, hidden_size)
                # LSTMã®ä»£ã‚ã‚Šã«GRUã‚’ä½¿ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰
                self.temporal_encoder = torch.nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1)
                self.output_head = torch.nn.Linear(hidden_size, 4)  # 4ãƒ›ãƒ©ã‚¤ã‚ºãƒ³

                # æ”¹å–„æ©Ÿèƒ½ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                self.freq_dropout = torch.nn.Dropout(0.2)  # æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                self.layer_scale = torch.nn.Parameter(torch.ones(hidden_size) * 0.1)

            def forward(self, x):
                # ãƒãƒƒãƒã‚µã‚¤ã‚ºç¢ºèª
                if x.dim() != 4:
                    raise ValueError(f"Expected 4D input, got {x.dim()}D")

                batch_size, seq_len, n_stocks, input_size = x.shape

                # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå‡¦ç†
                x = x.contiguous()  # ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæœ€é©åŒ–

                # å…¥åŠ›æŠ•å½±
                x = self.input_projection(x)
                x = torch.relu(x)

                # å‘¨æ³¢æ•°ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
                x = self.freq_dropout(x)

                # LayerScale
                x = x * self.layer_scale

                # æ™‚ç³»åˆ—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«ï¼‰
                x = x.view(batch_size * n_stocks, seq_len, -1)

                # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªRNNå‡¦ç†
                x, _ = self.temporal_encoder(x)

                x = x[:, -1, :]  # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—
                x = x.view(batch_size, n_stocks, -1)

                # å‡ºåŠ›
                output = self.output_head(x)
                return {
                    'predictions': output,
                    'hidden_states': x
                }

        return MockATFTGATFAN()

    def validate_batch_processing(self, scenario: str) -> dict[str, Any]:
        """ãƒãƒƒãƒå‡¦ç†æ¤œè¨¼"""
        logger.info(f"Validating batch processing for scenario: {scenario}")

        params = self.scenarios[scenario]
        batch_size = params['batch_size']

        # GPUä½¿ç”¨ç¢ºèªï¼ˆé«˜ã‚¹ãƒšãƒƒã‚¯ãƒã‚·ãƒ³ãªã®ã§GPUã‚’ä½¿ç”¨ï¼‰
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {device}")

        if torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            logger.info(f"GPU: {gpu_props.name}")
            logger.info(f"GPU Memory: {gpu_props.total_memory / (1024**3):.1f} GB")
            logger.info(f"CUDA Version: {torch.version.cuda}")

            # é«˜ã‚¹ãƒšãƒƒã‚¯GPUå‘ã‘ã®æœ€é©åŒ–è¨­å®š
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.enabled = True
            torch.cuda.empty_cache()  # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢

            # CUDAãƒ‡ãƒãƒƒã‚°è¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã«æœ‰åŠ¹åŒ–ï¼‰
            os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = self.create_mock_model().to(device)

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶è¨­å®š
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

        # æå¤±é–¢æ•°
        criterion = torch.nn.HuberLoss(delta=0.01)  # æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        results = {
            'scenario': scenario,
            'batch_size': batch_size,
            'device': str(device),
            'iterations': [],
            'memory_usage': [],
            'throughput': []
        }

        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        logger.info("Warm-up phase...")
        for _ in range(5):
            data = self.create_synthetic_data(batch_size, 60, 100)
            dynamic_features = data['dynamic_features'].to(device)

            with torch.no_grad():
                _ = model(dynamic_features)

        # æœ¬æ¤œè¨¼
        logger.info("Validation phase...")
        n_iterations = 10

        for i in range(n_iterations):
            logger.info(f"Iteration {i+1}/{n_iterations}")

            # ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            data = self.create_synthetic_data(batch_size, 60, 100)
            dynamic_features = data['dynamic_features'].to(device)
            targets = data['targets']['h1'].to(device)

            # é«˜ã‚¹ãƒšãƒƒã‚¯GPUå‘ã‘ãƒ¡ãƒ¢ãƒªç®¡ç†
            if torch.cuda.is_available() and i % 3 == 0:  # 3ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«ã‚¯ãƒªã‚¢
                torch.cuda.empty_cache()

            # ãƒ¡ãƒ¢ãƒªç›£è¦–ä»˜ããƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            with memory_monitor() as mem_info:
                start_time = time.time()

                # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
                optimizer.zero_grad()
                outputs = model(dynamic_features)
                predictions = outputs['predictions'][:, :, 0]  # h1äºˆæ¸¬

                # æå¤±è¨ˆç®—
                loss = criterion(predictions, targets)

                # ãƒãƒƒã‚¯ãƒ¯ãƒ¼ãƒ‰
                loss.backward()

                # å‹¾é…ã‚¯ãƒªãƒƒãƒ—
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.8)

                # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚¹ãƒ†ãƒƒãƒ—
                optimizer.step()

                end_time = time.time()

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
            iteration_time = end_time - start_time
            throughput = batch_size / iteration_time

            iteration_result = {
                'iteration': i,
                'loss': loss.item(),
                'time_seconds': iteration_time,
                'throughput_samples_per_sec': throughput,
                'memory_info': mem_info
            }

            results['iterations'].append(iteration_result)
            results['throughput'].append(throughput)

            logger.info(f"  Iteration {i+1}: loss={loss.item():.4f}, throughput={throughput:.2f} samples/sec")
        # çµ±è¨ˆè¨ˆç®—
        throughput_mean = sum(results['throughput']) / len(results['throughput'])
        throughput_std = torch.std(torch.tensor(results['throughput']))

        loss_values = [iter['loss'] for iter in results['iterations']]
        loss_mean = sum(loss_values) / len(loss_values)
        loss_std = torch.std(torch.tensor(loss_values))

        results['summary'] = {
            'throughput_mean': throughput_mean,
            'throughput_std': throughput_std,
            'loss_mean': loss_mean,
            'loss_std': loss_std,
            'stability_score': 1.0 / (1.0 + throughput_std / throughput_mean),  # å®‰å®šæ€§ã‚¹ã‚³ã‚¢
        }

        logger.info(f"Batch processing validation completed for {scenario}")
        logger.info(f"  Average throughput: {throughput_mean:.2f} samples/sec")
        logger.info(f"  Stability score: {results['summary']['stability_score']:.3f}")

        return results

    def validate_memory_efficiency(self) -> dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ¤œè¨¼"""
        logger.info("Validating memory efficiency...")

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        results = {
            'device': str(device),
            'batch_sizes': [],
            'memory_usage': [],
            'peak_memory': [],
            'efficiency_scores': []
        }

        # æ§˜ã€…ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ãƒ†ã‚¹ãƒˆ
        batch_sizes = [512, 1024, 2048, 4096]

        for batch_size in batch_sizes:
            logger.info(f"Testing batch size: {batch_size}")

            model = self.create_mock_model().to(device)
            data = self.create_synthetic_data(batch_size, 60, 100)
            dynamic_features = data['dynamic_features'].to(device)

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()

            with torch.no_grad():
                _ = model(dynamic_features)

            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / (1024**3)  # GB
                peak_memory = torch.cuda.max_memory_allocated() / (1024**3)  # GB

                # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚¹ã‚³ã‚¢ï¼ˆä½¿ç”¨ãƒ¡ãƒ¢ãƒª / ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªï¼‰
                efficiency = memory_used / peak_memory if peak_memory > 0 else 1.0

                results['batch_sizes'].append(batch_size)
                results['memory_usage'].append(memory_used)
                results['peak_memory'].append(peak_memory)
                results['efficiency_scores'].append(efficiency)

                logger.info(f"  Batch {batch_size}: {memory_used:.2f}GB used, {peak_memory:.2f}GB peak, efficiency={efficiency:.2f}")

        return results

    def validate_scalability(self) -> dict[str, Any]:
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ¤œè¨¼"""
        logger.info("Validating scalability...")

        results = {}

        for scenario_name, params in self.scenarios.items():
            logger.info(f"Testing scalability for {scenario_name}")

            try:
                batch_result = self.validate_batch_processing(scenario_name)
                results[scenario_name] = {
                    'success': True,
                    'throughput': batch_result['summary']['throughput_mean'],
                    'stability': batch_result['summary']['stability_score'],
                    'params': params
                }
            except Exception as e:
                logger.error(f"Failed to validate {scenario_name}: {e}")
                results[scenario_name] = {
                    'success': False,
                    'error': str(e),
                    'params': params
                }

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ
        successful_scenarios = [k for k, v in results.items() if v['success']]
        if len(successful_scenarios) >= 2:
            throughputs = [results[s]['throughput'] for s in successful_scenarios]
            scalability_score = throughputs[-1] / throughputs[0] if throughputs[0] > 0 else 0
            results['scalability_analysis'] = {
                'score': scalability_score,
                'improvement_ratio': scalability_score,
                'successful_scenarios': successful_scenarios
            }

        return results

    def run_full_validation(self) -> dict[str, Any]:
        """ãƒ•ãƒ«æ¤œè¨¼å®Ÿè¡Œ"""
        logger.info("Running full production workload validation...")

        validation_results = {
            'timestamp': datetime.now().isoformat(),
            'config_path': str(self.config_path),
            'data_path': str(self.data_path),
            'system_info': self._get_system_info(),
            'batch_processing': {},
            'memory_efficiency': {},
            'scalability': {},
            'overall_score': 0
        }

        # ãƒãƒƒãƒå‡¦ç†æ¤œè¨¼
        logger.info("=== Batch Processing Validation ===")
        validation_results['batch_processing'] = self.validate_batch_processing('medium_scale')

        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ¤œè¨¼
        logger.info("=== Memory Efficiency Validation ===")
        validation_results['memory_efficiency'] = self.validate_memory_efficiency()

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ¤œè¨¼
        logger.info("=== Scalability Validation ===")
        validation_results['scalability'] = self.validate_scalability()

        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = []

        # ãƒãƒƒãƒå‡¦ç†ã‚¹ã‚³ã‚¢
        if 'summary' in validation_results['batch_processing']:
            batch_score = validation_results['batch_processing']['summary']['stability_score']
            scores.append(batch_score)

        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚¹ã‚³ã‚¢
        if validation_results['memory_efficiency'].get('efficiency_scores'):
            memory_scores = validation_results['memory_efficiency']['efficiency_scores']
            memory_score = sum(memory_scores) / len(memory_scores) if memory_scores else 0
            scores.append(memory_score)

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢
        if 'scalability_analysis' in validation_results['scalability']:
            scalability_score = min(validation_results['scalability']['scalability_analysis']['score'], 1.0)
            scores.append(scalability_score)

        validation_results['overall_score'] = sum(scores) / len(scores) if scores else 0

        # çµæœä¿å­˜
        self._save_validation_results(validation_results)

        return validation_results

    def _get_system_info(self) -> dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—"""
        info = {
            'cpu_count': psutil.cpu_count(),
            'cpu_count_logical': psutil.cpu_count(logical=True),
            'memory_total_gb': psutil.virtual_memory().total / (1024**3),
            'pytorch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
        }

        if torch.cuda.is_available():
            info.update({
                'cuda_version': torch.version.cuda,
                'gpu_count': torch.cuda.device_count(),
                'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else 'Unknown'
            })

        return info

    def _save_validation_results(self, results: dict[str, Any]):
        """æ¤œè¨¼çµæœä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"production_validation_{timestamp}.json"

        result_file = self.validation_results_dir / filename
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"Validation results saved: {result_file}")

    def display_results(self, results: dict[str, Any]):
        """çµæœè¡¨ç¤º"""
        print("\n" + "="*80)
        print("ATFT-GAT-FAN PRODUCTION WORKLOAD VALIDATION RESULTS")
        print("="*80)

        print("\nğŸ“Š OVERVIEW")
        print(f"  Overall Score: {results.get('overall_score', 0):.3f}")
        print(f"  Timestamp: {results['timestamp']}")

        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        sys_info = results.get('system_info', {})
        print("\nğŸ–¥ï¸ SYSTEM INFO")
        print(f"  CPU Cores: {sys_info.get('cpu_count', 'N/A')}")
        print(f"  Memory: {sys_info.get('memory_total_gb', 0):.1f} GB")
        print(f"  CUDA Available: {sys_info.get('cuda_available', False)}")
        if sys_info.get('cuda_available'):
            print(f"  GPU: {sys_info.get('gpu_name', 'Unknown')}")
            print(f"  GPU Count: {sys_info.get('gpu_count', 0)}")

        # ãƒãƒƒãƒå‡¦ç†çµæœ
        if 'batch_processing' in results and 'summary' in results['batch_processing']:
            batch = results['batch_processing']['summary']
            print("\nâš¡ BATCH PROCESSING")
            print(f"  Throughput: {batch.get('throughput_mean', 0):.0f} samples/sec")
            print(f"  Stability Score: {batch.get('stability_score', 0):.3f}")
            print(f"  Loss: {batch.get('loss_mean', 0):.4f} Â± {batch.get('loss_std', 0):.4f}")

        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çµæœ
        if 'memory_efficiency' in results and results['memory_efficiency'].get('efficiency_scores'):
            memory = results['memory_efficiency']
            avg_efficiency = sum(memory['efficiency_scores']) / len(memory['efficiency_scores'])
            print("\nğŸ’¾ MEMORY EFFICIENCY")
            print(f"  Average Efficiency: {avg_efficiency:.3f}")
            print(f"  Tested Batch Sizes: {memory.get('batch_sizes', [])}")

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£çµæœ
        if 'scalability' in results:
            print("\nğŸ“ˆ SCALABILITY")
            for scenario, data in results['scalability'].items():
                if scenario != 'scalability_analysis' and isinstance(data, dict):
                    success = data.get('success', False)
                    status = "âœ…" if success else "âŒ"
                    if success:
                        throughput = data.get('throughput', 0)
                        stability = data.get('stability', 0)
                        print(f"  {status} {scenario}: {throughput:.0f} samples/sec, stability={stability:.3f}")
                    else:
                        print(f"  {status} {scenario}: Failed")

            if 'scalability_analysis' in results['scalability']:
                analysis = results['scalability']['scalability_analysis']
                print(f"  Scalability Score: {analysis.get('score', 0):.3f}")

        # è©•ä¾¡åŸºæº–
        overall_score = results.get('overall_score', 0)
        print("\nğŸ¯ EVALUATION")
        if overall_score >= 0.8:
            print("  Status: âœ… EXCELLENT - Ready for production")
        elif overall_score >= 0.6:
            print("  Status: âš ï¸ GOOD - Minor optimizations needed")
        else:
            print("  Status: âŒ NEEDS IMPROVEMENT - Further optimization required")

        print(f"  Overall Score: {overall_score:.3f}/1.0")

        print("\n" + "="*80)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="ATFT-GAT-FAN Production Workload Validation")
    parser.add_argument(
        "--data",
        type=str,
        default="output/ml_dataset_20250827_174908.parquet",
        help="Path to test data"
    )
    parser.add_argument(
        "--config",
        type=str,
        help="Path to config file"
    )
    parser.add_argument(
        "--scenario",
        type=str,
        choices=['tiny_scale', 'small_scale', 'medium_scale', 'large_scale', 'production_scale'],
        help="Specific scenario to test"
    )
    parser.add_argument(
        "--full-validation",
        action="store_true",
        help="Run full validation suite"
    )

    args = parser.parse_args()

    validator = ProductionValidator(args.data, args.config)

    if args.scenario:
        # ç‰¹å®šã®ã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ
        logger.info(f"Running validation for scenario: {args.scenario}")
        result = validator.validate_batch_processing(args.scenario)
        validator.display_results({'batch_processing': result})

    elif args.full_validation:
        # ãƒ•ãƒ«æ¤œè¨¼å®Ÿè¡Œ
        logger.info("Running full production validation...")
        results = validator.run_full_validation()
        validator.display_results(results)

    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆ
        logger.info("Running default medium-scale validation...")
        result = validator.validate_batch_processing('medium_scale')
        print("\n" + "="*60)
        print("MEDIUM SCALE VALIDATION RESULTS")
        print("="*60)
        print(f"  Average throughput: {result['summary']['throughput_mean']:.2f} samples/sec")
        print(f"  Stability score: {result['summary']['stability_score']:.3f}")
        print("="*60)


if __name__ == "__main__":
    main()
