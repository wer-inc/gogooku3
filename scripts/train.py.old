#!/usr/bin/env python3
"""
ATFT-GAT-FAN å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¨­è¨ˆæ›¸v3.2ã«åŸºã¥ãé«˜åº¦ãªå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""
import hydra
from omegaconf import DictConfig, OmegaConf
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import MLFlowLogger, WandbLogger
import torch
import logging
from pathlib import Path

from src.atft_gat_fan.data.loaders.parquet_loader import ParquetDataModule
from src.atft_gat_fan.models.architectures.atft_gat_fan import ATFT_GAT_FAN
from src.atft_gat_fan.training.phases.phase_trainer import PhaseTrainer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@hydra.main(version_base=None, config_path="../configs", config_name="config")
def train(config: DictConfig):
    """å­¦ç¿’ãƒ¡ã‚¤ãƒ³é–¢æ•°"""

    logger.info(f"ğŸš€ Starting ATFT-GAT-FAN training with config: {config.mode}")

    # è¨­å®šã®æ¤œè¨¼
    logger.info("ğŸ“‹ Configuration validation:")
    logger.info(f"  Mode: {config.mode}")
    logger.info(f"  Model: {config.model.name}")
    logger.info(f"  Data: {config.data.name}")
    logger.info(f"  Train: {config.train.name}")

    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
    if config.debug.enabled:
        logger.info("ğŸ› Debug mode enabled")
        config.trainer.fast_dev_run = config.debug.fast_dev_run

    # ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æº–å‚™
    logger.info("ğŸ“Š Preparing data module...")
    data_module = ParquetDataModule(config)

    # Phase Trainingã®å®Ÿè¡Œ
    if config.train.phase_training.enabled:
        logger.info("ğŸ¯ Phase training enabled")
        phase_trainer = PhaseTrainer(config)
        final_model = phase_trainer.train_all_phases(data_module)
    else:
        # é€šå¸¸ã®å­¦ç¿’
        logger.info("ğŸ”¥ Single phase training")
        final_model = train_single_phase(config, data_module)

    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    save_final_model(final_model, config)

    logger.info("âœ… Training completed successfully!")
    return final_model


def train_single_phase(config: DictConfig, data_module):
    """å˜ä¸€ãƒ•ã‚§ãƒ¼ã‚ºã®å­¦ç¿’"""

    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model = ATFT_GAT_FAN(config)

    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
    callbacks = setup_callbacks(config)

    # ãƒ­ã‚¬ãƒ¼è¨­å®š
    loggers = setup_loggers(config)

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®š
    trainer_config = OmegaConf.to_container(config.trainer, resolve=True)
    trainer = pl.Trainer(
        **trainer_config,
        callbacks=callbacks,
        logger=loggers,
        enable_progress_bar=True,
    )

    # å­¦ç¿’å®Ÿè¡Œ
    logger.info("ğŸš€ Starting training...")
    trainer.fit(model, data_module)

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    logger.info("ğŸ§ª Running evaluation...")
    trainer.test(model, data_module)

    return model


def setup_callbacks(config: DictConfig):
    """ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š"""
    callbacks = []

    # Model Checkpoint
    checkpoint_config = OmegaConf.to_container(config.train.checkpoint, resolve=True)
    checkpoint_callback = ModelCheckpoint(**checkpoint_config)
    callbacks.append(checkpoint_callback)

    # Early Stopping
    if config.train.early_stopping:
        early_stopping_config = OmegaConf.to_container(config.train.early_stopping, resolve=True)
        early_stopping = EarlyStopping(**early_stopping_config)
        callbacks.append(early_stopping)

    return callbacks


def setup_loggers(config: DictConfig):
    """ãƒ­ã‚¬ãƒ¼è¨­å®š"""
    loggers = []

    # MLFlow Logger
    if config.logging.mlflow:
        mlflow_config = OmegaConf.to_container(config.logging.mlflow, resolve=True)
        mlflow_logger = MLFlowLogger(**mlflow_config)
        loggers.append(mlflow_logger)

    # Wandb Logger
    if config.logging.wandb.enabled:
        wandb_config = OmegaConf.to_container(config.logging.wandb, resolve=True)
        wandb_logger = WandbLogger(**wandb_config)
        loggers.append(wandb_logger)

    return loggers


def save_final_model(model, config: DictConfig):
    """æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
    save_path = Path(config.paths.models) / f"{config.model.name}_final.ckpt"

    # PyTorch Lightningã®ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    trainer = pl.Trainer()
    trainer.save_checkpoint(save_path)

    logger.info(f"ğŸ’¾ Model saved to: {save_path}")

    # è¨­å®šã‚‚ä¿å­˜
    config_path = save_path.parent / f"{config.model.name}_config.yaml"
    with open(config_path, 'w') as f:
        OmegaConf.save(config, f)

    logger.info(f"ğŸ“‹ Config saved to: {config_path}")


@hydra.main(version_base=None, config_path="../configs", config_name="config")
def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    train()


if __name__ == "__main__":
    main()
