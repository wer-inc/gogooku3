#!/usr/bin/env python3
"""
Complete ATFT-GAT-FAN Training Pipeline for gogooku3
ATFT-GAT-FANã®æˆæœï¼ˆSharpe 0.849ï¼‰ã‚’å®Œå…¨ã«å†ç¾ã™ã‚‹çµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import asyncio
import json
import logging
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import polars as pl
import torch

# ãƒ‘ã‚¹ã‚’è¿½åŠ ï¼ˆrepo root ã¨ src ã‚’ import path ã¸ï¼‰
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))
SRC_PATH = REPO_ROOT / "src"
if str(SRC_PATH) not in sys.path:
    sys.path.insert(0, str(SRC_PATH))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("logs/ml_training.log"), logging.StreamHandler()],
)
logger = logging.getLogger(__name__)


class CompleteATFTTrainingPipeline:
    """ATFT-GAT-FANã®æˆæœã‚’å®Œå…¨ã«å†ç¾ã™ã‚‹çµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self, data_path: str | None = None, sample_size: int | None = None,
                 run_safe_pipeline: bool = False, extra_overrides: list[str] | None = None):
        self.output_dir = Path("output")
        self.logs_dir = Path("logs")
        self.logs_dir.mkdir(exist_ok=True)
        self.data_path = Path(data_path) if data_path else None
        # å°è¦æ¨¡å®Ÿè¡Œç”¨ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¡Œæ•°ï¼ˆæ¦‚ç®—ï¼‰
        self.sample_size: int | None = int(sample_size) if sample_size else None
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: SafeTrainingPipeline ã‚’äº‹å‰ã«å®Ÿè¡Œ
        self.run_safe_pipeline: bool = bool(run_safe_pipeline)
        # è¿½åŠ ã®Hydraã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼ˆtrain_atft.pyã¸å¼•ãæ¸¡ã—ï¼‰
        self.extra_overrides: list[str] = list(extra_overrides or [])
        # ç›´è¿‘ã«ä½¿ç”¨ã—ãŸMLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ï¼ˆæ¤œè¨¼ã‚„SafePipelineå®Ÿè¡Œã«åˆ©ç”¨ï¼‰
        self._last_ml_dataset_path: Path | None = None

        # ATFT-GAT-FANã®æˆæœè¨­å®š
        self.atft_settings = {
            "expected_sharpe": 0.849,
            "model_params": 5611803,
            "input_dim": 8,
            "sequence_length": 20,
            "prediction_horizons": [1, 5, 10, 20],
            "batch_size": 4096,
            "learning_rate": 2e-4,
            "max_epochs": 75,
            "precision": "16-mixed",
        }

        # å®‰å®šæ€§è¨­å®šï¼ˆATFT-GAT-FANã®æˆæœã‹ã‚‰ï¼‰
        self.stability_settings = {
            "USE_T_NLL": 1,
            "OUTPUT_NOISE_STD": 0.02,
            "HEAD_NOISE_STD": 0.05,
            "HEAD_NOISE_WARMUP_EPOCHS": 5,
            "GAT_ALPHA_INIT": 0.3,
            "GAT_ALPHA_MIN": 0.1,
            "GAT_ALPHA_PENALTY": 1e-3,
            "EDGE_DROPOUT_INPUT_P": 0.1,
            "DEGENERACY_GUARD": 1,
            "DEGENERACY_WARMUP_STEPS": 1000,
            "DEGENERACY_CHECK_EVERY": 200,
            "DEGENERACY_MIN_RATIO": 0.05,
            "USE_AMP": 1,
            "AMP_DTYPE": "bf16",
        }

    async def run_complete_training_pipeline(self) -> tuple[bool, dict]:
        """ATFT-GAT-FANã®æˆæœã‚’å®Œå…¨ã«å†ç¾ã™ã‚‹çµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        start_time = time.time()

        try:
            logger.info("ğŸš€ Complete ATFT-GAT-FAN Training Pipeline started")
            logger.info(
                f"ğŸ¯ Target Sharpe Ratio: {self.atft_settings['expected_sharpe']}"
            )

            # 1. ç’°å¢ƒè¨­å®šï¼ˆATFT-GAT-FANã®æˆæœè¨­å®šï¼‰
            success = await self._setup_atft_environment()
            if not success:
                return False, {"error": "Environment setup failed", "stage": "setup"}

            # 2. MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼
            success, data_info = await self._load_and_validate_ml_dataset()
            if not success:
                return False, {"error": "ML dataset loading failed", "stage": "load"}

            # 3. ç‰¹å¾´é‡å¤‰æ›ï¼ˆML â†’ ATFTå½¢å¼ï¼‰
            # 2.5. ï¼ˆä»»æ„ï¼‰SafeTrainingPipelineã®å®Ÿè¡Œï¼ˆãƒªãƒ¼ã‚¯é˜²æ­¢ãƒã‚§ãƒƒã‚¯ã¨åŸºæº–æ€§èƒ½ã®ç¢ºèªï¼‰
            try:
                if self.run_safe_pipeline:
                    ds_path = data_info.get("path") or self._last_ml_dataset_path
                    if ds_path and Path(ds_path).exists():
                        await self._run_safe_training_pipeline(Path(ds_path))
                    else:
                        logger.warning("Safe pipeline requested, but dataset path is unavailable. Skipping.")
            except Exception as _e:
                logger.warning(f"SafeTrainingPipeline step skipped: {_e}")

            # 3. ç‰¹å¾´é‡å¤‰æ›ï¼ˆML â†’ ATFTå½¢å¼ï¼‰
            success, conversion_info = await self._convert_ml_to_atft_format(
                data_info["df"]
            )
            if not success:
                return False, {
                    "error": "Feature conversion failed",
                    "stage": "conversion",
                }

            # 4. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆATFT-GAT-FANå½¢å¼ï¼‰
            success, training_data_info = await self._prepare_atft_training_data(
                conversion_info
            )
            if not success:
                return False, {
                    "error": "Training data preparation failed",
                    "stage": "preparation",
                }

            # 5. ATFT-GAT-FANå­¦ç¿’ã®å®Ÿè¡Œï¼ˆæˆæœå†ç¾ï¼‰
            success, training_info = await self._execute_atft_training_with_results(
                training_data_info
            )
            if not success:
                return False, {"error": "ATFT training failed", "stage": "training"}

            # 6. æˆæœæ¤œè¨¼
            success, validation_info = await self._validate_training_results(
                training_info
            )
            if not success:
                return False, {
                    "error": "Results validation failed",
                    "stage": "validation",
                }

            # 7. çµæœè¨˜éŒ²
            elapsed_time = time.time() - start_time
            result = {
                "status": "success",
                "elapsed_time": elapsed_time,
                "atft_settings": self.atft_settings,
                "stability_settings": self.stability_settings,
                "data_info": data_info,
                "conversion_info": conversion_info,
                "training_data_info": training_data_info,
                "training_info": training_info,
                "validation_info": validation_info,
                "portfolio_optimization": {},
                "timestamp": datetime.now().isoformat(),
            }
            # 8. ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–ï¼ˆæ¤œè¨¼äºˆæ¸¬ãŒã‚ã‚‹å ´åˆï¼‰
            try:
                ok_po, po_info = await self._run_portfolio_optimization()
                if ok_po:
                    result["portfolio_optimization"] = po_info
                else:
                    result["portfolio_optimization"] = {"error": po_info.get("error", "unknown")}
            except Exception as _e:
                logger.warning(f"Portfolio optimization step skipped: {_e}")

            self._save_complete_training_result(result)
            logger.info(
                f"âœ… Complete ATFT-GAT-FAN Training Pipeline completed successfully in {elapsed_time:.2f}s"
            )
            ach = validation_info.get('sharpe_ratio', None)
            if ach is not None:
                logger.info(f"ğŸ¯ Achieved Sharpe Ratio: {ach}")
            try:
                po = result.get("portfolio_optimization", {})
                rep = po.get("report", {})
                if rep and "sharpe" in rep:
                    logger.info(f"ğŸ“ˆ Portfolio Sharpe (net, cost 5bps): {rep['sharpe']:.4f}")
            except Exception:
                pass

            return True, result

        except Exception as e:
            logger.error(f"âŒ Complete training pipeline failed: {e}")
            import traceback

            traceback.print_exc()
            return False, {"error": str(e), "stage": "unknown"}

    async def _setup_atft_environment(self) -> bool:
        """ATFT-GAT-FANã®æˆæœã‚’å†ç¾ã™ã‚‹ãŸã‚ã®ç’°å¢ƒè¨­å®š"""
        try:
            logger.info("ğŸ”§ Setting up ATFT-GAT-FAN environment...")

            # å®‰å®šæ€§è¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦è¨­å®š
            for key, value in self.stability_settings.items():
                os.environ[key] = str(value)

            # ATFT-GAT-FANã®ãƒ‘ã‚¹è¨­å®šï¼ˆã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é‹ç”¨æ™‚ã¯ä»»æ„ï¼‰
            # - ATFT_EXTERNAL_PATH: æ—¢å­˜ATFTãƒªãƒã‚¸ãƒˆãƒªã®å ´æ‰€ï¼ˆæœªè¨­å®šãªã‚‰æ—¢å®šãƒ‘ã‚¹ï¼‰
            # - REQUIRE_ATFT_EXTERNAL: 1/trueã§å¿…é ˆåŒ–ï¼ˆæ—¢å®šã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            ext_path_env = os.getenv("ATFT_EXTERNAL_PATH", "/home/ubuntu/gogooku2/apps/ATFT-GAT-FAN")
            atft_path = Path(ext_path_env)
            require_ext = os.getenv("REQUIRE_ATFT_EXTERNAL", "0").lower() in ("1", "true", "yes")
            if not atft_path.exists():
                if require_ext:
                    logger.error(
                        f"ATFT-GAT-FAN external path not found: {atft_path} (set ATFT_EXTERNAL_PATH or disable by REQUIRE_ATFT_EXTERNAL=0)"
                    )
                    return False
                else:
                    logger.warning(
                        f"ATFT-GAT-FAN external path not found: {atft_path} â€” continue in standalone mode"
                    )

            # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            (self.output_dir / "atft_data").mkdir(parents=True, exist_ok=True)
            (self.output_dir / "checkpoints").mkdir(parents=True, exist_ok=True)
            (self.output_dir / "results").mkdir(parents=True, exist_ok=True)

            logger.info("âœ… ATFT-GAT-FAN environment setup completed")
            return True

        except Exception as e:
            logger.error(f"âŒ Environment setup failed: {e}")
            return False

    async def _load_and_validate_ml_dataset(self) -> tuple[bool, dict]:
        """MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼ï¼ˆATFT-GAT-FANå¯¾å¿œï¼‰"""
        try:
            logger.info("ğŸ“Š Loading and validating ML dataset...")

            # MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
            # å„ªå…ˆé †ä½: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•° > output/ml_dataset_*.parquet > data/processed/ml_dataset_latest.parquet > data/ml_dataset.parquet
            ml_dataset_paths = []

            # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°æœ€å„ªå…ˆ
            if self.data_path and self.data_path.exists():
                ml_dataset_paths.append(self.data_path)

            # outputå†…ã®æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¢ã™
            output_datasets = sorted(Path("output").glob("ml_dataset_*.parquet"), reverse=True)
            ml_dataset_paths.extend(output_datasets[:3])  # æœ€æ–°3ã¤ã¾ã§

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹
            ml_dataset_paths.extend([
                Path("output/ml_dataset_production.parquet"),
                Path("data/processed/ml_dataset_latest.parquet"),
                Path("data/ml_dataset.parquet")
            ])

            ml_dataset_path = None
            for path in ml_dataset_paths:
                if path.exists():
                    ml_dataset_path = path
                    break

            if ml_dataset_path is None:
                # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                logger.warning("ML dataset not found, creating sample data for testing")
                df = self._create_sample_ml_dataset()
            else:
                logger.info(f"ğŸ“‚ Loading ML dataset from: {ml_dataset_path}")
                df = pl.read_parquet(ml_dataset_path)
                self._last_ml_dataset_path = ml_dataset_path

            # è¿…é€Ÿæ¤œè¨¼ãƒ¢ãƒ¼ãƒ‰: --sample-size ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯å¤‰æ›å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚’ç¸®å°
            if self.sample_size is not None and self.sample_size > 0:
                try:
                    # éŠ˜æŸ„å˜ä½ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ç´¯ç©è¡Œæ•°ãŒ sample_size ã‚’è¶…ãˆã‚‹ã¾ã§æ¡ç”¨
                    # ã“ã‚Œã«ã‚ˆã‚Šæ™‚ç³»åˆ—ã®é€£ç¶šæ€§ã‚’ä¿ã¡ã¤ã¤ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚‚å‰Šæ¸›ã§ãã‚‹
                    min_seq = int(self.atft_settings.get("sequence_length", 20))
                    gb = (
                        df.group_by("Code")
                        .agg(pl.len().alias("n"))
                        .filter(pl.col("n") >= min_seq)  # å­¦ç¿’ã«å¿…è¦ãªç³»åˆ—é•·ã‚’æº€ãŸã™éŠ˜æŸ„ã®ã¿
                        .sort("n")  # éå‰°ã‚µãƒ³ãƒ—ãƒ«ã‚’é¿ã‘ã‚‹ãŸã‚è¡Œæ•°ã®å°‘ãªã„éŠ˜æŸ„ã‹ã‚‰æ¡ç”¨
                    )
                    codes = gb.select(["Code", "n"]).to_dict(as_series=False)
                    sel_codes = []
                    cum = 0
                    for code, n in zip(codes["Code"], codes["n"], strict=False):
                        if cum >= self.sample_size:
                            break
                        sel_codes.append(code)
                        cum += int(n)
                    if sel_codes:
                        df = df.filter(pl.col("Code").is_in(sel_codes))
                        logger.info(
                            f"ğŸ” Sample mode: selected {len(sel_codes)} codes for ~{self.sample_size} rows (actual={len(df)})"
                        )
                    else:
                        logger.warning("Sample mode requested but could not determine codes; falling back to head() sampling")
                        df = df.head(self.sample_size)
                except Exception as e:
                    logger.warning(f"Sample mode failed ({e}); falling back to head() sampling")
                    try:
                        df = df.head(self.sample_size)
                    except Exception:
                        pass

            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            validation_result = self._validate_ml_dataset(df)
            if not validation_result["valid"]:
                error_details = []
                if validation_result['missing_columns']:
                    error_details.append(f"Missing columns: {validation_result['missing_columns']}")
                if not validation_result.get('has_return_column', False):
                    error_details.append("No return/target column found (needs one of: returns_1d, feat_ret_1d, target, returns)")
                if validation_result['total_columns'] < 50:
                    error_details.append(f"Not enough features: {validation_result['total_columns']} < 50")
                if validation_result['total_rows'] == 0:
                    error_details.append("Dataset is empty")

                error_msg = "; ".join(error_details) if error_details else "Unknown validation error"
                logger.error(f"Dataset validation failed: {error_msg}")
                logger.info(f"Dataset info - Rows: {validation_result['total_rows']}, Cols: {validation_result['total_columns']}")
                logger.info(f"Sample columns: {validation_result.get('column_sample', [])}")
                return False, {"error": error_msg}

            data_info = {
                "df": df,
                "shape": df.shape,
                "columns": df.columns,
                "validation": validation_result,
                "path": self._last_ml_dataset_path,
            }

            logger.info(f"âœ… ML dataset loaded: {df.shape}")
            return True, data_info

        except Exception as e:
            logger.error(f"âŒ ML dataset loading failed: {e}")
            return False, {"error": str(e)}

    async def _run_safe_training_pipeline(self, dataset_path: Path) -> None:
        """ä»»æ„ã‚¹ãƒ†ãƒƒãƒ—: SafeTrainingPipeline ã‚’å®Ÿè¡Œã—ã¦å“è³ªæ¤œè¨¼ãƒ»åŸºæº–æ€§èƒ½ã‚’å–å¾—"""
        try:
            from gogooku3.training.safe_training_pipeline import SafeTrainingPipeline
        except Exception:
            # äº’æ›ãƒ‘ã‚¹ï¼ˆç§»è¡Œä¸­ã®ç’°å¢ƒï¼‰
            from src.gogooku3.training.safe_training_pipeline import (
                SafeTrainingPipeline,  # type: ignore
            )

        logger.info("ğŸ›¡ï¸ Running SafeTrainingPipeline (n_splits=2, embargo=20d)...")
        out_dir = Path("output/safe_training")
        pipe = SafeTrainingPipeline(
            data_path=dataset_path,
            output_dir=out_dir,
            experiment_name="integrated_safe",
            verbose=False,
        )
        res = pipe.run_pipeline(n_splits=2, embargo_days=20, memory_limit_gb=8.0, save_results=True)
        # ä»£è¡¨çš„ãªæŒ‡æ¨™ã‚’ãƒ­ã‚°
        try:
            rep = (res or {}).get("final_report", {})
            baseline = (res or {}).get("step5_baseline", {})
            logger.info(f"Safe pipeline report: keys={list(rep.keys())[:5]}")
            if baseline:
                logger.info("Baseline metrics (subset): " + ", ".join(f"{k}={v}" for k, v in list(baseline.items())[:5]))
        except Exception:
            pass

    async def _convert_ml_to_atft_format(self, df: pl.DataFrame) -> tuple[bool, dict]:
        """MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ATFT-GAT-FANå½¢å¼ã«å¤‰æ›"""
        try:
            logger.info("ğŸ”„ Converting ML dataset to ATFT-GAT-FAN format...")

            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œã¯åˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸æ›¸ãå‡ºã—ã¦æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã‚’ä¿è­·ï¼‰
            out_dir = (
                f"output/atft_data_sample_{self.sample_size}"
                if (self.sample_size is not None and self.sample_size > 0)
                else "output/atft_data"
            )

            # Try to import UnifiedFeatureConverter
            try:
                from scripts.models.unified_feature_converter import (
                    UnifiedFeatureConverter,
                )
                converter = UnifiedFeatureConverter()
                # æ—¢å­˜ã®å¤‰æ›çµæœãŒã‚ã‚Šã€å†åˆ©ç”¨å¯èƒ½ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
                try:
                    from pathlib import Path as _P

                    _train_dir = _P(out_dir) / "train"
                    force_reconvert = os.getenv("FORCE_CONVERT", "0") == "1"
                    if (not force_reconvert) and _train_dir.exists() and any(_train_dir.glob("*.parquet")):
                        logger.info(
                            f"â™»ï¸  Reusing existing converted data at {out_dir} (skip conversion)"
                        )
                        file_paths = {
                            "train_files": sorted(str(p) for p in _train_dir.glob("*.parquet")),
                            "val_files": sorted(
                                str(p) for p in (_P(out_dir) / "val").glob("*.parquet")
                            ),
                            "test_files": sorted(
                                str(p) for p in (_P(out_dir) / "test").glob("*.parquet")
                            ),
                            "metadata": str(_P(out_dir) / "metadata.json"),
                        }
                    else:
                        file_paths = converter.convert_to_atft_format(df, out_dir)
                except Exception:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¸¸ã«å¤‰æ›
                    file_paths = converter.convert_to_atft_format(df, out_dir)
            except ImportError:
                logger.warning("UnifiedFeatureConverter not found, using direct training approach")
                # Create mock file paths for compatibility
                file_paths = {
                    "train_files": ["direct_training"],
                    "val_files": [],
                    "test_files": [],
                    "metadata": {"direct_mode": True}
                }

            conversion_info = {
                "file_paths": file_paths,
                "converter": "Direct" if "direct_training" in str(file_paths) else "UnifiedFeatureConverter",
                "output_dir": out_dir,
                "dataframe": df  # Keep dataframe for direct training
            }

            logger.info(f"âœ… Conversion completed: Mode = {conversion_info['converter']}")
            return True, conversion_info

        except Exception as e:
            logger.error(f"âŒ Conversion failed: {e}")
            return False, {"error": str(e)}

    async def _prepare_atft_training_data(
        self, conversion_info: dict
    ) -> tuple[bool, dict]:
        """ATFT-GAT-FANå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
        try:
            logger.info("ğŸ“‹ Preparing ATFT-GAT-FAN training data...")

            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æƒ…å ±ã®ç¢ºèª
            file_paths = conversion_info.get("file_paths", {})
            train_files = file_paths.get("train_files", [])
            val_files = file_paths.get("val_files", [])
            test_files = file_paths.get("test_files", [])

            if not train_files:
                return False, {"error": "No training files found"}

            training_data_info = {
                "train_files": train_files,
                "val_files": val_files,
                "test_files": test_files,
                "data_dir": conversion_info.get("output_dir", "output/atft_data"),
                "sequence_length": self.atft_settings["sequence_length"],
                "input_dim": self.atft_settings["input_dim"],
                "metadata": file_paths.get("metadata"),
            }

            logger.info(
                f"âœ… ATFT-GAT-FAN training data prepared: {len(train_files)} train files"
            )
            return True, training_data_info

        except Exception as e:
            logger.error(f"âŒ Training data preparation failed: {e}")
            return False, {"error": str(e)}

    async def _execute_atft_training_with_results(
        self, training_data_info: dict
    ) -> tuple[bool, dict]:
        """ATFT-GAT-FANå­¦ç¿’ã®å®Ÿè¡Œï¼ˆæˆæœå†ç¾ï¼‰"""
        try:
            logger.info(
                "ğŸ‹ï¸ Executing ATFT-GAT-FAN training with results reproduction..."
            )

            # max_epochs=0 ã®å ´åˆã¯å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé…ç·šæ¤œè¨¼ãªã©ã®ç”¨é€”ï¼‰
            if int(self.atft_settings.get("max_epochs", 0)) <= 0:
                msg = "Training skipped because max_epochs=0"
                logger.info(msg)
                training_info = {
                    "command": [],
                    "return_code": 0,
                    "log": msg,
                    "metrics": {},
                    "lines": 1,
                    "skipped": True,
                }
                return True, training_info

            # å†…è£½ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½¿ç”¨ï¼ˆHydraè¨­å®šã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼‰
            # Hydraæ§‹æˆã¯defaultsã§å®‰å®šåŒ–æ¸ˆã¿ã€‚
            # ã“ã“ã§ã¯å¿…è¦ãªå­¦ç¿’ãƒã‚¤ãƒ‘ãƒ©ã ã‘é€šå¸¸ã®ä¸Šæ›¸ãã§æ¸¡ã™ï¼ˆstructå®‰å…¨ï¼‰ã€‚
            cmd = [
                "python",
                "scripts/train_atft.py",
                # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
                f"data.source.data_dir={training_data_info['data_dir']}",
                # data/model/train ã¯ configs/config.yaml ã® defaults ã§å›ºå®š
                # å­¦ç¿’ãƒã‚¤ãƒ‘ãƒ©ã®ã¿èª¿æ•´ï¼ˆæ—¢å­˜ã‚­ãƒ¼ãªã®ã§ + ã¯ä¸è¦ï¼‰
                f"train.batch.train_batch_size={self.atft_settings['batch_size']}",
                f"train.optimizer.lr={self.atft_settings['learning_rate']}",
                f"train.trainer.max_epochs={self.atft_settings['max_epochs']}",
                f"train.trainer.precision={self.atft_settings['precision']}",
                # é€²æ—ã¨æ¤œè¨¼é »åº¦ã¯æ˜ç¤º
                "train.trainer.check_val_every_n_epoch=1",
                "train.trainer.enable_progress_bar=true",
            ]

            # è¿½åŠ ã®Hydraã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼ˆHPOã‚„è©³ç´°è¨­å®šã‚’ãƒ‘ã‚¹ã‚¹ãƒ«ãƒ¼ï¼‰
            if self.extra_overrides:
                cmd.extend(self.extra_overrides)

            # ã¾ãšGPU/CPUè¨ˆç”»ã‚’åˆ¤å®šï¼ˆã“ã®çµæœã‚’ä»¥é™ã®è¨­å®šã«åˆ©ç”¨ï¼‰
            force_gpu = os.getenv("FORCE_GPU", "0") == "1"
            acc_env = os.getenv("ACCELERATOR", "").lower()
            use_gpu_plan = (torch.cuda.is_available() or force_gpu) and acc_env != "cpu"

            # å°è¦æ¨¡ã‚µãƒ³ãƒ—ãƒ«ã§ã®å®Ÿè¡Œæ™‚ã¯ã€minibatchå´©å£Šã¨æ¤œè¨¼0ä»¶ã‚’é¿ã‘ã‚‹ãŸã‚ã®ä¿è­·ã‚’å…¥ã‚Œã‚‹
            debug_small_data = (
                self.sample_size is not None and int(self.sample_size) > 0
            )
            if debug_small_data:
                # çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§valå´ã®ã‚µãƒ³ãƒ—ãƒ«ä¸è¶³ã‚’å›é¿ï¼ˆæœ¬ç•ªã¯20ã§OKï¼‰
                debug_seq_len = 10
                cmd.append(f"data.time_series.sequence_length={debug_seq_len}")
                logger.info(
                    f"[debug-small] Override sequence_length -> {debug_seq_len} to ensure non-empty validation"
                )
                if not use_gpu_plan:
                    # CPUè¨ˆç”»æ™‚ã®ã¿ DataLoader ã‚’å˜ç´”åŒ–
                    cmd.extend(
                        [
                            "train.batch.num_workers=0",
                            "train.batch.prefetch_factor=null",
                            "train.batch.persistent_workers=false",
                            "train.batch.pin_memory=false",
                        ]
                    )

            # å­¦ç¿’å®Ÿè¡Œï¼ˆãƒªãƒã‚¸ãƒˆãƒªç›´ä¸‹ã§å®Ÿè¡Œï¼‰
            # Ensure train script sees data directory via config override
            env = os.environ.copy()
            # æ’ä¹…é‹ç”¨ã§ã¯Validatorã‚’æœ‰åŠ¹åŒ–
            env.pop("VALIDATE_CONFIG", None)
            env["HYDRA_FULL_ERROR"] = "1"  # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å–å¾—
            # GPU/CPU ã®å®Ÿè¡Œæ–¹é‡ã«å¿œã˜ã¦ DataLoader é–¢é€£ã‚’èª¿æ•´ï¼ˆä¸Šã®åˆ¤å®šã‚’å†åˆ©ç”¨ï¼‰

            if use_gpu_plan:
                # GPU å®Ÿè¡Œ: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€æœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–ï¼ˆä¸Šæ›¸ãï¼‰
                # æ—¢å­˜ã®CPUå‘ã‘ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ãŒæ··å…¥ã—ã¦ã„ã‚Œã°å–ã‚Šé™¤ã
                for bad in [
                    "train.batch.prefetch_factor=null",
                    "train.batch.persistent_workers=false",
                    "train.batch.pin_memory=false",
                ]:
                    if bad in cmd:
                        cmd.remove(bad)
                # æ¨å¥¨GPUè¨­å®šã‚’ä»˜ä¸ï¼ˆæœªæŒ‡å®šã®å ´åˆï¼‰
                if not any(s.startswith("train.batch.persistent_workers=") for s in cmd):
                    cmd.append("train.batch.persistent_workers=true")
                if not any(s.startswith("train.batch.pin_memory=") for s in cmd):
                    cmd.append("train.batch.pin_memory=true")
                if not any(s.startswith("train.batch.prefetch_factor=") for s in cmd):
                    cmd.append("train.batch.prefetch_factor=8")
                env.setdefault("ACCELERATOR", "gpu")
                env.setdefault("CUDA_VISIBLE_DEVICES", os.getenv("CUDA_VISIBLE_DEVICES", "0"))
                # GPUãƒ­ã‚°ã‚’æ˜ç¤º
                logger.info("[pipeline] Using GPU execution plan (persistent_workers, pin_memory, prefetch_factor=8)")
            else:
                # CPU å®Ÿè¡Œ: DataLoader ã‚’å˜ç´”åŒ–
                if "train.batch.prefetch_factor=null" not in cmd:
                    cmd.append("train.batch.prefetch_factor=null")
                if "train.batch.persistent_workers=false" not in cmd:
                    cmd.append("train.batch.persistent_workers=false")
                if "train.batch.pin_memory=false" not in cmd:
                    cmd.append("train.batch.pin_memory=false")
            logger.info(f"Running command: {' '.join(cmd)}")
            if not use_gpu_plan:
                if env.get("USE_DAY_BATCH", "1") not in ("0", "false", "False"):
                    logger.info("[pipeline] Forcing USE_DAY_BATCH=0 for CPU execution")
                env["USE_DAY_BATCH"] = "0"
                env.setdefault("NUM_WORKERS", "0")
                env.setdefault("PERSISTENT_WORKERS", "0")
                env.setdefault("PREFETCH_FACTOR", "0")
            if debug_small_data and not use_gpu_plan:
                # åˆ†å‰²ã¨gapã®ä¿å®ˆè¨­å®šï¼ˆæ¤œè¨¼æœŸé–“ã‚’ååˆ†ç¢ºä¿ï¼‰
                env.setdefault("TRAIN_RATIO", "0.6")
                # VAL_RATIOã¯éç´¯ç©ï¼ˆtrainã¨ã¯ç‹¬ç«‹æ¯”ç‡ï¼‰ã§æ‰±ã‚ã‚Œã‚‹
                env.setdefault("VAL_RATIO", "0.3")
                env.setdefault("GAP_DAYS", "1")
                # DayBatchSamplerã¯å°è¦æ¨¡ã ã¨1ãƒãƒƒãƒåŒ–ã—ã‚„ã™ã„ã®ã§ç„¡åŠ¹åŒ–
                env.setdefault("USE_DAY_BATCH", "0")
                env.setdefault("MIN_NODES_PER_DAY", "4")
                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é–“å¼•ãï¼ˆè¨ˆç®—é‡ã‚’æŠ‘åˆ¶ï¼‰
                env.setdefault("DATASET_STRIDE", "2")
                # DataLoaderã‚’å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ã«å›ºå®š
                env.setdefault("NUM_WORKERS", "0")
                env.setdefault("PERSISTENT_WORKERS", "0")
                env.setdefault("PIN_MEMORY", "0")
                env.setdefault("DL_SEED", "42")
                # å°è¦æ¨¡æ™‚ã¯Sharpeè¨ˆç®—ã®å®‰å®šåŒ–ï¼ˆåˆ†æ¯ã«å¾®å°Îµï¼‰
                env.setdefault("SHARPE_EPS", "1e-8")
                logger.info(
                    "[debug-small] Applied env overrides: "
                    "TRAIN_RATIO=0.6 VAL_RATIO=0.3 GAP_DAYS=1 "
                    "USE_DAY_BATCH=0 MIN_NODES_PER_DAY=4 DATASET_STRIDE=2 "
                    "NUM_WORKERS=0 SHARPE_EPS=1e-8"
                )
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=env,
            )

            if result.returncode != 0:
                logger.error(f"Training failed: {result.stderr}")
                return False, {"error": result.stderr}

            # å­¦ç¿’çµæœã®è§£æï¼ˆstdout + stderr ã‚’åˆã‚ã›ã¦è§£æï¼‰
            combined_output = "\n".join([
                result.stdout or "",
                result.stderr or "",
            ])
            training_info = self._parse_training_output(combined_output)
            training_info["command"] = cmd
            training_info["return_code"] = result.returncode

            # HPOäº’æ›: hpo.output_metrics_json=... ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ã€æ—¢å­˜ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é›†ç´„ã—ã¦å‡ºåŠ›
            try:
                out_json = None
                for ov in self.extra_overrides:
                    if ov.startswith("hpo.output_metrics_json="):
                        out_json = ov.split("=", 1)[1]
                        break
                if out_json:
                    metrics_payload = {"rank_ic": {}, "sharpe": {}}
                    # æ—¢å®šä¿å­˜å ´æ‰€ã‹ã‚‰èª­ã¿å–ã‚Šï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰
                    for pth in [
                        Path("runs/last/metrics_summary.json"),
                        Path("runs/last/latest_metrics.json"),
                    ]:
                        if pth.exists():
                            try:
                                payload = json.loads(pth.read_text())
                                # ä»£è¡¨ã‚­ãƒ¼ã‚’å¯èƒ½ãªç¯„å›²ã§ãƒãƒƒãƒ”ãƒ³ã‚°
                                if "rank_ic" in payload:
                                    metrics_payload["rank_ic"].update(payload["rank_ic"])  # type: ignore
                                if "sharpe" in payload:
                                    metrics_payload["sharpe"].update(payload["sharpe"])  # type: ignore
                            except Exception:
                                pass
                    # ãƒ­ã‚°ã‹ã‚‰Sharpeã‚’æŠ½å‡ºï¼ˆå˜ä¸€å€¤ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    if not metrics_payload["sharpe"]:
                        sr = self._extract_sharpe_ratio(training_info.get("log", ""))
                        if sr is not None:
                            metrics_payload["sharpe"] = {"avg": sr}
                    Path(out_json).parent.mkdir(parents=True, exist_ok=True)
                    with open(out_json, "w", encoding="utf-8") as f:
                        json.dump(metrics_payload, f, indent=2)
                    logger.info(f"HPO metrics emitted: {out_json}")
            except Exception as _e:
                logger.warning(f"Failed to emit HPO metrics JSON: {_e}")

            logger.info("âœ… ATFT-GAT-FAN training completed successfully")
            return True, training_info

        except Exception as e:
            logger.error(f"âŒ ATFT training execution failed: {e}")
            return False, {"error": str(e)}

    async def _validate_training_results(
        self, training_info: dict
    ) -> tuple[bool, dict]:
        """å­¦ç¿’çµæœã®æ¤œè¨¼ï¼ˆSharpe 0.849ã®å†ç¾ç¢ºèªï¼‰"""
        try:
            logger.info("ğŸ” Validating training results...")

            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ç¢ºèª
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒªãƒã‚¸ãƒˆãƒªé…ä¸‹ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å‚ç…§
            checkpoint_path = Path("models/checkpoints")
            checkpoints = list(checkpoint_path.glob("*.pt"))

            if not checkpoints:
                # å­¦ç¿’ã‚¹ã‚­ãƒƒãƒ—æ™‚ã‚„å¤±æ•—æ™‚ã¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹JSONã®ã¿ã§æ¤œè¨¼
                sharpe = None
                try:
                    ms_path = Path("runs/last/metrics_summary.json")
                    if ms_path.exists():
                        sharpe = json.loads(ms_path.read_text()).get("avg_sharpe")
                except Exception:
                    sharpe = None
                if sharpe is None:
                    try:
                        lm_path = Path("runs/last/latest_metrics.json")
                        if lm_path.exists():
                            sharpe = json.loads(lm_path.read_text()).get("avg_sharpe")
                    except Exception:
                        sharpe = None
                validation_info = {
                    "checkpoint_path": None,
                    "param_count": 0,
                    "expected_params": self.atft_settings["model_params"],
                    "param_match": False,
                    "training_log": training_info.get("log", ""),
                    "sharpe_ratio": sharpe,
                    "target_sharpe": self.atft_settings["expected_sharpe"],
                    "checkpoint_size_mb": 0.0,
                    "note": "No checkpoints found; using metrics only",
                }
                logger.info("âœ… Validation completed: 0 parameters (no checkpoint)")
                return True, validation_info

            latest_checkpoint = max(checkpoints, key=lambda p: p.stat().st_mtime)

            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç¢ºèªï¼ˆstate_dict/ãƒ¢ãƒ‡ãƒ«åŒæ–¹ã«é ‘å¥ï¼‰
            def _count_params(ckpt_path: Path) -> int:
                try:
                    obj = torch.load(ckpt_path, map_location="cpu")
                except Exception:
                    return int(ckpt_path.stat().st_size // 4)  # ç²—ã„æ¦‚ç®—
                try:
                    if isinstance(obj, dict):
                        # Lightningãªã©ã®å½¢å¼: state_dictã‚­ãƒ¼ã‚„ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®å€™è£œã‚’æ¢ã™
                        for k in ("state_dict", "model_state_dict", "model", "weights"):
                            if k in obj and isinstance(obj[k], dict):
                                sd = obj[k]
                                return sum(int(p.numel()) for p in sd.values() if isinstance(p, torch.Tensor))
                        # ç›´æ¥ state_dict ã®å ´åˆ
                        if all(isinstance(v, torch.Tensor) for v in obj.values()):
                            return sum(int(p.numel()) for p in obj.values())
                    # ãã‚Œä»¥å¤–ï¼ˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¿å­˜ãªã©ï¼‰ã¯ã‚µã‚¤ã‚ºã‹ã‚‰æ¦‚ç®—
                    return int(ckpt_path.stat().st_size // 4)
                except Exception:
                    return int(ckpt_path.stat().st_size // 4)

            param_count = _count_params(latest_checkpoint)

            # æˆæœã®æ¤œè¨¼
            # Prefer metrics_summary.json, then latest_metrics.json, else parse logs
            sharpe = None
            try:
                ms_path = Path("runs/last/metrics_summary.json")
                if ms_path.exists():
                    with open(ms_path) as mf:
                        jm = json.load(mf)
                        if isinstance(jm, dict):
                            sharpe = jm.get("avg_sharpe")
            except Exception:
                sharpe = None
            if sharpe is None:
                try:
                    metrics_path = Path("runs/last/latest_metrics.json")
                    if metrics_path.exists():
                        with open(metrics_path) as mf:
                            jm = json.load(mf)
                            if isinstance(jm, dict):
                                sharpe = jm.get("avg_sharpe")
                except Exception:
                    sharpe = None
            if sharpe is None:
                # Fallback to training logs
                sharpe = self._extract_sharpe_ratio(training_info.get("log", ""))
                if sharpe is None:
                    try:
                        log_path = Path("logs/ml_training.log")
                        if log_path.exists():
                            tail = log_path.read_text(errors="ignore")
                            sharpe = self._extract_sharpe_ratio(tail)
                    except Exception:
                        sharpe = None

            validation_info = {
                "checkpoint_path": str(latest_checkpoint),
                "param_count": param_count,
                "expected_params": self.atft_settings["model_params"],
                "param_match": abs(param_count - self.atft_settings["model_params"])
                < 1000000,  # è¨±å®¹èª¤å·®
                "training_log": training_info.get("log", ""),
                "sharpe_ratio": sharpe,
                "target_sharpe": self.atft_settings["expected_sharpe"],
                "checkpoint_size_mb": latest_checkpoint.stat().st_size / (1024 * 1024),
            }

            logger.info(f"âœ… Validation completed: {param_count} parameters")
            return True, validation_info

        except Exception as e:
            logger.error(f"âŒ Results validation failed: {e}")
            return False, {"error": str(e)}

    async def _run_portfolio_optimization(self) -> tuple[bool, dict]:
        """æ¤œè¨¼ç”¨äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨ã„ã¦ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–ã‚’å®Ÿè¡Œ"""
        try:
            pred_path = Path("runs/last/predictions_val.parquet")
            if not pred_path.exists():
                logger.warning(f"Predictions file not found: {pred_path}")
                return False, {"error": "predictions_val.parquet not found"}

            cmd = [
                "python",
                "scripts/advanced_portfolio_optimization.py",
                "--input",
                str(pred_path),
                "--pred-col",
                "predicted_return",
                "--ret-col",
                "actual_return",
                "--mode",
                "ls",
                "--long-frac",
                "0.2",
                "--short-frac",
                "0.2",
                "--invert-sign",
                "--cost-bps",
                "5",
            ]
            logger.info(f"Running portfolio optimization: {' '.join(cmd)}")
            res = subprocess.run(cmd, capture_output=True, text=True)
            if res.returncode != 0:
                logger.error(f"Portfolio optimization failed: {res.stderr}")
                return False, {"error": res.stderr}

            # æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿å–ã‚‹
            out_dir = Path("output/portfolio")
            report = {}
            try:
                if out_dir.exists():
                    latest = max(out_dir.glob("report_*.json"), key=lambda p: p.stat().st_mtime)
                    report = json.loads(latest.read_text())
                    logger.info(f"Portfolio report loaded: {latest}")
                else:
                    logger.warning("Portfolio output directory not found")
            except Exception as _e:
                logger.warning(f"Failed to load portfolio report: {_e}")
            return True, {"stdout": res.stdout, "report": report}
        except Exception as e:
            logger.error(f"âŒ Portfolio optimization failed: {e}")
            return False, {"error": str(e)}

    def _create_sample_ml_dataset(self) -> pl.DataFrame:
        """ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
        # å®Ÿéš›ã®ML_DATASET_COLUMNS.mdã®ä»•æ§˜ã«åŸºã¥ã„ã¦ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        n_stocks = 10
        n_days = 100

        data = []
        for stock_id in range(n_stocks):
            for day in range(n_days):
                row = {
                    "Code": f"STOCK_{stock_id:04d}",
                    "Date": f"2024-01-{day+1:02d}",
                    "Open": 1000 + np.random.randn() * 50,
                    "High": 1020 + np.random.randn() * 30,
                    "Low": 980 + np.random.randn() * 30,
                    "Close": 1000 + np.random.randn() * 50,
                    "Volume": np.random.randint(1000, 10000),
                    "returns_1d": np.random.randn() * 0.02,
                    "returns_5d": np.random.randn() * 0.05,
                    "returns_10d": np.random.randn() * 0.08,
                    "returns_20d": np.random.randn() * 0.12,
                    "ema_5": 1000 + np.random.randn() * 20,
                    "ema_10": 1000 + np.random.randn() * 25,
                    "ema_20": 1000 + np.random.randn() * 30,
                    "ema_60": 1000 + np.random.randn() * 35,
                    "ema_200": 1000 + np.random.randn() * 40,
                    "rsi_14": np.random.uniform(30, 70),
                    "rsi_2": np.random.uniform(20, 80),
                    "macd_signal": np.random.randn() * 10,
                    "macd_histogram": np.random.randn() * 5,
                    "bb_pct_b": np.random.uniform(0, 1),
                    "bb_bandwidth": np.random.uniform(0.1, 0.5),
                    "volatility_20d": np.random.uniform(0.1, 0.3),
                    "sharpe_1d": np.random.randn() * 0.5,
                }
                data.append(row)

        return pl.DataFrame(data)

    def _validate_ml_dataset(self, df: pl.DataFrame) -> dict:
        """MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¤œè¨¼"""
        # æœ€å°é™å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿ãƒã‚§ãƒƒã‚¯
        essential_columns = [
            "Code",
            "Date",
            "Open",
            "High",
            "Low",
            "Close",
            "Volume"
        ]

        # ãƒªã‚¿ãƒ¼ãƒ³ç³»ã®ã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã©ã‚Œã‹1ã¤ã‚ã‚Œã°OKï¼‰
        return_columns = ["returns_1d", "feat_ret_1d", "target", "returns"]
        has_return = any(col in df.columns for col in return_columns)

        missing_essential = set(essential_columns) - set(df.columns)

        # ååˆ†ãªç‰¹å¾´é‡ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½50ã‚«ãƒ©ãƒ ä»¥ä¸Šï¼‰
        has_enough_features = len(df.columns) >= 50

        # æ¤œè¨¼çµæœ
        is_valid = (len(missing_essential) == 0 and
                   has_return and
                   has_enough_features and
                   len(df) > 0)

        return {
            "valid": is_valid,
            "missing_columns": list(missing_essential),
            "has_return_column": has_return,
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "column_sample": df.columns[:10] if len(df.columns) > 10 else df.columns,
        }

    def _parse_training_output(self, output: str) -> dict:
        """å­¦ç¿’å‡ºåŠ›ã®è§£æ"""
        lines = output.split("\n")

        # é‡è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
        metrics = {}
        for line in lines:
            if "Sharpe" in line:
                metrics["sharpe"] = line
            elif "Loss" in line:
                metrics["loss"] = line
            elif "Epoch" in line:
                metrics["epoch"] = line

        return {"log": output, "metrics": metrics, "lines": len(lines)}

    def _extract_sharpe_ratio(self, log: str) -> float | None:
        """ãƒ­ã‚°ã‹ã‚‰Sharpeæ¯”ç‡ã‚’æŠ½å‡º"""
        import re

        # è² å€¤ã‚‚ãƒãƒƒãƒï¼ˆä¾‹: "Sharpe: -0.0123"ï¼‰
        sharpe_pattern = r"Sharpe[:\s]*(-?[0-9]*\.?[0-9]+)"
        match = re.search(sharpe_pattern, log)

        if match:
            return float(match.group(1))
        return None

    def _save_complete_training_result(self, result: dict):
        """å®Œå…¨ãªå­¦ç¿’çµæœã®ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = (
            self.output_dir / "results" / f"complete_training_result_{timestamp}.json"
        )

        with open(result_file, "w") as f:
            json.dump(result, f, indent=2, default=str)

        logger.info(f"ğŸ’¾ Complete training result saved: {result_file}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description="Complete ATFT-GAT-FAN Training Pipeline", add_help=True)
    parser.add_argument("--data-path", type=str, help="Path to ML dataset parquet file")
    parser.add_argument("--max-epochs", type=int, default=None, help="Maximum epochs (0 to skip training)")
    parser.add_argument("--batch-size", type=int, default=None, help="Batch size")
    parser.add_argument("--lr", type=float, default=None, help="Learning rate override (e.g., 2e-4)")
    parser.add_argument("--sample-size", type=int, help="Sample size for testing")
    parser.add_argument("--dry-run", action="store_true", help="Show planned steps and exit")
    parser.add_argument(
        "--adv-graph-train",
        action="store_true",
        help="Enable advanced FinancialGraphBuilder during training (EWM+shrinkage)",
    )
    parser.add_argument(
        "--run-safe-pipeline",
        action="store_true",
        help="Run SafeTrainingPipeline prior to training for leakage checks",
    )
    # æ—¢çŸ¥ä»¥å¤–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯Hydraã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã¨ã—ã¦ãã®ã¾ã¾train_atft.pyã«æ¸¡ã™
    args, unknown = parser.parse_known_args()

    if args.dry_run:
        print("=" * 60)
        print("[DRY-RUN] Complete ATFT-GAT-FAN Training Pipeline")
        print("Steps:")
        print(" 1) Setup ATFT-GAT-FAN environment")
        print(" 2) Load and validate ML dataset")
        print(" 3) Convert ML features to ATFT format")
        print(" 4) Prepare ATFT training data")
        print(" 5) Execute ATFT training (epochs configurable)")
        print(" 6) Validate results against target metrics")
        print(" 7) Save results and optional portfolio optimization")
        print("=" * 60)
        return True, {"dry_run": True}

    pipeline = CompleteATFTTrainingPipeline(
        data_path=args.data_path,
        sample_size=args.sample_size,
        run_safe_pipeline=bool(args.run_safe_pipeline),
        extra_overrides=unknown,
    )

    # å¼•æ•°ã§è¨­å®šã‚’ä¸Šæ›¸ãï¼ˆ0ã‚‚æœ‰åŠ¹å€¤ã¨ã—ã¦æ‰±ã†ï¼‰
    if args.max_epochs is not None:
        pipeline.atft_settings["max_epochs"] = int(args.max_epochs)
    if args.batch_size is not None:
        pipeline.atft_settings["batch_size"] = int(args.batch_size)
    if args.lr is not None:
        pipeline.atft_settings["learning_rate"] = float(args.lr)

    print("=" * 60)
    print("Complete ATFT-GAT-FAN Training Pipeline")
    print("Target Sharpe Ratio: 0.849")
    print("=" * 60)

    # Optionally enable advanced graph builder for training
    if args.adv_graph_train:
        os.environ["USE_ADV_GRAPH_TRAIN"] = "1"
        # Provide sensible defaults if not set (recommended)
        os.environ.setdefault("GRAPH_CORR_METHOD", "ewm_demean")
        os.environ.setdefault("EWM_HALFLIFE", "30")
        os.environ.setdefault("SHRINKAGE_GAMMA", "0.1")
        os.environ.setdefault("GRAPH_K", "15")
        os.environ.setdefault("GRAPH_EDGE_THR", "0.25")
        os.environ.setdefault("GRAPH_SYMMETRIC", "1")

    success, result = await pipeline.run_complete_training_pipeline()

    if success:
        print("ğŸ‰ Complete training pipeline succeeded!")
        sr = result.get('validation_info', {}).get('sharpe_ratio', None)
        if sr is not None:
            print(f"ğŸ“Š Results: {sr}")
    else:
        print(
            f"âŒ Complete training pipeline failed: {result.get('error', 'Unknown error')}"
        )

    return success, result


if __name__ == "__main__":
    asyncio.run(main())
