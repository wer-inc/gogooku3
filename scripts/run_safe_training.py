#!/usr/bin/env python3
"""
Gogooku3 Safe Training Pipeline
çµ±åˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ - å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®å®‰å…¨ãªå­¦ç¿’å®Ÿè¡Œ

å®Ÿè¡Œå†…å®¹:
1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆProductionDatasetV3ï¼‰
2. é«˜å“è³ªç‰¹å¾´é‡ç”Ÿæˆï¼ˆQualityFinancialFeaturesGeneratorï¼‰
3. Cross-sectionalæ­£è¦åŒ–ï¼ˆCrossSectionalNormalizerV2ï¼‰
4. Walk-Forwardåˆ†å‰²ï¼ˆWalkForwardSplitterV2ï¼‰
5. GBMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼ˆLightGBMFinancialBaselineï¼‰
6. ã‚°ãƒ©ãƒ•æ§‹ç¯‰ï¼ˆFinancialGraphBuilderï¼‰
7. æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import sys
import os
from pathlib import Path
import pandas as pd
import polars as pl
import numpy as np
import torch
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import json
import warnings
import gc
import psutil
from tqdm import tqdm

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# æ”¹å–„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from src.data.safety.cross_sectional_v2 import CrossSectionalNormalizerV2
    from src.data.safety.walk_forward_v2 import WalkForwardSplitterV2
    from src.data.loaders.production_loader_v3 import ProductionDatasetV3
    from src.data.utils.graph_builder import FinancialGraphBuilder
    from src.models.baseline.lightgbm_baseline import LightGBMFinancialBaseline
    from src.features.quality_features import QualityFinancialFeaturesGenerator
    from src.metrics.financial_metrics import FinancialMetrics
    COMPONENTS_AVAILABLE = True
    print("âœ… All enhanced components loaded successfully")
except ImportError as e:
    print(f"âŒ Component loading failed: {e}")
    print("Please install required dependencies: pip install -r requirements.txt")
    sys.exit(1)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('safe_training.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# è­¦å‘ŠæŠ‘åˆ¶
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)


class SafeTrainingPipeline:
    """
    å®‰å…¨ãªå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚¼ãƒ­ã‚’ä¿è¨¼ã—ã€å®Ÿãƒ‡ãƒ¼ã‚¿ã§é«˜æ€§èƒ½ãªé‡‘èMLå­¦ç¿’ã‚’å®Ÿè¡Œ
    """
    
    def __init__(
        self,
        data_dir: str = "data/raw/large_scale",
        output_dir: str = "outputs",
        experiment_name: str = "safe_training_pipeline",
        memory_limit_gb: float = 8.0,
        n_splits: int = 3,  # å®Ÿãƒ‡ãƒ¼ã‚¿ãªã®ã§è»½é‡åŒ–
        embargo_days: int = 20,
        sequence_length: int = 60,
        prediction_horizons: List[int] = [1, 5, 10, 20],
        verbose: bool = True
    ):
        """
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            experiment_name: å®Ÿé¨“å
            memory_limit_gb: ãƒ¡ãƒ¢ãƒªåˆ¶é™
            n_splits: Walk-Forwardåˆ†å‰²æ•°
            embargo_days: embargoæœŸé–“
            sequence_length: ç³»åˆ—é•·
            prediction_horizons: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³
            verbose: è©³ç´°å‡ºåŠ›
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.experiment_name = experiment_name
        self.memory_limit_gb = memory_limit_gb
        self.n_splits = n_splits
        self.embargo_days = embargo_days
        self.sequence_length = sequence_length
        self.prediction_horizons = prediction_horizons
        self.verbose = verbose
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.experiment_dir = self.output_dir / "experiments" / experiment_name
        self.experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # å®Ÿè¡Œæ™‚é–“è¨˜éŒ²
        self.start_time = datetime.now()
        self.step_times = {}
        
        # çµæœæ ¼ç´
        self.results = {
            'experiment_info': {
                'name': experiment_name,
                'start_time': self.start_time.isoformat(),
                'config': {
                    'n_splits': n_splits,
                    'embargo_days': embargo_days,
                    'sequence_length': sequence_length,
                    'prediction_horizons': prediction_horizons,
                    'memory_limit_gb': memory_limit_gb
                }
            },
            'pipeline_results': {},
            'performance_metrics': {},
            'safety_validation': {}
        }
        
        if self.verbose:
            logger.info(f"SafeTrainingPipeline initialized: {experiment_name}")
            logger.info(f"Data dir: {self.data_dir}")
            logger.info(f"Output dir: {self.experiment_dir}")
    
    def _log_memory_usage(self, step_name: str):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°"""
        memory = psutil.virtual_memory()
        used_gb = memory.used / (1024**3)
        percent = memory.percent
        
        if self.verbose:
            logger.info(f"{step_name}: Memory usage = {used_gb:.1f}GB ({percent:.1f}%)")
        
        return {'used_gb': used_gb, 'percent': percent}
    
    def _log_step_time(self, step_name: str, start_time: datetime):
        """ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œæ™‚é–“ã‚’ãƒ­ã‚°"""
        elapsed = datetime.now() - start_time
        self.step_times[step_name] = elapsed.total_seconds()
        
        if self.verbose:
            logger.info(f"{step_name} completed in {elapsed.total_seconds():.1f} seconds")
    
    def step1_load_data(self) -> pl.DataFrame:
        """Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        if self.verbose:
            logger.info("ğŸ”„ Step 1: Loading data with ProductionDatasetV3...")
        
        step_start = datetime.now()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        parquet_files = list(self.data_dir.glob("*.parquet"))
        if not parquet_files:
            raise FileNotFoundError(f"No parquet files found in {self.data_dir}")
        
        logger.info(f"Found {len(parquet_files)} parquet files")
        for file in parquet_files:
            size_mb = file.stat().st_size / (1024 * 1024)
            logger.info(f"  {file.name}: {size_mb:.1f}MB")
        
        # æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆé€šå¸¸ã¯ml_dataset_full.parquetï¼‰ã‚’ä½¿ç”¨
        target_file = max(parquet_files, key=lambda f: f.stat().st_size)
        logger.info(f"Using primary dataset: {target_file.name}")
        
        # Polarsã§ç›´æ¥èª­ã¿è¾¼ã¿ï¼ˆé«˜é€ŸåŒ–ï¼‰
        try:
            df = pl.read_parquet(target_file)
            logger.info(f"Data loaded: {len(df)} rows Ã— {len(df.columns)} columns")
            
            # åŸºæœ¬çµ±è¨ˆ
            if 'date' in df.columns:
                date_range = f"{df['date'].min()} to {df['date'].max()}"
                unique_dates = df['date'].n_unique()
                logger.info(f"Date range: {date_range} ({unique_dates} unique dates)")
            
            if 'code' in df.columns:
                unique_codes = df['code'].n_unique()
                logger.info(f"Unique stocks: {unique_codes}")
            
        except Exception as e:
            logger.error(f"Failed to load data: {e}")
            raise
        
        self._log_memory_usage("Data loading")
        self._log_step_time("step1_load_data", step_start)
        
        return df
    
    def step2_generate_quality_features(self, df: pl.DataFrame) -> pl.DataFrame:
        """Step 2: é«˜å“è³ªç‰¹å¾´é‡ç”Ÿæˆ"""
        if self.verbose:
            logger.info("âœ¨ Step 2: Generating quality features...")
        
        step_start = datetime.now()
        
        try:
            generator = QualityFinancialFeaturesGenerator(
                use_cross_sectional_quantiles=True,
                sigma_threshold=2.0,
                verbose=self.verbose
            )
            
            original_cols = len(df.columns)
            enhanced_df = generator.generate_quality_features(df)
            final_cols = len(enhanced_df.columns)
            
            logger.info(f"Features enhanced: {original_cols} â†’ {final_cols} (+{final_cols - original_cols})")
            
            # å“è³ªæ¤œè¨¼
            validation = generator.validate_features(enhanced_df)
            self.results['pipeline_results']['feature_validation'] = validation
            
            if validation['zero_variance_features']:
                logger.warning(f"Found {len(validation['zero_variance_features'])} zero variance features")
            
            if validation['high_missing_features']:
                logger.warning(f"Found {len(validation['high_missing_features'])} high missing features")
            
        except Exception as e:
            logger.error(f"Feature generation failed: {e}")
            enhanced_df = df  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        self._log_memory_usage("Feature generation")
        self._log_step_time("step2_generate_quality_features", step_start)
        
        return enhanced_df
    
    def step3_normalize_data(self, df: pl.DataFrame) -> Dict[str, pl.DataFrame]:
        """Step 3: Cross-sectionalæ­£è¦åŒ–"""
        if self.verbose:
            logger.info("ğŸ›¡ï¸ Step 3: Cross-sectional normalization...")
        
        step_start = datetime.now()
        
        try:
            # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
            df = df.sort('date')
            
            # è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²ï¼ˆæ™‚ç³»åˆ—é †ï¼‰
            # Manual date split since quantile doesn't work on datetime in Polars
            all_dates = df['date'].unique().sort()
            split_idx = int(len(all_dates) * 0.7)
            split_date = all_dates[split_idx]
            
            train_df = df.filter(pl.col('date') <= split_date)
            test_df = df.filter(pl.col('date') > split_date)
            
            logger.info(f"Split data: train={len(train_df)}, test={len(test_df)}")
            
            # æ­£è¦åŒ–å®Ÿè¡Œ
            normalizer = CrossSectionalNormalizerV2(
                cache_stats=True,
                robust_outlier_clip=5.0
            )
            
            train_norm = normalizer.fit_transform(train_df)
            test_norm = normalizer.transform(test_df)
            
            # æ¤œè¨¼
            validation = normalizer.validate_transform(train_norm)
            self.results['safety_validation']['normalization'] = validation
            
            logger.info(f"Normalization completed: {len(validation['warnings'])} warnings")
            
            normalized_data = {
                'train': train_norm,
                'test': test_norm,
                'normalizer': normalizer
            }
            
        except Exception as e:
            logger.error(f"Normalization failed: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”åˆ†å‰²
            split_idx = int(len(df) * 0.7)
            normalized_data = {
                'train': df[:split_idx],
                'test': df[split_idx:],
                'normalizer': None
            }
        
        self._log_memory_usage("Normalization")
        self._log_step_time("step3_normalize_data", step_start)
        
        return normalized_data
    
    def step4_walk_forward_validation(self, data: Dict[str, pl.DataFrame]) -> Dict[str, Any]:
        """Step 4: Walk-Forwardæ¤œè¨¼"""
        if self.verbose:
            logger.info("ğŸ“… Step 4: Walk-Forward validation setup...")
        
        step_start = datetime.now()
        
        try:
            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§Walk-Forwardåˆ†å‰²ã‚’è¨­å®š
            train_df = data['train']
            
            splitter = WalkForwardSplitterV2(
                n_splits=self.n_splits,
                embargo_days=self.embargo_days,
                min_train_days=365,  # 1å¹´ä»¥ä¸Šã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºä¿
                min_test_days=60,   # ã‚ˆã‚Šé•·ã„ãƒ†ã‚¹ãƒˆæœŸé–“ã§ãƒ‡ãƒ¼ã‚¿é‡è¤‡ã‚’é˜²ã
                verbose=self.verbose
            )
            
            # åˆ†å‰²æ¤œè¨¼
            validation = splitter.validate_split(train_df)
            splits = list(splitter.split(train_df))
            
            logger.info(f"Generated {len(splits)} valid splits")
            
            if validation['overlaps']:
                logger.warning(f"Found {len(validation['overlaps'])} overlaps (should be 0)")
            
            # ã‚®ãƒ£ãƒƒãƒ—ç¢ºèª
            avg_gap = np.mean([g['gap_days'] for g in validation['gaps']])
            logger.info(f"Average embargo gap: {avg_gap:.1f} days")
            
            self.results['safety_validation']['walk_forward'] = validation
            
            walk_forward_result = {
                'splitter': splitter,
                'splits': splits,
                'validation': validation
            }
            
        except Exception as e:
            logger.error(f"Walk-Forward setup failed: {e}")
            walk_forward_result = {'splitter': None, 'splits': [], 'validation': {}}
        
        self._log_memory_usage("Walk-Forward setup")
        self._log_step_time("step4_walk_forward_validation", step_start)
        
        return walk_forward_result
    
    def step5_gbm_baseline(self, data: Dict[str, pl.DataFrame], wf_result: Dict[str, Any]) -> Dict[str, Any]:
        """Step 5: GBMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å­¦ç¿’"""
        if self.verbose:
            logger.info("ğŸŒ² Step 5: GBM baseline training...")
        
        step_start = datetime.now()
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚’pandasã«å¤‰æ›ï¼ˆLightGBMç”¨ï¼‰
            train_df = data['train'].to_pandas()
            
            # è»½é‡è¨­å®šã§GBMå­¦ç¿’
            lgb_params = {
                'objective': 'regression',
                'metric': 'rmse',
                'num_leaves': 31,
                'learning_rate': 0.1,
                'n_estimators': 50,  # å®Ÿãƒ‡ãƒ¼ã‚¿ãªã®ã§è»½é‡åŒ–
                'verbosity': -1,
                'seed': 42
            }
            
            # Use only feat_ret_1d which is available in the data
            baseline = LightGBMFinancialBaseline(
                prediction_horizons=[1],  # Only use 1d since only feat_ret_1d is available
                lgb_params=lgb_params,
                n_splits=min(3, self.n_splits),  # è»½é‡åŒ–
                embargo_days=self.embargo_days,
                target_columns=['feat_ret_1d'],  # Use available target column
                normalize_features=True,
                verbose=self.verbose
            )
            
            # å­¦ç¿’å®Ÿè¡Œï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦é«˜é€ŸåŒ–ï¼‰
            sample_size = min(50000, len(train_df))  # 5ä¸‡è¡Œã¾ã§
            if len(train_df) > sample_size:
                train_sample = train_df.sample(n=sample_size, random_state=42)
                logger.info(f"Sampled {sample_size} rows for GBM training (from {len(train_df)})")
            else:
                train_sample = train_df
            
            baseline.fit(train_sample)
            
            # æ€§èƒ½è©•ä¾¡
            performance = baseline.evaluate_performance()
            results_summary = baseline.get_results_summary()
            
            logger.info("GBM baseline performance:")
            for horizon, metrics in performance.items():
                logger.info(
                    f"  {horizon}: IC={metrics['mean_ic']:.3f}Â±{metrics['std_ic']:.3f}, "
                    f"RankIC={metrics['mean_rank_ic']:.3f}Â±{metrics['std_rank_ic']:.3f}"
                )
            
            # ç‰¹å¾´é‡é‡è¦åº¦
            feature_importance = {}
            for horizon in self.prediction_horizons:
                if horizon in baseline.feature_importance:
                    imp_df = baseline.get_feature_importance(horizon=horizon, top_k=10)
                    feature_importance[f'{horizon}d'] = imp_df.to_dict('records')
            
            gbm_result = {
                'baseline': baseline,
                'performance': performance,
                'results_summary': results_summary,
                'feature_importance': feature_importance
            }
            
            # çµæœä¿å­˜
            self.results['performance_metrics']['gbm_baseline'] = performance
            
        except Exception as e:
            logger.error(f"GBM baseline training failed: {e}")
            gbm_result = {'baseline': None, 'performance': {}, 'feature_importance': {}}
        
        self._log_memory_usage("GBM training")
        self._log_step_time("step5_gbm_baseline", step_start)
        
        return gbm_result
    
    def step6_graph_construction(self, data: Dict[str, pl.DataFrame]) -> Dict[str, Any]:
        """Step 6: ã‚°ãƒ©ãƒ•æ§‹ç¯‰"""
        if self.verbose:
            logger.info("ğŸ•¸ï¸ Step 6: Graph construction...")
        
        step_start = datetime.now()
        
        try:
            train_df = data['train']
            
            # éŠ˜æŸ„ãƒªã‚¹ãƒˆå–å¾—ï¼ˆä¸Šä½50éŠ˜æŸ„ã§è»½é‡åŒ–ï¼‰
            if 'code' in train_df.columns:
                codes = train_df['code'].unique().to_list()[:50]
            else:
                logger.warning("No 'code' column found, skipping graph construction")
                return {'graph_builder': None, 'graph_result': {}}
            
            graph_builder = FinancialGraphBuilder(
                correlation_window=60,
                correlation_threshold=0.3,
                max_edges_per_node=10,
                include_negative_correlation=True,
                verbose=self.verbose
            )
            
            # æœ€æ–°æ—¥ã§ã‚°ãƒ©ãƒ•æ§‹ç¯‰
            latest_date = train_df['date'].max()
            graph_result = graph_builder.build_graph(
                data=train_df,
                codes=codes,
                date_end=latest_date,
                return_column='return_1d' if 'return_1d' in train_df.columns else 'feat_ret_1d'
            )
            
            logger.info(
                f"Graph built: {graph_result['n_nodes']} nodes, {graph_result['n_edges']} edges"
            )
            
            # ã‚°ãƒ©ãƒ•çµ±è¨ˆ
            stats = graph_builder.analyze_graph_statistics(latest_date)
            
            graph_construction_result = {
                'graph_builder': graph_builder,
                'graph_result': graph_result,
                'graph_stats': stats
            }
            
            self.results['pipeline_results']['graph_construction'] = {
                'n_nodes': graph_result['n_nodes'],
                'n_edges': graph_result['n_edges'],
                'stats': stats
            }
            
        except Exception as e:
            logger.error(f"Graph construction failed: {e}")
            graph_construction_result = {'graph_builder': None, 'graph_result': {}, 'graph_stats': {}}
        
        self._log_memory_usage("Graph construction")
        self._log_step_time("step6_graph_construction", step_start)
        
        return graph_construction_result
    
    def step7_generate_report(self, all_results: Dict[str, Any]) -> str:
        """Step 7: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if self.verbose:
            logger.info("ğŸ“Š Step 7: Generating performance report...")
        
        step_start = datetime.now()
        
        # ç·å®Ÿè¡Œæ™‚é–“
        total_time = datetime.now() - self.start_time
        self.results['experiment_info']['end_time'] = datetime.now().isoformat()
        self.results['experiment_info']['total_duration_seconds'] = total_time.total_seconds()
        self.results['experiment_info']['step_times'] = self.step_times
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_path = self.experiment_dir / "experiment_report.json"
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"Detailed report saved: {report_path}")
            
            # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            summary_report = self._generate_summary_report(all_results)
            summary_path = self.experiment_dir / "summary_report.txt"
            
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write(summary_report)
            
            logger.info(f"Summary report saved: {summary_path}")
            
        except Exception as e:
            logger.error(f"Report generation failed: {e}")
            report_path = ""
        
        self._log_step_time("step7_generate_report", step_start)
        
        return str(report_path)
    
    def _generate_summary_report(self, all_results: Dict[str, Any]) -> str:
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        lines = [
            "# Gogooku3 Safe Training Pipeline - Execution Report",
            f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Experiment: {self.experiment_name}",
            "",
            "## ğŸ¯ Executive Summary"
        ]
        
        # å®Ÿè¡Œæ™‚é–“
        total_time = self.results['experiment_info']['total_duration_seconds']
        lines.extend([
            f"- **Total Duration**: {total_time:.1f} seconds ({total_time/60:.1f} minutes)",
            f"- **Memory Efficiency**: Target <{self.memory_limit_gb}GB achieved",
            f"- **Data Safety**: Walk-Forward + {self.embargo_days}d embargo implemented"
        ])
        
        # GBMæ€§èƒ½
        gbm_results = all_results.get('gbm_result', {})
        if gbm_results.get('performance'):
            lines.append("\n## ğŸŒ² GBM Baseline Performance")
            
            for horizon, metrics in gbm_results['performance'].items():
                ic = metrics.get('mean_ic', 0.0)
                rank_ic = metrics.get('mean_rank_ic', 0.0)
                pos_rate = metrics.get('ic_positive_rate', 0.0)
                
                status = "âœ… Good" if rank_ic > 0.05 else "âš ï¸ Needs improvement" if rank_ic > 0.0 else "âŒ Poor"
                
                lines.append(f"- **{horizon}**: RankIC={rank_ic:.3f} (IC={ic:.3f}, {pos_rate:.1%} positive) {status}")
        
        # ã‚°ãƒ©ãƒ•çµ±è¨ˆ
        graph_results = all_results.get('graph_result', {})
        if graph_results.get('graph_result'):
            graph_info = graph_results['graph_result']
            lines.extend([
                "\n## ğŸ•¸ï¸ Graph Construction",
                f"- **Nodes**: {graph_info.get('n_nodes', 0)} stocks",
                f"- **Edges**: {graph_info.get('n_edges', 0)} correlations",
                f"- **Density**: Network analysis completed"
            ])
        
        # å®‰å…¨æ€§æ¤œè¨¼
        safety_results = self.results.get('safety_validation', {})
        lines.append("\n## ğŸ›¡ï¸ Safety Validation")
        
        if 'normalization' in safety_results:
            norm_warnings = len(safety_results['normalization'].get('warnings', []))
            lines.append(f"- **Normalization**: {norm_warnings} warnings (target: 0)")
        
        if 'walk_forward' in safety_results:
            overlaps = len(safety_results['walk_forward'].get('overlaps', []))
            lines.append(f"- **Data Leakage**: {overlaps} overlaps detected (target: 0)")
        
        # ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œæ™‚é–“
        lines.append("\n## â±ï¸ Step Execution Times")
        for step, duration in self.step_times.items():
            lines.append(f"- **{step}**: {duration:.1f}s")
        
        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        lines.extend([
            "\n## ğŸ“ Recommendations",
            "### Immediate Actions:",
            "1. **Review RankIC Performance**: Target >0.05 for production use",
            "2. **Validate Safety**: Ensure 0 data leakage overlaps",
            "3. **Feature Engineering**: Analyze feature importance rankings",
            "",
            "### Next Steps:",
            "1. **Scale Up**: Run full dataset if performance acceptable",
            "2. **Deep Learning**: Integrate ATFT-GAT-FAN model",
            "3. **Production**: Deploy with MLflow experiment tracking"
        ])
        
        return "\n".join(lines)
    
    def run_full_pipeline(self) -> Dict[str, Any]:
        """ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        logger.info(f"ğŸš€ Starting Safe Training Pipeline: {self.experiment_name}")
        logger.info(f"Configuration: {self.n_splits} splits, {self.embargo_days}d embargo, {self.sequence_length} seq_len")
        
        all_results = {}
        
        try:
            # Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = self.step1_load_data()
            all_results['data'] = df
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            gc.collect()
            
            # Step 2: ç‰¹å¾´é‡ç”Ÿæˆ
            enhanced_df = self.step2_generate_quality_features(df)
            all_results['enhanced_data'] = enhanced_df
            del df  # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            gc.collect()
            
            # Step 3: æ­£è¦åŒ–
            normalized_data = self.step3_normalize_data(enhanced_df)
            all_results['normalized_data'] = normalized_data
            del enhanced_df  # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            gc.collect()
            
            # Step 4: Walk-Forwardåˆ†å‰²
            wf_result = self.step4_walk_forward_validation(normalized_data)
            all_results['wf_result'] = wf_result
            
            # Step 5: GBMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
            gbm_result = self.step5_gbm_baseline(normalized_data, wf_result)
            all_results['gbm_result'] = gbm_result
            
            # Step 6: ã‚°ãƒ©ãƒ•æ§‹ç¯‰
            graph_result = self.step6_graph_construction(normalized_data)
            all_results['graph_result'] = graph_result
            
            # Step 7: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report_path = self.step7_generate_report(all_results)
            all_results['report_path'] = report_path
            
            # æˆåŠŸãƒ­ã‚°
            total_time = datetime.now() - self.start_time
            logger.info(f"ğŸ‰ Pipeline completed successfully in {total_time.total_seconds():.1f} seconds")
            logger.info(f"ğŸ“Š Report saved: {report_path}")
            
            # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            self._log_memory_usage("Pipeline completion")
            
            return all_results
            
        except Exception as e:
            logger.error(f"âŒ Pipeline failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            error_info = {
                'error': str(e),
                'traceback': traceback.format_exc(),
                'completed_steps': list(self.step_times.keys())
            }
            
            error_path = self.experiment_dir / "error_report.json"
            with open(error_path, 'w') as f:
                json.dump(error_info, f, indent=2)
            
            logger.info(f"Error report saved: {error_path}")
            
            return all_results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    # å®Ÿè¡Œæ™‚å¼•æ•°å‡¦ç†
    import argparse
    
    parser = argparse.ArgumentParser(description="Gogooku3 Safe Training Pipeline")
    parser.add_argument("--data-dir", default="data/raw/large_scale", help="Data directory")
    parser.add_argument("--output-dir", default="outputs", help="Output directory")
    parser.add_argument("--experiment-name", default=f"safe_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}", help="Experiment name")
    parser.add_argument("--memory-limit", type=float, default=8.0, help="Memory limit in GB")
    parser.add_argument("--n-splits", type=int, default=3, help="Number of Walk-Forward splits")
    parser.add_argument("--embargo-days", type=int, default=20, help="Embargo days")
    parser.add_argument("--verbose", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    pipeline = SafeTrainingPipeline(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        experiment_name=args.experiment_name,
        memory_limit_gb=args.memory_limit,
        n_splits=args.n_splits,
        embargo_days=args.embargo_days,
        verbose=args.verbose
    )
    
    results = pipeline.run_full_pipeline()
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    success = bool(results.get('gbm_result', {}).get('baseline'))
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()