# GPU ML Pipeline - å®Œå…¨ã‚¬ã‚¤ãƒ‰

## ğŸ“‹ æ¦‚è¦

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€gogooku3-standaloneã«ãŠã‘ã‚‹GPUç’°å¢ƒã§ã®æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‹ã‚‰å­¦ç¿’ã¾ã§ï¼‰ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚
ã™ã¹ã¦ã®è¨­å®šã¯æ°¸ç¶šåŒ–æ¸ˆã¿ã§ã€ç°¡å˜ãªã‚³ãƒãƒ³ãƒ‰ã§å®Ÿè¡Œå¯èƒ½ã§ã™ã€‚

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

```bash
# 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆGPU-ETLä½¿ç”¨ï¼‰
make dataset-full-gpu START=2020-09-19 END=2025-09-19

# 2. GPUå­¦ç¿’ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè‡ªå‹•æ¤œå‡ºï¼‰
make train-gpu-latest

# 3. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
watch -n 1 nvidia-smi
```

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆGPU-ETLï¼‰

### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆv2.1.0ä»¥é™ï¼‰

**GPU-ETLã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹ã«ãªã£ã¦ã„ã¾ã™ã€‚** ç‰¹åˆ¥ãªè¨­å®šã¯ä¸è¦ã§ã™ã€‚

### å®Ÿè¡Œæ–¹æ³•

#### 1. æ¨™æº–ã‚³ãƒãƒ³ãƒ‰ï¼ˆGPU-ETLè‡ªå‹•æœ‰åŠ¹ï¼‰
```bash
# Makefileã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
make dataset-full START=2020-09-19 END=2025-09-19

# ç›´æ¥å®Ÿè¡Œ
python scripts/pipelines/run_full_dataset.py \
  --jquants --start-date 2020-09-19 --end-date 2025-09-19
```

#### 2. æ˜ç¤ºçš„GPUç‰ˆã‚³ãƒãƒ³ãƒ‰
```bash
# GPU-ETLå¼·åˆ¶æœ‰åŠ¹
make dataset-full-gpu START=2020-09-19 END=2025-09-19

# ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ6ãƒ¶æœˆï¼‰
./scripts/run_dataset_gpu.sh

# ã‚«ã‚¹ã‚¿ãƒ æœŸé–“æŒ‡å®š
./scripts/run_dataset_gpu.sh --start-date 2020-09-19 --end-date 2025-09-19
```

#### 3. ç ”ç©¶ç”¨è¨­å®šï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»˜ãï¼‰
```bash
make dataset-full-research START=2020-09-19 END=2025-09-19
```

### GPU-ETLã§é«˜é€ŸåŒ–ã•ã‚Œã‚‹å‡¦ç†

1. **Cross-sectional normalization**: æ—¥æ¬¡æ–­é¢ã§ã®æ­£è¦åŒ–
2. **Rank computation**: ãƒ©ãƒ³ã‚¯è¨ˆç®—
3. **Z-score calculation**: Z-ã‚¹ã‚³ã‚¢è¨ˆç®—
4. **Correlation matrix**: ç›¸é–¢è¡Œåˆ—è¨ˆç®—

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®å®‰

- **å°è¦æ¨¡ï¼ˆ1-2æ—¥ï¼‰**: CPUã®æ–¹ãŒé«˜é€Ÿãªå ´åˆã‚ã‚Š
- **ä¸­è¦æ¨¡ï¼ˆ1é€±é–“-1ãƒ¶æœˆï¼‰**: GPU-ETLã§1.5-3å€é«˜é€ŸåŒ–
- **å¤§è¦æ¨¡ï¼ˆ6ãƒ¶æœˆä»¥ä¸Šï¼‰**: GPU-ETLã§3-5å€é«˜é€ŸåŒ–

## ğŸ§  GPUå­¦ç¿’

### æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè‡ªå‹•æ¤œå‡ºæ©Ÿèƒ½

`train_gpu_latest.sh`ã¯ä»¥ä¸‹ã®é †ã§æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•æ¤œå‡ºï¼š
1. `output/datasets/ml_dataset_*_full.parquet`
2. `output/ml_dataset_*_full.parquet`

### å®Ÿè¡Œæ–¹æ³•

#### 1. æ¨å¥¨ã‚³ãƒãƒ³ãƒ‰ âœ¨
```bash
# åŸºæœ¬GPUå­¦ç¿’
make train-gpu-latest

# SafeTrainingPipelineæ¤œè¨¼ä»˜ã
make train-gpu-latest-safe
```

#### 2. ã‚¹ã‚¯ãƒªãƒ—ãƒˆç›´æ¥å®Ÿè¡Œ
```bash
# æ¨™æº–å®Ÿè¡Œ
./scripts/train_gpu_latest.sh

# æ¤œè¨¼ä»˜ã
./scripts/train_gpu_latest.sh --safe

# ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
./scripts/train_gpu_latest.sh "" --adv-graph-train 2e-4 75
```

#### 3. ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
TRAIN_BATCH_SIZE=4096 \
TRAIN_VAL_BATCH_SIZE=6144 \
./scripts/train_gpu_latest.sh

# ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´
TRAIN_NUM_WORKERS=16 \
TRAIN_PREFETCH=8 \
./scripts/train_gpu_latest.sh

# ç²¾åº¦è¨­å®š
TRAIN_PRECISION=16-mixed \
./scripts/train_gpu_latest.sh
```

#### 4. è©³ç´°CLIï¼ˆãƒ•ãƒ«åˆ¶å¾¡ï¼‰
```bash
python scripts/integrated_ml_training_pipeline.py \
  --data-path output/datasets/ml_dataset_20250319_20250919_20250919_223415_full.parquet \
  --adv-graph-train \
  train.batch.train_batch_size=4096 \
  train.batch.val_batch_size=6144 \
  train.batch.num_workers=16 \
  train.batch.prefetch_factor=8 \
  train.trainer.accumulate_grad_batches=1 \
  train.trainer.precision=16-mixed \
  train.optimizer.lr=2e-4 \
  train.trainer.max_epochs=75
```

## âš™ï¸ GPUæœ€é©åŒ–è¨­å®š

### ç’°å¢ƒå¤‰æ•°ï¼ˆæ°¸ç¶šåŒ–æ¸ˆã¿ï¼‰

#### .envãƒ•ã‚¡ã‚¤ãƒ«
```bash
# GPUåŸºæœ¬è¨­å®š
FORCE_GPU=1
REQUIRE_GPU=1
USE_GPU_ETL=1
RMM_POOL_SIZE=70GB
CUDA_VISIBLE_DEVICES=0
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# GPUå­¦ç¿’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
GPU_TRAINING_ENABLED=1
DEFAULT_LEARNING_RATE=2e-4
DEFAULT_MAX_EPOCHS=75
ADV_GRAPH_TRAIN=1
```

#### train_gpu_latest.shå†…ã®æœ€é©åŒ–è¨­å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
```bash
# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:512

# cuDNNæœ€é©åŒ–
TORCH_CUDNN_V8_API_ENABLED=1
TORCH_CUDNN_V8_API_ALLOWED=1

# ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–
OMP_NUM_THREADS=16
CUDA_DEVICE_MAX_CONNECTIONS=32
NCCL_P2P_LEVEL=SYS

# ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆå¤§å¹…å¢—åŠ ï¼‰
TRAIN_BATCH_SIZE=4096
TRAIN_VAL_BATCH_SIZE=6144
TRAIN_NUM_WORKERS=16
TRAIN_PREFETCH=8
```

### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ | èª¬æ˜ |
|-----------|------------|------|
| batch_size | 4096 | è¨“ç·´ãƒãƒƒãƒã‚µã‚¤ã‚º |
| val_batch_size | 6144 | æ¤œè¨¼ãƒãƒƒãƒã‚µã‚¤ã‚º |
| num_workers | 16 | ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° |
| prefetch_factor | 8 | ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒä¿‚æ•° |
| precision | 16-mixed | æ··åˆç²¾åº¦å­¦ç¿’ |
| learning_rate | 2e-4 | å­¦ç¿’ç‡ |
| max_epochs | 75 | æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•° |
| accumulate_grad | 1 | å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ— |

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆGPU-ETLï¼‰
- **å‡¦ç†é€Ÿåº¦**: CPUæ¯” 3-5å€é«˜é€Ÿ
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨**: RMM pool 70GB
- **A100 80GB**: 6ãƒ¶æœˆãƒ‡ãƒ¼ã‚¿ã‚’ç´„30åˆ†ã§å‡¦ç†

### MLå­¦ç¿’ï¼ˆGPUï¼‰
- **å­¦ç¿’æ™‚é–“**: A100ã§2-3æ™‚é–“ï¼ˆ75ã‚¨ãƒãƒƒã‚¯ï¼‰
- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: 5130 samples/sec
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨**: æœ€å¤§60GBï¼ˆãƒ¢ãƒ‡ãƒ«+ãƒ‡ãƒ¼ã‚¿ï¼‰

### æœŸå¾…ã•ã‚Œã‚‹ç²¾åº¦
- **Target Sharpe**: 0.849
- **RankIC@1d**: 0.180ä»¥ä¸Š
- **Model Parameters**: 5.6M

## ğŸ”„ å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¾‹

### æœ¬ç•ªç’°å¢ƒå‘ã‘ï¼ˆ6ãƒ¶æœˆãƒ‡ãƒ¼ã‚¿ï¼‰
```bash
# 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
make dataset-full-gpu START=2020-09-19 END=2025-09-19

# 2. ãƒ‡ãƒ¼ã‚¿ç¢ºèª
ls -lht output/datasets/ml_dataset_*_full.parquet | head -1

# 3. GPUå­¦ç¿’é–‹å§‹
make train-gpu-latest

# 4. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
# åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§
watch -n 1 nvidia-smi
tensorboard --logdir logs/
```

### ç ”ç©¶ãƒ»å®Ÿé¨“å‘ã‘
```bash
# 1. ç ”ç©¶ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»˜ãï¼‰
make dataset-full-research START=2020-09-19 END=2025-09-19

# 2. SafeTrainingPipelineæ¤œè¨¼ä»˜ãå­¦ç¿’
make train-gpu-latest-safe

# 3. çµæœåˆ†æ
make research-plus DATASET=output/ml_dataset_latest_full.parquet
```

### HPOï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼‰ä½µç”¨
```bash
# 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
make dataset-full-gpu START=2020-09-19 END=2025-09-19

# 2. HPOå®Ÿè¡Œ
make train-integrated-hpo

# 3. æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å†å­¦ç¿’
make train-gpu-latest
```

## ğŸ” ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### GPUä½¿ç”¨çŠ¶æ³
```bash
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
watch -n 1 nvidia-smi

# GPUãƒ¡ãƒ¢ãƒªã®ã¿
nvidia-smi --query-gpu=memory.used,memory.total --format=csv -l 1

# ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèª
nvidia-smi pmon -i 0
```

### å­¦ç¿’é€²æ—
```bash
# ãƒ­ã‚°ç›£è¦–
tail -f logs/ml_training.log

# TensorBoard
tensorboard --logdir logs/ --port 6006
```

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¢ºèª
```bash
# æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
ls -lht output/datasets/ml_dataset_*_full.parquet | head -1

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª
python -c "import polars as pl; df=pl.scan_parquet('output/datasets/*.parquet'); print(df.collect().shape)"
```

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„
```bash
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
make dataset-full-gpu START=2020-09-19 END=2025-09-19

# ç¢ºèª
find output -name "*.parquet" -type f
```

### CUDA Out of Memory
```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
TRAIN_BATCH_SIZE=2048 \
TRAIN_VAL_BATCH_SIZE=3072 \
./scripts/train_gpu_latest.sh

# ã¾ãŸã¯CLIã§
python scripts/integrated_ml_training_pipeline.py \
  --data-path <dataset> \
  train.batch.train_batch_size=2048
```

### GPU ãŒæ¤œå‡ºã•ã‚Œãªã„
```bash
# CUDAç¢ºèª
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
python -c "import torch; print(f'Devices: {torch.cuda.device_count()}')"

# cuDFç¢ºèª
python -c "import cudf; print('cuDF OK')"
```

### RMMåˆæœŸåŒ–è­¦å‘Š
```
RMM init failed: module 'rmm' has no attribute 'rmm_cupy_allocator'
```
ã“ã‚Œã¯æ­£å¸¸ã§ã™ã€‚pool allocatorãªã—ã§ã‚‚GPU-ETLã¯å‹•ä½œã—ã¾ã™ã€‚

## ğŸ“Š ä½œæˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±

### æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ2025-09-19æ™‚ç‚¹ï¼‰
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `ml_dataset_20250319_20250919_20250919_223415_full.parquet`
- **ã‚µã‚¤ã‚º**: 480,973è¡Œ Ã— 395åˆ—
- **æœŸé–“**: 2025-03-19 ï½ 2025-09-19ï¼ˆ6ãƒ¶æœˆï¼‰
- **éŠ˜æŸ„æ•°**: 3,850
- **ç‰¹å¾´é‡**: 359 features + metadata

### ç‰¹å¾´é‡ã‚«ãƒ†ã‚´ãƒª
- **ä¾¡æ ¼/å‡ºæ¥é«˜**: ~70åˆ—
- **ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«**: ~20åˆ—
- **å¸‚å ´ï¼ˆTOPIXï¼‰**: ~30åˆ—
- **ãƒ•ãƒ­ãƒ¼**: ~37åˆ—ï¼ˆæ‹¡å¼µç‰ˆï¼‰
- **ãƒãƒ¼ã‚¸ãƒ³**: ~86åˆ—ï¼ˆé€±æ¬¡+æ—¥æ¬¡ï¼‰
- **è²¡å‹™**: ~20åˆ—
- **ãã®ä»–**: ~146åˆ—

## ğŸ“ é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«

### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
- `.env`: ç’°å¢ƒå¤‰æ•°è¨­å®š
- `configs/atft/train/production.yaml`: æœ¬ç•ªå­¦ç¿’è¨­å®š
- `configs/pipeline/full_dataset.yaml`: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆè¨­å®š

### ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- `scripts/train_gpu_latest.sh`: GPUå­¦ç¿’è‡ªå‹•å®Ÿè¡Œ
- `scripts/run_dataset_gpu.sh`: GPU-ETLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
- `scripts/integrated_ml_training_pipeline.py`: çµ±åˆMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- `GPU_ETL_USAGE.md`: GPU-ETLä½¿ç”¨ã‚¬ã‚¤ãƒ‰
- `GPU_TRAINING.md`: GPUå­¦ç¿’ã‚¬ã‚¤ãƒ‰
- `docs/ml/dataset_new.md`: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä»•æ§˜ï¼ˆ395åˆ—ï¼‰

## ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ã‚ˆã‚Šé•·æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿**: 1å¹´ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
2. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›
3. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–**: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
4. **AutoMLçµ±åˆ**: Optunaç­‰ã«ã‚ˆã‚‹ã•ã‚‰ãªã‚‹æœ€é©åŒ–

---
æœ€çµ‚æ›´æ–°: 2025-09-19
ãƒãƒ¼ã‚¸ãƒ§ãƒ³: v2.1.0