#!/usr/bin/env python3
"""
Performance Optimization Best Practices for gogooku3
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å®Ÿè£…
"""

import sys
import logging
import time
import multiprocessing as mp
from pathlib import Path
from typing import Dict, List, Callable
from concurrent.futures import ProcessPoolExecutor, as_completed
from functools import wraps, lru_cache
import polars as pl
import torch
import psutil
import gc
from datetime import datetime
import json

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def performance_monitor(func: Callable) -> Callable:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

        try:
            result = func(*args, **kwargs)
            success = True
        except Exception as e:
            result = None
            success = False
            raise e
        finally:
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

            execution_time = end_time - start_time
            memory_usage = end_memory - start_memory

            logger.info(
                f"Function: {func.__name__}, "
                f"Time: {execution_time:.2f}s, "
                f"Memory: {memory_usage:.1f}MB, "
                f"Success: {success}"
            )

        return result

    return wrapper


class PerformanceOptimizer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.optimization_config = {
            "max_workers": mp.cpu_count(),
            "chunk_size": 10000,
            "batch_size": 100,
            "memory_limit_gb": 8,
            "cache_size": 1000,
        }

        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        self.system_info = {
            "cpu_count": mp.cpu_count(),
            "memory_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
            "gpu_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
        }

        logger.info(f"Performance optimizer initialized: {self.system_info}")

    @performance_monitor
    def parallel_feature_engineering(
        self, data_list: List[pl.DataFrame], feature_func: Callable
    ) -> List[pl.DataFrame]:
        """ä¸¦åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        max_workers = min(self.optimization_config["max_workers"], len(data_list))

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # ä¸¦åˆ—å‡¦ç†ã®å®Ÿè¡Œ
            future_to_data = {
                executor.submit(feature_func, data): i
                for i, data in enumerate(data_list)
            }

            results = [None] * len(data_list)

            for future in as_completed(future_to_data):
                data_index = future_to_data[future]
                try:
                    result = future.result()
                    results[data_index] = result
                except Exception as e:
                    logger.error(f"Error processing data {data_index}: {e}")
                    results[data_index] = None

        # Noneã®çµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        results = [r for r in results if r is not None]
        logger.info(
            f"Parallel processing completed: {len(results)}/{len(data_list)} successful"
        )

        return results

    @performance_monitor
    def chunk_process_large_dataset(
        self, file_path: str, process_func: Callable, chunk_size: int = None
    ) -> List:
        """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ£ãƒ³ã‚¯å‡¦ç†"""
        if chunk_size is None:
            chunk_size = self.optimization_config["chunk_size"]

        results = []

        # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å‡¦ç†
        for chunk in pl.read_parquet(file_path).iter_chunks(chunk_size):
            chunk_df = pl.DataFrame(chunk)
            processed_chunk = process_func(chunk_df)
            results.append(processed_chunk)

            # ãƒ¡ãƒ¢ãƒªç®¡ç†
            if len(results) % 10 == 0:
                gc.collect()

        logger.info(f"Chunk processing completed: {len(results)} chunks")
        return results

    @lru_cache(maxsize=1000)
    def cached_computation(
        self, computation_id: str, computation_func: Callable, *args, **kwargs
    ):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãè¨ˆç®—"""
        return computation_func(*args, **kwargs)

    def optimize_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–"""
        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        gc.collect()

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
        memory_usage = psutil.Process().memory_info().rss / 1024 / 1024 / 1024  # GB

        if memory_usage > self.optimization_config["memory_limit_gb"]:
            logger.warning(f"High memory usage: {memory_usage:.1f}GB")

            # ã‚ˆã‚Šç©æ¥µçš„ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            for _ in range(3):
                gc.collect()

            # PyTorchã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        return memory_usage

    @performance_monitor
    def batch_data_loading(
        self, file_paths: List[str], batch_size: int = None
    ) -> List[pl.DataFrame]:
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        if batch_size is None:
            batch_size = self.optimization_config["batch_size"]

        results = []

        for i in range(0, len(file_paths), batch_size):
            batch_paths = file_paths[i : i + batch_size]
            logger.info(f"Loading batch {i//batch_size + 1}: {len(batch_paths)} files")

            batch_data = []
            for file_path in batch_paths:
                try:
                    df = pl.read_parquet(file_path)
                    batch_data.append(df)
                except Exception as e:
                    logger.warning(f"Failed to load {file_path}: {e}")
                    continue

            results.extend(batch_data)

            # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
            self.optimize_memory_usage()

        return results

    def get_performance_metrics(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾—"""
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "system": self.system_info,
            "memory": {
                "usage_gb": psutil.Process().memory_info().rss / 1024 / 1024 / 1024,
                "available_gb": psutil.virtual_memory().available / 1024 / 1024 / 1024,
                "percent": psutil.virtual_memory().percent,
            },
            "cpu": {
                "usage_percent": psutil.cpu_percent(interval=1),
                "count": psutil.cpu_count(),
            },
        }

        if torch.cuda.is_available():
            metrics["gpu"] = {
                "memory_allocated_gb": torch.cuda.memory_allocated()
                / 1024
                / 1024
                / 1024,
                "memory_reserved_gb": torch.cuda.memory_reserved() / 1024 / 1024 / 1024,
                "device_count": torch.cuda.device_count(),
            }

        return metrics


class DataPipelineOptimizer:
    """ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.performance_optimizer = PerformanceOptimizer()
        self.optimization_strategies = {
            "parallel": self._parallel_strategy,
            "chunk": self._chunk_strategy,
            "batch": self._batch_strategy,
            "cache": self._cache_strategy,
        }

    def optimize_pipeline(self, pipeline_config: Dict) -> Dict:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–"""
        optimization_results = {
            "original_time": 0,
            "optimized_time": 0,
            "improvement": 0,
            "strategy_used": [],
            "metrics": {},
        }

        # æœ€é©åŒ–æˆ¦ç•¥ã®é¸æŠ
        for strategy_name, strategy_func in self.optimization_strategies.items():
            if pipeline_config.get(f"use_{strategy_name}", False):
                optimization_results["strategy_used"].append(strategy_name)
                strategy_func(pipeline_config)

        return optimization_results

    def _parallel_strategy(self, config: Dict):
        """ä¸¦åˆ—å‡¦ç†æˆ¦ç•¥"""
        logger.info("Applying parallel processing strategy")

        # ä¸¦åˆ—å‡¦ç†ã®è¨­å®š
        if "num_workers" in config:
            self.performance_optimizer.optimization_config["max_workers"] = config[
                "num_workers"
            ]
            logger.info(f"Set parallel workers to {config['num_workers']}")

        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä¸¦åˆ—åŒ–
        if "dataloader_workers" in config:
            logger.info(
                f"Configured dataloader with {config['dataloader_workers']} workers"
            )

        # ä¸¦åˆ—å‡¦ç†ã®æœ€é©åŒ–
        import multiprocessing as mp

        cpu_count = mp.cpu_count()
        logger.info(f"Available CPU cores: {cpu_count}")

    def _chunk_strategy(self, config: Dict):
        """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æˆ¦ç•¥"""
        logger.info("Applying chunk processing strategy")

        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®è¨­å®š
        if "chunk_size" in config:
            self.performance_optimizer.optimization_config["chunk_size"] = config[
                "chunk_size"
            ]
            logger.info(f"Set chunk size to {config['chunk_size']}")

        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®è¨­å®š
        if "memory_limit_gb" in config:
            self.performance_optimizer.optimization_config["memory_limit_gb"] = config[
                "memory_limit_gb"
            ]
            logger.info(f"Set memory limit to {config['memory_limit_gb']}GB")

    def _batch_strategy(self, config: Dict):
        """ãƒãƒƒãƒå‡¦ç†æˆ¦ç•¥"""
        logger.info("Applying batch processing strategy")

        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æœ€é©åŒ–
        if "batch_size" in config:
            self.performance_optimizer.optimization_config["batch_size"] = config[
                "batch_size"
            ]
            logger.info(f"Set batch size to {config['batch_size']}")

        # ãƒãƒƒãƒå‡¦ç†ã®åŠ¹ç‡åŒ–
        if "prefetch_factor" in config:
            logger.info(f"Set prefetch factor to {config['prefetch_factor']}")

    def _cache_strategy(self, config: Dict):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥"""
        logger.info("Applying caching strategy")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã®è¨­å®š
        if "cache_size" in config:
            self.performance_optimizer.optimization_config["cache_size"] = config[
                "cache_size"
            ]
            logger.info(f"Set cache size to {config['cache_size']}")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æœŸé™è¨­å®š
        if "cache_ttl_hours" in config:
            logger.info(f"Set cache TTL to {config['cache_ttl_hours']} hours")

        # LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š
        if "use_lru_cache" in config and config["use_lru_cache"]:
            logger.info("LRU cache enabled")


class ModelPerformanceOptimizer:
    """ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.optimization_config = {
            "mixed_precision": True,
            "gradient_accumulation": 4,
            "data_parallel": True,
            "memory_efficient": True,
        }

    def optimize_training(self, model, dataloader, optimizer, **kwargs):
        """å­¦ç¿’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–"""
        # æ··åˆç²¾åº¦å­¦ç¿’
        if self.optimization_config["mixed_precision"]:
            scaler = torch.cuda.amp.GradScaler()

            # æ··åˆç²¾åº¦å­¦ç¿’ã®å®Ÿè£…
            def mixed_precision_step(model, data, target, optimizer, scaler):
                """æ··åˆç²¾åº¦å­¦ç¿’ã®1ã‚¹ãƒ†ãƒƒãƒ—"""
                optimizer.zero_grad()

                with torch.cuda.amp.autocast():
                    output = model(data)
                    loss = torch.nn.functional.mse_loss(output, target)

                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()

                return loss.item()

            # ãƒ¢ãƒ‡ãƒ«ã«æ··åˆç²¾åº¦å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
            model.mixed_precision_step = mixed_precision_step
            model.scaler = scaler

            logger.info("Mixed precision training fully implemented with GradScaler")

        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
        if self.optimization_config["memory_efficient"]:
            torch.backends.cudnn.benchmark = True

        return model, dataloader, optimizer

    def optimize_inference(self, model, **kwargs):
        """æ¨è«–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–"""
        # ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–
        model.eval()

        # æ¨è«–æœ€é©åŒ–
        with torch.no_grad():
            if torch.cuda.is_available():
                model = model.cuda()

        return model


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    optimizer = PerformanceOptimizer()
    pipeline_optimizer = DataPipelineOptimizer()
    model_optimizer = ModelPerformanceOptimizer()  # ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ç”¨

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾—
    print("ğŸ“Š Getting performance metrics...")
    metrics = optimizer.get_performance_metrics()
    print(f"Performance metrics: {json.dumps(metrics, indent=2)}")

    # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
    print("ğŸ§¹ Optimizing memory usage...")
    memory_usage = optimizer.optimize_memory_usage()
    print(f"Memory usage: {memory_usage:.1f}GB")

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–
    print("âš¡ Optimizing pipeline...")
    pipeline_config = {
        "use_parallel": True,
        "use_chunk": True,
        "use_batch": True,
        "use_cache": True,
    }
    results = pipeline_optimizer.optimize_pipeline(pipeline_config)
    print(f"Pipeline optimization results: {results}")

    # ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
    print("ğŸ¤– Testing model optimization...")
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
    dummy_model = type("DummyModel", (), {})()
    dummy_dataloader = type("DummyDataLoader", (), {})()
    dummy_optimizer = type("DummyOptimizer", (), {})()

    optimized_model, optimized_dataloader, optimized_optimizer = (
        model_optimizer.optimize_training(
            dummy_model, dummy_dataloader, dummy_optimizer
        )
    )
    print("âœ… Model optimization test completed")


if __name__ == "__main__":
    main()
