# gogooku5 Dataset Builder (Skeleton)

This package will host the standalone dataset generation pipeline described in `../MIGRATION_PLAN.md`.

## Roadmap
1. Implement configuration and API clients under `src/builder/api/`.
2. Migrate feature engineering modules (`core`, `legacy`, `macro`).
3. Introduce full and optimized pipelines under `src/builder/pipelines/` with caching support.
4. Deliver integration tests that rebuild one month of data and compare with `gogooku3` outputs.

## Usage
```bash
# Ensure environment variables are set
cp .env.example .env

# Build 30-day dataset (START/END default to last 30 days)
make build

# Override date range
make build START=2024-01-01 END=2024-01-31

# Warm caches without writing parquet output
make build-optimized START=2024-01-01 END=2024-01-31 CACHE_ONLY=1
```

## Output Artifacts
- Timestamped parquet + metadata pairs are written under `output/` using the pattern
  `ml_dataset_{YYYYMMDDYYYYMMDD}_{timestamp}_full.parquet`.
- Symlinks `ml_dataset_latest.parquet` and `ml_dataset_latest_full.parquet` always point to the newest dataset.
- Metadata is persisted alongside the parquet (`*_metadata.json`) with `ml_dataset_latest_metadata.json` tracking the latest snapshot.
- Retention (default: keep 3 snapshots) and compression (`zstd` by default) are configurable via `.env`.

## Schema: SecId Column (Phase 3 Migration)

**Version**: Introduced in schema v1.2.0 (2025-11-14)

### What is SecId?

`SecId` is a **globally stable integer identifier** (Int32 â†’ Categorical) for securities, designed to replace string-based `Code` joins with high-performance int32 joins internally.

**Key properties**:
- **Type**: `Categorical` (optimized from Int32, range 1-5088)
- **Source**: Generated from `dim_security.parquet` (security master table)
- **Nullability**: `true` (historical/delisted securities not in dim_security will have NULL SecId)
- **Backward compatibility**: `Code` column remains present alongside `SecId`

### Why SecId?

**Performance improvements** (Phase 3 join migration):
- **Join speed**: 30-50% faster (Int32 vs String comparison)
- **Memory**: ~50% reduction in join column footprint
- **Cache locality**: Better CPU cache utilization with int32 keys

**7 internal joins migrated** from `Code` (String) â†’ `sec_id` (Int32):
1. Quotes + Listed (eager/lazy)
2. Quotes + Margin features
3. Margin adjustment lookups
4. GPU features join

### Schema Details

```python
# Column specification
{
  "name": "SecId",
  "dtype": "Categorical",  # Optimized 8-bit encoding for 193 unique values (Q1 2024)
  "nullable": true         # NULL for delisted/unknown codes
}
```

### Usage

**For downstream consumers**:
```python
import polars as pl

# Load dataset
df = pl.read_parquet("ml_dataset.parquet")

# SecId is available alongside Code
assert "SecId" in df.columns  # âœ…
assert "Code" in df.columns   # âœ… Backward compatible

# High-performance joins (use SecId when possible)
dim_security = pl.read_parquet("dim_security.parquet")
result = df.join(dim_security, on="SecId", how="left")  # 30-50% faster than Code join
```

**NULL handling**:
```python
# Typical Q1 2024 stats:
# - Total rows: 222,774
# - Valid SecId: 10,244 (4.6%)
# - NULL SecId: 212,530 (95.4%) - delisted securities

# Filter to currently listed securities only
active_df = df.filter(pl.col("SecId").is_not_null())
```

### Migration Status

| Phase | Description | Status | Date |
|-------|-------------|--------|------|
| **Phase 1** | dim_security generation | âœ… Complete | 2025-10-XX |
| **Phase 2** | sec_id propagation + categorical | âœ… Complete | 2025-10-XX |
| **Phase 3.1** | Internal join migration (7 joins) | âœ… Complete | 2025-11-XX |
| **Phase 3.2** | SecId output propagation | âœ… Complete | 2025-11-14 |

**Implementation details**: See `/tmp/phase3_completion_report.md` for full technical documentation.

## Testing
```bash
# Run data package tests (requires using the package source path)
PYTHONPATH=gogooku5/data/src pytest gogooku5/data/tests -q
```

## Data Fetchers
- `builder.api.advanced_fetcher.AdvancedJQuantsFetcher` wraps legacy async clients so gogooku5 can download TOPIX, indices, trades spec, margin (daily/weekly), futures, options, short sellingã€dividendsã€earning statements, etc.
- `data/tools/clean_empty_cache.py` can purge stale cache files (0-row Parquet/IPC) before a new build. Run `python data/tools/clean_empty_cache.py --dry-run` to inspect and without `--dry-run` to delete.
- Use `AdvancedJQuantsFetcher` together with `builder.utils.asyncio.run_sync` when integrating new pipelines to avoid manual event loop handling.
- `builder.features.core.flow.enhanced.FlowFeatureEngineer` converts cached trades-spec data into flow metrics (`foreign_sentiment`, `smart_flow_indicator`, etc.), driven by `DataSourceManager.trades_spec()`.
- Parity check CLI: `python scripts/compare_parity.py <gogooku3 parquet> <gogooku5 parquet> [--output-json report.json]` to inspect schema and numeric differences. For automated runs, set `PARITY_BASELINE_PATH=/path/to/gogooku3_parquet` (and optionally `PARITY_CANDIDATE_PATH=/path/to/gogooku5_parquet`) before `python tools/project-health-check.sh`.
- DatasetBuilder now materialises a fullå–¶æ¥­æ—¥Ã—éŠ˜æŸ„ã‚°ãƒªãƒƒãƒ‰ï¼ˆæ—¥æœ¬ã®ç¥æ—¥ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ï¼‹å®Ÿè¦³æ¸¬æ—¥ä»˜ï¼‰ã‚’ãƒ™ãƒ¼ã‚¹ã«å„ç‰¹å¾´é‡ã‚’ä»˜ä¸ã—ã€æ¬ ææ—¥ã®å¯è¦–åŒ–ã¨ gogooku3 ã¨ã®ãƒ‘ãƒªãƒ†ã‚£ç¢ºèªã‚’å®¹æ˜“ã«ã—ã¦ã„ã¾ã™ã€‚

Index option fetches (/option/index_option) respect `INDEX_OPTION_PARALLEL_FETCH=true`ï¼ˆæ—¢å®šï¼‰ã¨ `INDEX_OPTION_PARALLEL_CONCURRENCY`ï¼ˆæ—¢å®š:8ï¼‰ã§ä¸¦åˆ—å–å¾—ã§ãã€`SOURCE_CACHE_*` è¨­å®šã¨çµ„ã¿åˆã‚ã›ã¦ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å†åˆ©ç”¨ã§ãã¾ã™ã€‚

### Source cache controls

APIã‚½ãƒ¼ã‚¹ï¼ˆè²¡å‹™ãƒ»é…å½“ãƒ»ç©ºå£²ã‚Šãƒ»ãƒãƒ¼ã‚¸ãƒ³ãƒ»æ±ºç®—ãªã©ï¼‰ã¯ `output/cache` ã«ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã•ã‚Œã¾ã™ã€‚ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•° or Dagster resource è¨­å®šã§æŒ™å‹•ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¾ã™ã€‚

| è¨­å®š | èª¬æ˜ |
| --- | --- |
| `SOURCE_CACHE_MODE` | `read_write` (æ—¢å®š) / `read` / `off` |
| `SOURCE_CACHE_FORCE_REFRESH` | `true` ã§ TTL ã‚’ç„¡è¦–ã—ã¦å¸¸ã« API ã‹ã‚‰å†å–å¾— |
| `SOURCE_CACHE_ASOF` | `YYYY-MM-DD` ã‚„ `today`ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã« `asof-<date>` ãŒä»˜ä¸ã•ã‚Œã€åŒä¸€ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å†åˆ©ç”¨å¯èƒ½ |
| `SOURCE_CACHE_TAG` | ä»»æ„ã‚¿ã‚°ï¼ˆä¾‹ `backfill`ï¼‰ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã¨ãƒ¡ã‚¿æƒ…å ±ã«è¨˜éŒ² |
| `SOURCE_CACHE_TTL_OVERRIDE_DAYS` | ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ã”ã¨ã® TTL ã‚’ã¾ã¨ã‚ã¦ä¸Šæ›¸ã |

Dagster ã§ã¯ `dataset_builder` resource ã« `source_cache_*` ã‚’æ¸¡ã™ã“ã¨ã§ run å˜ä½ã§ã“ã‚Œã‚‰ã‚’æŒ‡å®šã§ãã¾ã™ã€‚

### MLflow é€£æº

`ENABLE_MLFLOW_LOGGING=1` ã‚’è¨­å®šã™ã‚‹ã¨ã€Dagster è³‡ç”£ï¼ˆãƒãƒ£ãƒ³ã‚¯æ§‹ç¯‰ãƒ»ãƒãƒ¼ã‚¸ï¼‰ã¨ Apex Ranker å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒ MLflow ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’è¨˜éŒ²ã—ã¾ã™ã€‚

| å¤‰æ•° | èª¬æ˜ |
| --- | --- |
| `ENABLE_MLFLOW_LOGGING` | `1` ã§ãƒ­ã‚®ãƒ³ã‚°æœ‰åŠ¹åŒ– |
| `MLFLOW_EXPERIMENT_NAME` | å®Ÿé¨“åï¼ˆæ—¢å®š: `tse-forecasting`ï¼‰ |
| `MLFLOW_TRACKING_URI` | ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚µãƒ¼ãƒ URI |
| `dagster_run_id` (ã‚¿ã‚°) | Dagster run ã¨ MLflow run ã‚’ç´ä»˜ã‘ã‚‹ãŸã‚è‡ªå‹•ä»˜ä¸ |

Dagster ã® resource config ã§ã‚‚ `enable_mlflow_logging`, `mlflow_experiment_name`, `mlflow_tracking_uri` ã‚’ä¸Šæ›¸ãã§ãã¾ã™ã€‚

Detailed pipeline behavior, feature coverage, and validation routines will be documented as implementation progresses through the migration milestones.

### Chunkãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

ãƒãƒ£ãƒ³ã‚¯å‡ºåŠ›ã®æ•´åˆæ€§ã¯ `data/tools/check_chunks.py` ã§ç¢ºèªã§ãã¾ã™:

```bash
python gogooku5/data/tools/check_chunks.py \
  --chunks-dir /workspace/gogooku3/output/chunks \
  --fail-on-warning
```

`status.json`/`metadata.json`/Parquet ã®æ¬ è½ã‚„ `rows=0`ã€`state!="completed"` ãªã©ã‚’ä¸€è¦§åŒ–ã—ã€`--fail-on-warning` ã‚’ä»˜ã‘ã‚‹ã¨ç•°å¸¸æ™‚ã«çµ‚äº†ã‚³ãƒ¼ãƒ‰1ã‚’è¿”ã—ã¾ã™ã€‚

### Dataset hash / schema fingerprint

`merge_chunks.py` ã¯æœ€çµ‚ Parquet ã‚’æ›¸ãå‡ºã™éš›ã«

- `dataset_hash`ï¼ˆParquetæœ¬ä½“ã®SHA256ï¼‰
- `feature_schema_version`ï¼ˆåˆ—å+dtype hashï¼‰

ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¸åŸ‹ã‚è¾¼ã¿ã€Dagster assetï¼å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã“ã®æƒ…å ±ã‚’ MLflow ã‚¿ã‚°ã«è¨˜éŒ²ã—ã¾ã™ã€‚  
`metadata.json` ã«ä¸¡æ–¹ã®å€¤ãŒç„¡ã„å ´åˆã¯å­¦ç¿’ã‚’é–‹å§‹ã§ããªã„ã®ã§ã€å¸¸ã«æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ãƒ“ãƒ«ãƒ€ãƒ¼ã§ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

### Dataset quality checker

`data/tools/check_dataset_quality.py` ã¯å®Œæˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒãƒ£ãƒ³ã‚¯å˜ä½ / ãƒ•ãƒ«ãƒãƒ¼ã‚¸ï¼‰ã«å¯¾ã—ã¦

- `(date, code)` ä¸»ã‚­ãƒ¼é‡è¤‡
- æŒ‡å®šã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã®æ¬ æ
- æœªæ¥æ—¥ãƒ‡ãƒ¼ã‚¿æ··å…¥
- as-of é †åºï¼ˆä¾‹: `fs_disclosed_date <= date`ï¼‰

ã‚’ä¸€æ‹¬æ¤œæŸ»ã—ã¾ã™ã€‚JSON ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ã«ã‚‚å¯¾å¿œã—ã¦ã„ã‚‹ãŸã‚ã€`tools/project-health-check.sh` ã‚„ CI ã«çµ„ã¿è¾¼ã‚“ã§å“è³ªã‚’è‡ªå‹•ç›£è¦–ã—ã¦ãã ã•ã„ã€‚

ç’°å¢ƒå¤‰æ•°ã§ DatasetBuilder å®Ÿè¡Œæ™‚ã«è‡ªå‹•ãƒã‚§ãƒƒã‚¯ã‚’æœ‰åŠ¹åŒ–ã§ãã¾ã™:

| å¤‰æ•° | èª¬æ˜ |
| --- | --- |
| `ENABLE_DATASET_QUALITY_CHECK=1` | ãƒãƒ£ãƒ³ã‚¯/ãƒ•ãƒ«æ›¸ãå‡ºã—ç›´å¾Œã«ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œï¼ˆå¤±æ•—ã§ãƒ“ãƒ«ãƒ‰åœæ­¢ï¼‰ |
| `DATASET_QUALITY_TARGETS` | ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ or ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰ |
| `DATASET_QUALITY_ASOF_CHECKS` | `col<=reference_col` å½¢å¼ã® as-of åˆ¶ç´„ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ or ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰ |

`.env.example` ã§ã¯ `ret_prev_1d/5d/20d/60d` ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã€`DisclosedDate` ã¨ `earnings_event_date` ã‚’ as-of åˆ¶ç´„ã¨ã—ã¦å®šç¾©ã—ã¦ã„ã¾ã™ã€‚åˆ¥ã®åˆ—ï¼é–¾å€¤ã‚’ä½¿ã„ãŸã„å ´åˆã¯ä¸Šè¨˜ã®å¤‰æ•°ã‚’ä¸Šæ›¸ãã—ã¦ãã ã•ã„ã€‚
| `DATASET_QUALITY_FAIL_ON_WARNING` | `1` ã§è­¦å‘Šã‚‚å¤±æ•—æ‰±ã„ |
| `DATASET_QUALITY_DATE_COL` / `DATASET_QUALITY_CODE_COL` | ä¸»ã‚­ãƒ¼åˆ—åï¼ˆæ—¢å®š: `date` / `code`ï¼‰ |
| `DATASET_QUALITY_ALLOW_FUTURE_DAYS` | æœªæ¥æ—¥è¨±å®¹æ—¥æ•°ï¼ˆæ—¢å®š: 0ï¼‰ |

## Dagster Integration
`gogooku5/data/src/dagster_gogooku5` ships reusable Dagster assets that wrap the dataset builder:

```bash
# Launch Dagster UI with the gogooku5 definitions
export DAGSTER_HOME=/workspace/gogooku3/gogooku5   # use absolute path
PYTHONPATH=gogooku5/data/src dagster dev -m dagster_gogooku5.defs
```

- `g5_dataset_chunks`: builds DatasetBuilder chunks for a configurable date range. Configure `start`, `end`, `chunk_months`, etc. directly in Dagster.
- `g5_dataset_full`: merges the latest completed chunks by invoking the existing `data/tools/merge_chunks.py` helper.
- `dataset_builder_resource`: initializes `DatasetBuilder` with optional overrides (output dir, dataset tag, refresh behavior).

These assets allow you to schedule recurring dataset builds via Dagster jobs or run adâ€‘hoc chunk builds/merges from the UI with full observability.

> ğŸ•’ **Timezone**
> `gogooku5/dagster.yaml` sets `instance.local_timezone` to `Asia/Tokyo` to ensure all Dagster run timestamps are in JST.
> Export `DAGSTER_HOME=/absolute/path/to/gogooku5` ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰ before running `dagster dev` / `dagster job â€¦` to use this configuration.
