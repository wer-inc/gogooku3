#!/usr/bin/env python3
"""
gogooku3-standalone ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å£Šã‚Œãšãƒ»å¼·ããƒ»é€Ÿã ã‚’å®Ÿç¾ã™ã‚‹é‡‘èML ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆå®Ÿè¡Œ

åˆ©ç”¨å¯èƒ½ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:
1. å®‰å…¨å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆSafe Training Pipelineï¼‰
2. MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ï¼ˆML Dataset Builderï¼‰
3. ç›´æ¥APIãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ï¼ˆDirect API Dataset Builderï¼‰
4. å®Œå…¨ATFTå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆComplete ATFT Training Pipelineï¼‰
5. ATFTæ¨è«–ï¼ˆATFT Inferenceï¼‰
"""

import sys
import asyncio
import argparse
import logging
from pathlib import Path
from typing import Optional, List, Dict
from datetime import datetime
import pandas as pd
import polars as pl

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(str(Path(__file__).parent))

from scripts.run_safe_training import SafeTrainingPipeline
from scripts.data.ml_dataset_builder import MLDatasetBuilder
from scripts.data.direct_api_dataset_builder import DirectAPIDatasetBuilder
from scripts.integrated_ml_training_pipeline import CompleteATFTTrainingPipeline

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/main.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class GoGooKu3MainRunner:
    """gogooku3-standalone ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.ensure_directories()
    
    def ensure_directories(self):
        """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        dirs = [
            "logs",
            "data/processed", 
            "output",
            "output/results",
            "output/checkpoints",
            "output/atft_data"
        ]
        
        for dir_path in dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)

    def run_safe_training(self, mode: str = "full"):
        """å®‰å…¨å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting Safe Training Pipeline...")
        
        pipeline = SafeTrainingPipeline()
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼ˆåŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        result = pipeline.run_full_pipeline()

        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if 'error' in result:
            logger.error(f"âŒ Safe Training Pipeline failed: {result['error']}")
            return False, result
        else:
            logger.info("âœ… Safe Training Pipeline completed successfully!")
            logger.info(f"ğŸ“Š Report saved: {result.get('report_path', 'N/A')}")
            return True, result

    def run_ml_dataset_builder(self):
        """MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting ML Dataset Builder...")
        
        builder = MLDatasetBuilder()
        result = builder.build_enhanced_dataset()
        
        if result:
            logger.info("âœ… ML Dataset Builder completed successfully!")
            logger.info(f"ğŸ“Š Dataset: {len(result['df'])} rows, {result['metadata']['stocks']} stocks")
            return True, result
        else:
            logger.error("âŒ ML Dataset Builder failed")
            return False, {}

    def run_expand_dataset(self, max_stocks: int = 500):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ‹¡å¼µã®å®Ÿè¡Œï¼ˆéŠ˜æŸ„ãƒ™ãƒ¼ã‚¹ï¼‰"""
        logger.info("ğŸš€ Starting Dataset Expansion...")
        logger.info(f"ğŸ“Š Target: {max_stocks} new stocks")

        try:
            from scripts.data.expand_dataset import JQuantsDatasetExpander

            expander = JQuantsDatasetExpander()
            success = expander.expand_dataset(max_stocks=max_stocks)

            if success:
                logger.info("âœ… Dataset expansion completed successfully!")
                return True, {"message": f"Dataset expanded with {max_stocks} new stocks"}
            else:
                logger.error("âŒ Dataset expansion failed")
                return False, {"error": "Dataset expansion failed"}

        except ImportError as e:
            logger.error(f"âŒ Failed to import expansion module: {e}")
            return False, {"error": f"Import error: {e}"}
        except Exception as e:
            logger.error(f"âŒ Dataset expansion failed with exception: {e}")
            return False, {"error": str(e)}

    def run_expand_dataset_by_date(self, date: str, exclude_market_codes: Optional[List[str]] = None):
        """æ—¥ä»˜ãƒ™ãƒ¼ã‚¹å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—ã®å®Ÿè¡Œï¼ˆMarketCodeãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œï¼‰"""
        logger.info("ğŸš€ Starting Date-based Dataset Expansion...")
        logger.info(f"ğŸ“… Target Date: {date}")
        logger.info(f"ğŸ“Š Exclude MarketCodes: {exclude_market_codes or ['0105', '0109']}")

        try:
            from scripts.data.expand_dataset import JQuantsDatasetExpander

            expander = JQuantsDatasetExpander()
            all_quotes = expander.expand_dataset_by_date(date, exclude_market_codes=exclude_market_codes)

            if all_quotes:
                logger.info("âœ… Date-based dataset expansion completed successfully!")
                return True, {"message": f"Retrieved {len(all_quotes)} filtered stocks for {date}"}
            else:
                logger.error("âŒ Date-based dataset expansion failed")
                return False, {"error": "Date-based dataset expansion failed"}

        except ImportError as e:
            logger.error(f"âŒ Failed to import expansion module: {e}")
            return False, {"error": f"Import error: {e}"}
        except Exception as e:
            logger.error(f"âŒ Date-based dataset expansion failed with exception: {e}")
            return False, {"error": str(e)}

    def run_expand_dataset_by_range(self, start_date: str, end_date: str):
        """æœŸé–“ãƒ™ãƒ¼ã‚¹å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—ã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting Range-based Dataset Expansion...")
        logger.info(f"ğŸ“… Date Range: {start_date} ~ {end_date}")

        try:
            from scripts.data.expand_dataset import JQuantsDatasetExpander

            expander = JQuantsDatasetExpander()
            all_quotes = expander.expand_dataset_by_date_range(start_date, end_date)

            if all_quotes:
                logger.info("âœ… Range-based dataset expansion completed successfully!")
                return True, {"message": f"Retrieved {len(all_quotes)} records for {start_date} ~ {end_date}"}
            else:
                logger.error("âŒ Range-based dataset expansion failed")
                return False, {"error": "Range-based dataset expansion failed"}

        except ImportError as e:
            logger.error(f"âŒ Failed to import expansion module: {e}")
            return False, {"error": f"Import error: {e}"}
        except Exception as e:
            logger.error(f"âŒ Range-based dataset expansion failed with exception: {e}")
            return False, {"error": str(e)}

    def run_expand_historical_all_stocks(self, years: int = 5, max_days: Optional[int] = None, exclude_market_codes: Optional[List[str]] = None):
        """å–å¼•ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‚’ä½¿ã£ãŸéå»Nå¹´åˆ†ã®å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—ã®å®Ÿè¡Œï¼ˆMarketCodeãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œï¼‰"""
        logger.info("ğŸš€ Starting Historical All Stocks Dataset Expansion...")
        logger.info(f"ğŸ“… Years: {years}")
        logger.info(f"ğŸ“Š Max Days: {max_days if max_days else 'No limit'}")
        logger.info(f"ğŸ“Š Exclude MarketCodes: {exclude_market_codes or ['0105', '0109']}")

        try:
            from scripts.data.expand_dataset import JQuantsDatasetExpander

            expander = JQuantsDatasetExpander()
            all_quotes = expander.get_historical_all_stocks(years=years, max_days=max_days, exclude_market_codes=exclude_market_codes)

            if all_quotes:
                # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                df = pd.DataFrame(all_quotes)

                # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‡¦ç†
                if "Date" in df.columns:
                    df["Date"] = pd.to_datetime(df["Date"])
                    df = df.rename(columns={"Date": "date"})

                # ãƒ•ã‚¡ã‚¤ãƒ«åã«æœŸé–“ã‚’å«ã‚ã‚‹
                output_file = expander.data_dir / f"historical_filtered_stocks_{years}years_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
                df.to_parquet(output_file, index=False)

                logger.info(f"ğŸ’¾ éå»{years}å¹´åˆ†ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {output_file}")
                logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿: {len(df)}è¡Œ, {df['Code'].nunique()}éŠ˜æŸ„")

                logger.info("âœ… Historical all stocks dataset expansion completed successfully!")
                return True, {
                    "message": f"Retrieved {len(all_quotes)} filtered records for {years} years",
                    "file": str(output_file),
                    "stocks": df['Code'].nunique(),
                    "total_records": len(df)
                }
            else:
                logger.error("âŒ Historical all stocks dataset expansion failed")
                return False, {"error": "Historical all stocks dataset expansion failed"}

        except ImportError as e:
            logger.error(f"âŒ Failed to import expansion module: {e}")
            return False, {"error": f"Import error: {e}"}
        except Exception as e:
            logger.error(f"âŒ Historical all stocks dataset expansion failed with exception: {e}")
            return False, {"error": str(e)}

    def create_ml_dataset(self, years: int = 5, exclude_market_codes: Optional[List[str]] = None, use_existing_data: bool = True):
        """éå»å–ã‚Œã‚‹å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦MLç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        logger.info("ğŸš€ Creating ML Dataset from Historical Data...")
        logger.info(f"ğŸ“… Years: {years}")
        logger.info(f"ğŸ“Š Exclude MarketCodes: {exclude_market_codes or ['0105', '0109']}")
        logger.info(f"ğŸ“Š Use Existing Data: {use_existing_data}")

        try:
            all_quotes = []

            if use_existing_data:
                # Step 1: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨
                logger.info("ğŸ“Š Step 1: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹")
                existing_quotes = self._load_existing_data()
                if existing_quotes:
                    all_quotes.extend(existing_quotes)
                    logger.info(f"âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(existing_quotes)}ä»¶")

            # Step 2: æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆAPIãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            if not use_existing_data or len(all_quotes) < 10000:  # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã¯APIå–å¾—
                logger.info("ğŸ“Š Step 2: æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚’é–‹å§‹")
                try:
                    from scripts.data.expand_dataset import JQuantsDatasetExpander
                    expander = JQuantsDatasetExpander()

                    new_quotes = expander.get_historical_all_stocks(
                        years=min(years, 2),  # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦æœ€å¤§2å¹´ã«åˆ¶é™
                        max_days=50,  # æœ€å¤§50æ—¥é–“ã«åˆ¶é™
                        exclude_market_codes=exclude_market_codes
                    )

                    if new_quotes:
                        all_quotes.extend(new_quotes)
                        logger.info(f"âœ… æ–°è¦ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(new_quotes)}ä»¶")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ–°è¦ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¹ã‚­ãƒƒãƒ—: {e}")

            if not all_quotes:
                logger.error("âŒ åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return False, {"error": "No data available"}

            # Step 3: MLç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            logger.info("ğŸ”§ Step 3: MLç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚’é–‹å§‹")
            ml_dataset_path = self._create_ml_dataset_from_quotes(all_quotes)

            if not ml_dataset_path:
                logger.error("âŒ MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False, {"error": "Failed to create ML dataset"}

            logger.info("âœ… ML Dataset Creation completed successfully!")

            return True, {
                "message": f"Created ML dataset from {len(all_quotes)} historical records",
                "raw_data_records": len(all_quotes),
                "ml_dataset_path": ml_dataset_path,
                "years": years,
                "excluded_markets": exclude_market_codes or ['0105', '0109'],
                "used_existing_data": use_existing_data
            }

        except ImportError as e:
            logger.error(f"âŒ Failed to import expansion module: {e}")
            return False, {"error": f"Import error: {e}"}
        except Exception as e:
            logger.error(f"âŒ ML Dataset creation failed with exception: {e}")
            return False, {"error": str(e)}

    def _create_ml_dataset_from_quotes(self, all_quotes: List[Dict]) -> Optional[str]:
        """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰MLç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        logger.info("ğŸ”§ MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå‡¦ç†ã‚’é–‹å§‹")

        try:
            import numpy as np

            df = pl.DataFrame(all_quotes)

            # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‡¦ç†
            if "Date" in df.columns:
                df = df.with_columns([
                    pl.col("Date").str.strptime(pl.Date, format="%Y-%m-%d", strict=False).alias("date")
                ]).drop("Date")

            logger.info(f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿: {len(df)}è¡Œ")

            # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
            df = self._clean_stock_data(df)

            # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            df = self._create_ml_features(df)

            # MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ä¿å­˜
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = Path("data/processed") / f"ml_dataset_{timestamp}.parquet"
            output_path.parent.mkdir(parents=True, exist_ok=True)

            df.write_parquet(output_path)

            logger.info(f"ğŸ’¾ MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜: {output_path}")
            logger.info(f"ğŸ“Š MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")

            return str(output_path)

        except Exception as e:
            logger.error(f"âŒ MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _load_existing_data(self) -> List[Dict]:
        """æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§çµ±åˆ"""
        logger.info("ğŸ“ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹")

        try:
            import polars as pl
            import pandas as pd
            from pathlib import Path

            data_dir = Path("data/raw/large_scale")
            all_quotes = []

            # åˆ©ç”¨å¯èƒ½ãªparquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            parquet_files = list(data_dir.glob("*.parquet"))

            for file_path in parquet_files:
                try:
                    logger.info(f"ğŸ“„ {file_path.name} ã‚’èª­ã¿è¾¼ã¿ä¸­...")

                    # Polarsã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                    df = pl.read_parquet(file_path)

                    # pandas DataFrameã«å¤‰æ›ã—ã¦è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                    pandas_df = df.to_pandas()

                    # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‡¦ç†
                    if "Date" in pandas_df.columns:
                        pandas_df["Date"] = pd.to_datetime(pandas_df["Date"])
                        pandas_df = pandas_df.rename(columns={"Date": "date"})

                    # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚«ãƒ©ãƒ ã®çµ±ä¸€ï¼ˆCode or code â†’ Codeï¼‰
                    if "code" in pandas_df.columns and "Code" not in pandas_df.columns:
                        pandas_df = pandas_df.rename(columns={"code": "Code"})

                    # ä¾¡æ ¼ã‚«ãƒ©ãƒ ã®çµ±ä¸€ï¼ˆClose/c_use â†’ Closeï¼‰
                    if "c_use" in pandas_df.columns and "Close" not in pandas_df.columns:
                        pandas_df = pandas_df.rename(columns={"c_use": "Close"})
                    elif "close" in pandas_df.columns and "Close" not in pandas_df.columns:
                        pandas_df = pandas_df.rename(columns={"close": "Close"})

                    # å¿…è¦ãªã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
                    if "Code" in pandas_df.columns and "date" in pandas_df.columns and "Close" in pandas_df.columns:
                        # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿æŠ½å‡º
                        pandas_df = pandas_df[["Code", "date", "Close"]]
                        quotes = pandas_df.to_dict('records')
                        all_quotes.extend(quotes)
                        logger.info(f"   âœ… {len(quotes)}ä»¶èª­ã¿è¾¼ã¿å®Œäº†")
                    else:
                        logger.warning(f"   âš ï¸ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚‰ãªã„: Code={('Code' in pandas_df.columns)}, date={('date' in pandas_df.columns)}, Close={('Close' in pandas_df.columns)}")
                        continue

                    logger.info(f"   ğŸ“Š ç¾åœ¨ã®ç´¯è¨ˆ: {len(all_quotes)}ä»¶")

                except Exception as e:
                    logger.warning(f"   âš ï¸ {file_path.name} ã®èª­ã¿è¾¼ã¿ã‚¹ã‚­ãƒƒãƒ—: {e}")
                    continue

            # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®é™¤å»ï¼ˆåŒã˜éŠ˜æŸ„ãƒ»æ—¥ä»˜ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ã‚½ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆï¼‰
            if all_quotes:
                logger.info("ğŸ”„ é‡è¤‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’é–‹å§‹")
                temp_df = pd.DataFrame(all_quotes)

                # ã¾ãšã€å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
                logger.info(f"ğŸ“Š é‡è¤‡å‡¦ç†å‰: {len(temp_df)}è¡Œ")

                # Codeã¨dateã§é‡è¤‡ã‚’é™¤å»ï¼ˆã‚ˆã‚Šè©³ç´°ãªæ¡ä»¶ã§ï¼‰
                if 'Code' in temp_df.columns and 'date' in temp_df.columns:
                    before_count = len(temp_df)
                    # åŒã˜éŠ˜æŸ„ãƒ»åŒã˜æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
                    temp_df = temp_df.sort_values(['Code', 'date'])
                    temp_df = temp_df.drop_duplicates(subset=['Code', 'date'], keep='first')
                    after_count = len(temp_df)

                    duplicates_removed = before_count - after_count
                    logger.info(f"ğŸ§¹ é‡è¤‡ãƒ‡ãƒ¼ã‚¿é™¤å»: {duplicates_removed}ä»¶")

                    # éŠ˜æŸ„æ•°ã®å¤‰åŒ–ã‚’ç¢ºèª
                    unique_stocks_before = before_count  # ã“ã‚Œã¯æ­£ç¢ºã§ã¯ãªã„ãŒå‚è€ƒå€¤
                    unique_stocks_after = temp_df['Code'].nunique()
                    logger.info(f"ğŸ“Š éŠ˜æŸ„æ•°: {unique_stocks_after}éŠ˜æŸ„")
                else:
                    logger.warning("âš ï¸ Codeã¾ãŸã¯dateã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚é‡è¤‡é™¤å»ã‚’ã‚¹ã‚­ãƒƒãƒ—")

                # è¾æ›¸ãƒªã‚¹ãƒˆã«æˆ»ã™
                all_quotes = temp_df.to_dict('records')

            logger.info(f"ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(all_quotes)}ä»¶")
            return all_quotes

        except Exception as e:
            logger.error(f"âŒ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _clean_stock_data(self, df):
        """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆPolarsæœ€é©åŒ–ç‰ˆï¼‰"""
        logger.info("ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚’é–‹å§‹")
        original_count = len(df)

        # æ¬ æå€¤å‡¦ç†ï¼ˆå¿…é ˆé …ç›®ã®ã¿ï¼‰
        df = df.filter(pl.col('Close').is_not_null())

        # ç•°å¸¸å€¤é™¤å»ï¼ˆä¾¡æ ¼ãŒ0ä»¥ä¸‹ã¯é™¤å¤–ï¼‰
        df = df.filter(pl.col('Close') > 0)

        # OHLCãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ã‚‹å ´åˆã®ã¿ï¼‰
        ohlc_cols = ['Open', 'High', 'Low']
        if all(col in df.columns for col in ohlc_cols):
            df = df.filter(
                (pl.col('Open') > 0) & 
                (pl.col('High') > 0) & 
                (pl.col('Low') > 0)
            )

        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        df = df.sort(['Code', 'date'])

        cleaned_count = len(df)
        removed_count = original_count - cleaned_count

        logger.info(f"âœ… ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å®Œäº†: {cleaned_count}è¡Œ (é™¤å»: {removed_count}è¡Œ)")

        return df

    def _create_ml_features(self, df):
        """MLç”¨ç‰¹å¾´é‡ã®ä½œæˆï¼ˆPolarsæœ€é©åŒ–ç‰ˆï¼‰"""
        logger.info("ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’é–‹å§‹")

        stock_counts = df.group_by('Code').len().filter(pl.col('len') >= 2)
        valid_codes = stock_counts.select('Code').to_series().to_list()
        
        if not valid_codes:
            logger.warning("âš ï¸ ç‰¹å¾´é‡ä½œæˆå¯¾è±¡ã®éŠ˜æŸ„ãŒã‚ã‚Šã¾ã›ã‚“")
            return df
        
        df = df.filter(pl.col('Code').is_in(valid_codes))

        df = df.sort(['Code', 'date']).with_columns([
            # ä¾¡æ ¼å¤‰å‹•ç‡
            pl.col('Close').pct_change().over('Code').alias('price_change'),
            
            pl.col('Close').rolling_mean(window_size=5, min_periods=1).over('Code').alias('ma5'),
            pl.col('Close').rolling_mean(window_size=10, min_periods=1).over('Code').alias('ma10'),
            pl.col('Close').rolling_mean(window_size=20, min_periods=1).over('Code').alias('ma20'),
            pl.col('Close').rolling_mean(window_size=60, min_periods=1).over('Code').alias('ma60'),
            
            pl.col('Close').pct_change().rolling_std(window_size=20, min_periods=5).over('Code').alias('volatility'),
            
            # ç›®çš„å¤‰æ•°: ç¿Œæ—¥ã®ä¾¡æ ¼å¤‰å‹•
            (pl.col('Close').shift(-1).over('Code') / pl.col('Close') - 1).alias('target')
        ])

        df = df.with_columns([
            pl.col('Close').diff().over('Code').alias('price_diff')
        ]).with_columns([
            pl.when(pl.col('price_diff') > 0)
            .then(pl.col('price_diff'))
            .otherwise(0)
            .rolling_mean(window_size=14, min_periods=1)
            .over('Code')
            .alias('gain'),
            
            pl.when(pl.col('price_diff') < 0)
            .then(-pl.col('price_diff'))
            .otherwise(0)
            .rolling_mean(window_size=14, min_periods=1)
            .over('Code')
            .alias('loss')
        ]).with_columns([
            (100 - (100 / (1 + pl.col('gain') / (pl.col('loss') + 1e-10)))).alias('rsi')
        ]).drop(['price_diff', 'gain', 'loss'])

        df = df.with_columns([
            pl.col('Close').ewm_mean(span=12, adjust=False).over('Code').alias('ema12'),
            pl.col('Close').ewm_mean(span=26, adjust=False).over('Code').alias('ema26')
        ]).with_columns([
            (pl.col('ema12') - pl.col('ema26')).alias('macd')
        ]).with_columns([
            pl.col('macd').ewm_mean(span=9, adjust=False).over('Code').alias('macd_signal')
        ]).drop(['ema12', 'ema26'])

        # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡
        potential_features = ['price_change', 'ma5', 'ma10', 'ma20', 'ma60',
                            'volatility', 'rsi', 'macd', 'macd_signal']
        available_features = [col for col in potential_features if col in df.columns]

        # ç‰¹å¾´é‡ã®æ¬ æå€¤ã‚’åŸ‹ã‚ã‚‹ï¼ˆå‰å€¤è£œå®Œï¼‰
        if available_features:
            df = df.with_columns([
                pl.col(col).forward_fill().over('Code') for col in available_features
            ])

        # targetãŒNaNã®è¡Œã‚’é™¤å¤–
        if 'target' in df.columns:
            df = df.filter(pl.col('target').is_not_null())

        logger.info(f"âœ… ç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(df)}è¡Œ")
        logger.info(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: {available_features}")

        return df.to_pandas()

    async def run_direct_api_dataset_builder(self):
        """ç›´æ¥APIãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting Direct API Dataset Builder...")
        
        builder = DirectAPIDatasetBuilder()
        result = await builder.build_direct_api_dataset()
        
        if result:
            logger.info("âœ… Direct API Dataset Builder completed successfully!")
            logger.info(f"ğŸ“Š Dataset: {len(result['df'])} rows, {result['df']['Code'].n_unique()} stocks")
            return True, result
        else:
            logger.error("âŒ Direct API Dataset Builder failed")
            return False, {}

    async def run_complete_atft_training(self):
        """å®Œå…¨ATFTå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting Complete ATFT Training Pipeline...")
        
        pipeline = CompleteATFTTrainingPipeline()
        success, result = await pipeline.run_complete_training_pipeline()
        
        if success:
            logger.info("âœ… Complete ATFT Training Pipeline completed successfully!")
            logger.info(f"ğŸ¯ Target Sharpe: 0.849")
            return True, result
        else:
            logger.error(f"âŒ Complete ATFT Training Pipeline failed: {result.get('error')}")
            return False, result

    def run_workflow(self, workflow: str, mode: str = "full", stocks: int = 500, date: Optional[str] = None, start_date: Optional[str] = None, end_date: Optional[str] = None, years: int = 5, max_days: Optional[int] = None, exclude_market_codes: Optional[List[str]] = None, use_existing_data: bool = True):
        """æŒ‡å®šã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ"""
        logger.info(f"ğŸ¬ Starting workflow: {workflow}")

        try:
            if workflow == "safe-training":
                return self.run_safe_training(mode)
            elif workflow == "ml-dataset":
                return self.run_ml_dataset_builder()
            elif workflow == "direct-api-dataset":
                return asyncio.run(self.run_direct_api_dataset_builder())
            elif workflow == "complete-atft":
                return asyncio.run(self.run_complete_atft_training())
            elif workflow == "expand-dataset":
                return self.run_expand_dataset(max_stocks=stocks)
            elif workflow == "expand-dataset-by-date":
                if date is None:
                    logger.error("âŒ date parameter is required for expand-dataset-by-date")
                    return False, {"error": "date parameter is required"}
                return self.run_expand_dataset_by_date(date, exclude_market_codes=exclude_market_codes)
            elif workflow == "expand-dataset-by-range":
                if start_date is None or end_date is None:
                    logger.error("âŒ start_date and end_date parameters are required for expand-dataset-by-range")
                    return False, {"error": "start_date and end_date parameters are required"}
                return self.run_expand_dataset_by_range(start_date, end_date)
            elif workflow == "expand-historical-all-stocks":
                return self.run_expand_historical_all_stocks(years=years, max_days=max_days, exclude_market_codes=exclude_market_codes)
            elif workflow == "create-ml-dataset":
                return self.create_ml_dataset(years=years, exclude_market_codes=exclude_market_codes, use_existing_data=use_existing_data)
            else:
                logger.error(f"âŒ Unknown workflow: {workflow}")
                return False, {"error": f"Unknown workflow: {workflow}"}

        except Exception as e:
            logger.error(f"âŒ Workflow {workflow} failed with exception: {e}")
            import traceback
            traceback.print_exc()
            return False, {"error": str(e)}


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="gogooku3-standalone - å£Šã‚Œãšãƒ»å¼·ããƒ»é€Ÿã ã‚’å®Ÿç¾ã™ã‚‹é‡‘èML ã‚·ã‚¹ãƒ†ãƒ ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
åˆ©ç”¨å¯èƒ½ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:
  safe-training               å®‰å…¨å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆæ¨å¥¨ï¼‰
  ml-dataset                  MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
  direct-api-dataset          ç›´æ¥APIãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
  complete-atft               å®Œå…¨ATFTå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
  expand-dataset              J-Quants APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‹¡å¼µï¼ˆéŠ˜æŸ„ãƒ™ãƒ¼ã‚¹ï¼‰
  expand-dataset-by-date      J-Quants APIã‹ã‚‰æ—¥ä»˜ãƒ™ãƒ¼ã‚¹å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
  expand-dataset-by-range     J-Quants APIã‹ã‚‰æœŸé–“ãƒ™ãƒ¼ã‚¹å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
  expand-historical-all-stocks J-Quantså–å¼•ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼APIã‚’ä½¿ã£ã¦éå»Nå¹´åˆ†ã®å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
  create-ml-dataset           éå»å–ã‚Œã‚‹å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦MLç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ

ä½¿ç”¨ä¾‹:
  python main.py safe-training --mode full
  python main.py ml-dataset
  python main.py direct-api-dataset
  python main.py complete-atft
  python main.py expand-dataset --stocks 1000
  python main.py expand-dataset-by-date --date 2024-08-29
  python main.py expand-dataset-by-range --start-date 2024-08-01 --end-date 2024-08-29
  python main.py expand-historical-all-stocks --years 5 --max-days 100
  python main.py expand-historical-all-stocks --years 3 --exclude-market-codes 0105 0109 0110
  python main.py create-ml-dataset --years 5 --exclude-market-codes 0105 0109
  python main.py create-ml-dataset --years 2 --use-existing-data --exclude-market-codes 0105 0109
  python main.py create-ml-dataset --years 1 --no-existing-data
        """
    )
    
    parser.add_argument(
        "workflow",
        choices=["safe-training", "ml-dataset", "direct-api-dataset", "complete-atft", "expand-dataset", "expand-dataset-by-date", "expand-dataset-by-range", "expand-historical-all-stocks", "create-ml-dataset"],
        help="å®Ÿè¡Œã™ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"
    )

    parser.add_argument(
        "--mode",
        choices=["quick", "full"],
        default="full",
        help="å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ï¼ˆsafe-trainingã®ã¿ï¼šquick=1ã‚¨ãƒãƒƒã‚¯, full=å®Œå…¨å­¦ç¿’ï¼‰"
    )

    parser.add_argument(
        "--stocks",
        type=int,
        default=500,
        help="expand-datasetæ™‚ã®å–å¾—éŠ˜æŸ„æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 500ï¼‰"
    )

    parser.add_argument(
        "--date",
        type=str,
        help="expand-dataset-by-dateæ™‚ã®å¯¾è±¡æ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰"
    )

    parser.add_argument(
        "--start-date",
        type=str,
        help="expand-dataset-by-rangeæ™‚ã®é–‹å§‹æ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰"
    )

    parser.add_argument(
        "--end-date",
        type=str,
        help="expand-dataset-by-rangeæ™‚ã®çµ‚äº†æ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰"
    )

    parser.add_argument(
        "--years",
        type=int,
        default=5,
        help="expand-historical-all-stocksæ™‚ã®éå»å¹´æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰"
    )

    parser.add_argument(
        "--max-days",
        type=int,
        help="expand-historical-all-stocksæ™‚ã®æœ€å¤§å–å¾—æ—¥æ•°ï¼ˆåˆ¶é™ãªã—ã®å ´åˆã¯æŒ‡å®šãªã—ï¼‰"
    )

    parser.add_argument(
        "--exclude-market-codes",
        type=str,
        nargs='*',
        default=['0105', '0109'],
        help="é™¤å¤–ã™ã‚‹MarketCodeã®ãƒªã‚¹ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0105 0109ï¼‰"
    )

    parser.add_argument(
        "--use-existing-data",
        action='store_true',
        default=True,
        help="æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰"
    )

    parser.add_argument(
        "--no-existing-data",
        action='store_false',
        dest='use_existing_data',
        help="æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ç„¡è¦–ã—ã¦æ–°è¦å–å¾—ã®ã¿ã‚’è¡Œã†"
    )
    
    args = parser.parse_args()
    
    # ãƒãƒŠãƒ¼è¡¨ç¤º
    print("=" * 80)
    print("ğŸš€ gogooku3-standalone - å£Šã‚Œãšãƒ»å¼·ããƒ»é€Ÿã")
    print("ğŸ“ˆ é‡‘èML ã‚·ã‚¹ãƒ†ãƒ çµ±åˆå®Ÿè¡Œç’°å¢ƒ")
    print("=" * 80)
    print(f"Workflow: {args.workflow}")
    print(f"Mode: {args.mode}")
    print("=" * 80)
    
    # ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒŠãƒ¼å®Ÿè¡Œ
    runner = GoGooKu3MainRunner()
    
    # åŒæœŸå®Ÿè¡Œ
    success, result = runner.run_workflow(
        args.workflow,
        args.mode,
        args.stocks,
        args.date,
        args.start_date,
        args.end_date,
        args.years,
        args.max_days,
        args.exclude_market_codes,
        args.use_existing_data
    )
    
    # çµæœè¡¨ç¤º
    print("\n" + "=" * 80)
    if success:
        print("ğŸ‰ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡ŒæˆåŠŸ!")
        print("=" * 80)
        
        # çµæœã‚µãƒãƒªãƒ¼
        if args.workflow == "safe-training":
            if isinstance(result, dict):
                summary = result.get("summary", {})
                if isinstance(summary, dict):
                    print(f"ğŸ“Š å­¦ç¿’çµæœ:")
                    print(f"   - ã‚¨ãƒãƒƒã‚¯æ•°: {summary.get('epochs', 'N/A')}")
                    print(f"   - æœ€çµ‚æå¤±: {summary.get('final_loss', 'N/A')}")
                    elapsed_time = summary.get('elapsed_time', 'N/A')
                    if isinstance(elapsed_time, (int, float)):
                        print(f"   - å®Ÿè¡Œæ™‚é–“: {elapsed_time:.2f}ç§’")
                    else:
                        print(f"   - å®Ÿè¡Œæ™‚é–“: {elapsed_time}")
                else:
                    print(f"ğŸ“Š å­¦ç¿’çµæœ: {summary}")
            else:
                print(f"ğŸ“Š å­¦ç¿’çµæœ: {result}")
        elif args.workflow in ["ml-dataset", "direct-api-dataset"]:
            if isinstance(result, dict) and "df" in result:
                df = result['df']
                print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰çµæœ:")
                print(f"   - è¡Œæ•°: {len(df):,}")
                try:
                    unique_count = "N/A"
                    df_type = type(df).__name__
                    if 'DataFrame' in df_type:
                        if hasattr(df, 'select'):
                            import polars as pl
                            try:
                                select_method = getattr(df, 'select', None)
                                if callable(select_method):
                                    result = select_method(pl.col('Code').n_unique())
                                    unique_count = str(result.item())
                            except Exception:
                                unique_count = "N/A"
                        elif hasattr(df, 'nunique'):
                            try:
                                getitem_method = getattr(df, '__getitem__', None)
                                if callable(getitem_method):
                                    code_col = getitem_method('Code')
                                    nunique_method = getattr(code_col, 'nunique', None)
                                    if callable(nunique_method):
                                        unique_count = str(nunique_method())
                            except Exception:
                                unique_count = "N/A"
                    print(f"   - éŠ˜æŸ„æ•°: {unique_count}")
                except Exception:
                    print(f"   - éŠ˜æŸ„æ•°: N/A")
                if "metadata" in result and isinstance(getattr(result, 'get', lambda x: None)("metadata"), dict):
                    metadata = getattr(result, 'get', lambda x, d=None: d)("metadata", {})
                    if "features" in metadata and isinstance(getattr(metadata, 'get', lambda x: None)("features"), dict):
                        features_dict = getattr(metadata, 'get', lambda x, d=None: d)("features", {})
                        get_method = getattr(features_dict, 'get', None)
                        count_value = get_method('count', 'N/A') if callable(get_method) else 'N/A'
                        print(f"   - ç‰¹å¾´é‡æ•°: {count_value}")
            else:
                print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰çµæœ: {result}")
        elif args.workflow == "complete-atft":
            if isinstance(result, dict):
                validation_info = result.get("validation_info", {})
                print(f"ğŸ¯ ATFTå­¦ç¿’çµæœ:")
                print(f"   - ç›®æ¨™Sharpe: 0.849")
                if isinstance(validation_info, dict):
                    sharpe_ratio = validation_info.get('sharpe_ratio')
                    if sharpe_ratio is not None:
                        print(f"   - é”æˆSharpe: {sharpe_ratio}")
                    param_count = validation_info.get('param_count', 'N/A')
                    if isinstance(param_count, (int, float)):
                        print(f"   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {param_count:,}")
                    else:
                        print(f"   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {param_count}")
            else:
                print(f"ğŸ¯ ATFTå­¦ç¿’çµæœ: {result}")
        elif args.workflow == "create-ml-dataset":
            print(f"ğŸ¤– MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆçµæœ:")
            if isinstance(result, dict):
                raw_records = result.get('raw_data_records', 'N/A')
                years = result.get('years', 'N/A')
                excluded_markets = result.get('excluded_markets', [])
                used_existing = result.get('used_existing_data', 'N/A')
                
                if isinstance(raw_records, (int, float)):
                    print(f"   - å…ƒãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {raw_records:,}")
                else:
                    print(f"   - å…ƒãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {raw_records}")
                    
                print(f"   - å¯¾è±¡å¹´æ•°: {years}å¹´")
                
                if isinstance(excluded_markets, list):
                    print(f"   - é™¤å¤–å¸‚å ´: {', '.join(excluded_markets)}")
                else:
                    print(f"   - é™¤å¤–å¸‚å ´: {excluded_markets}")
                    
                print(f"   - æ—¢å­˜ãƒ‡ãƒ¼ã‚¿æ´»ç”¨: {used_existing}")
                
                if "ml_dataset_path" in result:
                    print(f"   - ä¿å­˜å…ˆ: {result['ml_dataset_path']}")
            else:
                print(f"ğŸ¤– MLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆçµæœ: {result}")

        print("=" * 80)
        print("âœ… å®Ÿè¡Œå®Œäº†")
        
    else:
        print("âŒ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œå¤±æ•—")
        print(f"ã‚¨ãƒ©ãƒ¼: {result.get('error', 'Unknown error')}")
        print("=" * 80)
        
        sys.exit(1)


if __name__ == "__main__":
    main()
