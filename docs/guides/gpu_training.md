# GPUå­¦ç¿’ã®æ°¸ç¶šåŒ–è¨­å®š

## æ¦‚è¦
æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è‡ªå‹•çš„ã«GPUå­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸã€‚

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ã‚³ãƒãƒ³ãƒ‰ï¼ˆæ¨å¥¨ï¼‰
```bash
# æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è‡ªå‹•GPUå­¦ç¿’
make train-gpu-latest

# SafeTrainingPipelineæ¤œè¨¼ä»˜ã
make train-gpu-latest-safe
```

### ç›´æ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
```bash
# æ¨™æº–GPUå­¦ç¿’
./scripts/train_gpu_latest.sh

# æ¤œè¨¼ä»˜ã
./scripts/train_gpu_latest.sh --safe

# ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
./scripts/train_gpu_latest.sh "" --adv-graph-train 2e-4 75
```

### CLIã‚³ãƒãƒ³ãƒ‰ï¼ˆè©³ç´°åˆ¶å¾¡ï¼‰
```bash
# GPUç’°å¢ƒè¨­å®šï¼ˆè‡ªå‹•é©ç”¨æ¸ˆã¿ï¼‰
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export CUDA_VISIBLE_DEVICES=0

# çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
python scripts/integrated_ml_training_pipeline.py \
  --data-path output/datasets/ml_dataset_20250319_20250919_20250919_223415_full.parquet \
  --adv-graph-train \
  train.optimizer.lr=2e-4 \
  train.trainer.max_epochs=75
```

## âš™ï¸ æ°¸ç¶šåŒ–è¨­å®š

### 1. ç’°å¢ƒå¤‰æ•°ï¼ˆ.envï¼‰
```bash
# GPUè¨­å®šï¼ˆæ°¸ç¶šåŒ–æ¸ˆã¿ï¼‰
FORCE_GPU=1
REQUIRE_GPU=1
USE_GPU_ETL=1
RMM_POOL_SIZE=70GB
CUDA_VISIBLE_DEVICES=0
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# GPUå­¦ç¿’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
GPU_TRAINING_ENABLED=1
DEFAULT_LEARNING_RATE=2e-4
DEFAULT_MAX_EPOCHS=75
ADV_GRAPH_TRAIN=1
```

### 2. è‡ªå‹•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œå‡º
ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®é †ã§æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•æ¤œå‡ºï¼š
1. `output/datasets/ml_dataset_*_full.parquet`
2. `output/ml_dataset_*_full.parquet`

### 3. Makefileã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
```makefile
train-gpu-latest         # æ¨™æº–GPUå­¦ç¿’
train-gpu-latest-safe    # SafeTrainingPipelineæ¤œè¨¼ä»˜ã
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š

### GPUæœ€é©åŒ–
- **ãƒ¡ãƒ¢ãƒªæ‹¡å¼µ**: `expandable_segments:True`ã§OOMå›é¿
- **ãƒãƒƒãƒã‚µã‚¤ã‚º**: è‡ªå‹•èª¿æ•´ï¼ˆOOMæ™‚ã«ç¸®å°ï¼‰
- **æ··åˆç²¾åº¦**: bf16ä½¿ç”¨ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Š

### å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- **å­¦ç¿’ç‡**: 2e-4ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- **ã‚¨ãƒãƒƒã‚¯æ•°**: 75ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- **ã‚°ãƒ©ãƒ•å­¦ç¿’**: Advanced Graph Trainingæœ‰åŠ¹

## ğŸ” ç¢ºèªæ–¹æ³•

### GPUä½¿ç”¨çŠ¶æ³
```bash
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
watch -n 1 nvidia-smi

# å­¦ç¿’ãƒ­ã‚°ç¢ºèª
tail -f logs/ml_training.log
```

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¢ºèª
```bash
# æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¡¨ç¤º
ls -lht output/datasets/ml_dataset_*_full.parquet | head -1
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
```bash
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆï¼ˆGPU-ETLä½¿ç”¨ï¼‰
make dataset-full-gpu START=2020-09-19 END=2025-09-19
```

### CUDA Out of Memory
```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
python scripts/integrated_ml_training_pipeline.py \
  --data-path <dataset> \
  data.batch.batch_size=256
```

### GPU ãŒæ¤œå‡ºã•ã‚Œãªã„
```bash
# CUDAç¢ºèª
python -c "import torch; print(torch.cuda.is_available())"
```

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½
- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: 480,973è¡Œ Ã— 395åˆ—ï¼ˆ6ãƒ¶æœˆï¼‰
- **å­¦ç¿’æ™‚é–“**: A100ã§ç´„2-3æ™‚é–“ï¼ˆ75ã‚¨ãƒãƒƒã‚¯ï¼‰
- **ç›®æ¨™Sharpe**: 0.849
- **RankIC@1d**: 0.180ä»¥ä¸Š