#!/usr/bin/env python3
"""
Phase 2ç‰¹å¾´é‡è¿½åŠ ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

GATä¿®æ­£ã¨çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã™ã‚‹ã€ã‚»ã‚¯ã‚¿ãƒ¼ãƒ»å¸‚å ´æŒ‡æ•°ç‰¹å¾´é‡ã®è¿½åŠ ã€‚

Usage:
    python scripts/pipelines/add_phase2_features.py
    python scripts/pipelines/add_phase2_features.py --input output/ml_dataset_latest_full.parquet
"""
import argparse
import logging
from pathlib import Path

import polars as pl

# gogooku3ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰ã‚»ã‚¯ã‚¿ãƒ¼é›†ç´„ç‰¹å¾´ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from gogooku3.features.sector_aggregation import add_sector_aggregation_features

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def add_market_index_features(df: pl.DataFrame) -> pl.DataFrame:
    """å¸‚å ´æŒ‡æ•°ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆTOPIX betaç­‰ï¼‰"""
    logger.info("Adding market index features...")

    # TOPIXé–¢é€£ã®ã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‹ç¢ºèª
    topix_cols = [c for c in df.columns if "topix" in c.lower()]
    if not topix_cols:
        logger.warning("No TOPIX columns found. Skipping market index features.")
        return df

    # returns_1dã‚«ãƒ©ãƒ ã‚’æ¢ã™
    ret_col = None
    for candidate in ["returns_1d", "feat_ret_1d", "ret_1d"]:
        if candidate in df.columns:
            ret_col = candidate
            break

    if ret_col is None:
        logger.warning("No returns column found. Skipping market index features.")
        return df

    # TOPIX retã‚«ãƒ©ãƒ ã‚’æ¢ã™
    topix_ret_col = None
    for candidate in ["topix_ret_1d", "topix_return_1d", "TOPIX_ret_1d"]:
        if candidate in df.columns:
            topix_ret_col = candidate
            break

    if topix_ret_col is None:
        logger.warning("No TOPIX return column found. Skipping market index features.")
        return df

    logger.info(f"Using {ret_col} and {topix_ret_col} for market features")

    # 60æ—¥TOPIX beta
    df = df.with_columns(
        [
            (
                pl.col(ret_col).rolling_cov(pl.col(topix_ret_col), 60)
                / (pl.col(topix_ret_col).rolling_var(60) + 1e-8)
            ).alias("beta_topix_60"),
        ]
    )

    # 20æ—¥å¸‚å ´é€£å‹•åº¦ï¼ˆç›¸é–¢ï¼‰
    df = df.with_columns(
        [
            pl.col(ret_col)
            .rolling_corr(pl.col(topix_ret_col), 20)
            .alias("corr_topix_20"),
        ]
    )

    # å¸‚å ´è¶…éãƒªã‚¿ãƒ¼ãƒ³ï¼ˆalphaï¼‰
    df = df.with_columns(
        [
            (pl.col(ret_col) - pl.col(topix_ret_col)).alias("excess_ret_topix_1d"),
        ]
    )

    logger.info(
        "âœ… Market index features added: beta_topix_60, corr_topix_20, excess_ret_topix_1d"
    )
    return df


def enrich_with_phase2_features(df: pl.DataFrame) -> pl.DataFrame:
    """Phase 2ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆã‚»ã‚¯ã‚¿ãƒ¼+å¸‚å ´æŒ‡æ•°ï¼‰"""
    logger.info("=" * 60)
    logger.info("Phase 2ç‰¹å¾´é‡è¿½åŠ é–‹å§‹")
    logger.info(f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿: {len(df):,} rows, {len(df.columns)} columns")
    logger.info("=" * 60)

    # 1. ã‚»ã‚¯ã‚¿ãƒ¼é›†ç´„ç‰¹å¾´ï¼ˆ~30åˆ—ï¼‰
    try:
        df = add_sector_aggregation_features(df, min_members=5)
        logger.info("âœ… Sector aggregation features added")
    except Exception as e:
        logger.error(f"Failed to add sector features: {e}")

    # 2. å¸‚å ´æŒ‡æ•°ç‰¹å¾´ï¼ˆ~3åˆ—ï¼‰
    try:
        df = add_market_index_features(df)
        logger.info("âœ… Market index features added")
    except Exception as e:
        logger.error(f"Failed to add market index features: {e}")

    logger.info("=" * 60)
    logger.info(f"å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿: {len(df):,} rows, {len(df.columns)} columns")
    logger.info(
        f"è¿½åŠ ç‰¹å¾´é‡: {len(df.columns)} columns (å…¥åŠ›ã‹ã‚‰ +{len(df.columns) - len(df.columns)} åˆ—å¢—åŠ )"
    )
    logger.info("=" * 60)

    return df


def main():
    parser = argparse.ArgumentParser(description="Phase 2ç‰¹å¾´é‡è¿½åŠ ")
    parser.add_argument(
        "--input",
        type=str,
        default="output/ml_dataset_latest_full.parquet",
        help="å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="output/ml_dataset_phase2_enriched.parquet",
        help="å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹",
    )
    args = parser.parse_args()

    input_path = Path(args.input)
    output_path = Path(args.output)

    if not input_path.exists():
        logger.error(f"Input file not found: {input_path}")
        return 1

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    logger.info(f"ğŸ“‚ Loading dataset from: {input_path}")
    df = pl.read_parquet(input_path)

    # Phase 2ç‰¹å¾´é‡è¿½åŠ 
    df_enriched = enrich_with_phase2_features(df)

    # ä¿å­˜
    output_path.parent.mkdir(parents=True, exist_ok=True)
    logger.info(f"ğŸ’¾ Saving enriched dataset to: {output_path}")
    df_enriched.write_parquet(output_path)

    logger.info("âœ… Phase 2 features added successfully!")
    logger.info(f"   Output: {output_path}")
    logger.info(f"   Shape: {df_enriched.shape}")

    # è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡ã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    new_cols = [c for c in df_enriched.columns if c not in df.columns]
    if new_cols:
        logger.info(f"   New features ({len(new_cols)}): {', '.join(new_cols[:10])}...")

    return 0


if __name__ == "__main__":
    import sys

    sys.exit(main())
