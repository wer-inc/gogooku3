"""
Graph Builder for Financial Time Series Correlation
ç³»åˆ—ç›¸é–¢ã«åŸºã¥ãé‡‘èã‚°ãƒ©ãƒ•æ§‹ç¯‰å™¨

ä¸»ãªæ©Ÿèƒ½:
- NÃ—Tè¡Œåˆ—ã‹ã‚‰NÃ—Nç›¸é–¢è¡Œåˆ—ã®è¨ˆç®—
- è² ç›¸é–¢ã‚’å«ã‚€æœ‰æ„ãªã‚¨ãƒƒã‚¸ã®æŠ½å‡º
- é€±æ¬¡æ›´æ–°ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
- peer_mean/peer_varç‰¹å¾´é‡ã®ç”Ÿæˆ
"""

from __future__ import annotations

import hashlib
import logging
import pickle
from datetime import date, datetime, timedelta
import gzip
from pathlib import Path
from typing import Any

import networkx as nx
import numpy as np
import pandas as pd
import polars as pl
import torch

logger = logging.getLogger(__name__)


class FinancialGraphBuilder:
    """
    é‡‘èæ™‚ç³»åˆ—ã®ç›¸é–¢ã‚°ãƒ©ãƒ•æ§‹ç¯‰å™¨
    
    æ­£ã—ã„ç³»åˆ—ç›¸é–¢ã®å®Ÿè£…:
    - åŒä¸€æœŸé–“Tæ—¥ã®ãƒªã‚¿ãƒ¼ãƒ³ãƒ™ã‚¯ãƒˆãƒ«ã§éŠ˜æŸ„Ã—éŠ˜æŸ„ã®ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—
    - è² ç›¸é–¢ã‚‚å«ã‚€æœ‰æ„ãªã‚¨ãƒƒã‚¸ã‚’æŠ½å‡º
    - peerç‰¹å¾´é‡ã®ç”Ÿæˆ
    """

    def __init__(
        self,
        correlation_window: int = 60,
        min_observations: int = 40,
        correlation_threshold: float = 0.3,
        max_edges_per_node: int = 10,
        include_negative_correlation: bool = True,
        update_frequency: str = 'weekly',
        correlation_method: str = 'pearson',  # 'pearson' | 'spearman' | 'ewm_demean'
        ewm_halflife: int = 20,
        shrinkage_gamma: float = 0.05,
        symmetric: bool = True,
        cache_dir: str | None = None,
        verbose: bool = True,
        sector_col: str | None = None,
        market_col: str | None = None,
        keep_in_memory: bool = False,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§False
    ):
        """
        Args:
            correlation_window: ç›¸é–¢è¨ˆç®—ã®çª“ã‚µã‚¤ã‚ºï¼ˆæ—¥æ•°ï¼‰
            min_observations: æœ€å°è¦³æ¸¬æ•°
            correlation_threshold: ç›¸é–¢é–¾å€¤ï¼ˆçµ¶å¯¾å€¤ï¼‰
            max_edges_per_node: ãƒãƒ¼ãƒ‰ã‚ãŸã‚Šæœ€å¤§ã‚¨ãƒƒã‚¸æ•°
            include_negative_correlation: è² ç›¸é–¢ã‚’å«ã‚€ã‹
            update_frequency: æ›´æ–°é »åº¦ï¼ˆ'daily', 'weekly', 'monthly'ï¼‰
            correlation_method: ç›¸é–¢è¨ˆç®—æ–¹æ³•ï¼ˆ'pearson', 'spearman'ï¼‰
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            verbose: è©³ç´°ãƒ­ã‚°å‡ºåŠ›
            keep_in_memory: ç›¸é–¢è¡Œåˆ—ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Falseã€ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
        """
        self.correlation_window = correlation_window
        self.min_observations = min_observations
        self.correlation_threshold = correlation_threshold
        self.max_edges_per_node = max_edges_per_node
        self.include_negative_correlation = include_negative_correlation
        self.update_frequency = update_frequency
        self.correlation_method = correlation_method
        self.ewm_halflife = int(max(1, ewm_halflife))
        self.shrinkage_gamma = float(max(0.0, min(1.0, shrinkage_gamma)))
        self.symmetric = bool(symmetric)
        self.cache_dir = Path(cache_dir) if cache_dir else None
        self.verbose = verbose
        self.keep_in_memory = keep_in_memory  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ãƒ•ãƒ©ã‚°
        # Column preferences (best-effort; falls back to common aliases)
        self.sector_col = sector_col
        self.market_col = market_col

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        if self.cache_dir:
            self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿
        self.correlation_matrices: dict[str, np.ndarray] = {}
        self.edge_indices: dict[str, torch.Tensor] = {}
        self.edge_attributes: dict[str, torch.Tensor] = {}
        self.node_mappings: dict[str, dict[str, int]] = {}
        self.peer_features: dict[str, dict[str, float]] = {}

        if self.verbose:
            logger.info(
                f"FinancialGraphBuilder: window={correlation_window}, "
                f"threshold={correlation_threshold}, method={correlation_method}, "
                f"ewm_halflife={self.ewm_halflife}, shrinkage_gamma={self.shrinkage_gamma}, symmetric={self.symmetric}"
            )

    def _get_cache_key(self, date: date, codes: list[str]) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆæ±ºå®šçš„ãƒãƒƒã‚·ãƒ¥ã§è¡çªã‚’å›é¿ï¼‰"""
        # æ±ºå®šçš„ãªãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆï¼ˆãƒ—ãƒ­ã‚»ã‚¹é–“ã§å†ç¾å¯èƒ½ï¼‰
        codes_str = "|".join(sorted(codes))
        code_digest = hashlib.blake2s(
            codes_str.encode('utf-8'),
            digest_size=8
        ).hexdigest()

        # é–¾å€¤ã¯2æ¡ç²¾åº¦ã§è¡¨ç¾ï¼ˆä¾‹: 0.3 -> 030ï¼‰
        try:
            thr100 = int(round(float(self.correlation_threshold) * 100))
        except Exception:
            thr100 = 0

        # ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚­ãƒ¼ã«å«ã‚ã‚‹ï¼ˆGPUç‰ˆã¨ä¸€è²«æ€§ã‚’ä¿ã¤ï¼‰
        parts = [
            "graph",
            str(date),
            code_digest,  # æ±ºå®šçš„ãªãƒãƒƒã‚·ãƒ¥
            f"w{int(self.correlation_window)}",
            f"t{thr100:03d}",
            f"k{int(self.max_edges_per_node)}",
            f"m{str(self.correlation_method)}",
            f"freq-{getattr(self, 'update_frequency', 'daily')}",
            f"neg-{int(bool(getattr(self, 'include_negative_correlation', True)))}",
            f"sym-{int(bool(getattr(self, 'symmetric', True)))}",
        ]
        return "_".join(parts)

    def _load_from_cache(self, cache_key: str) -> dict[str, Any] | None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        if not self.cache_dir:
            return None

        cache_file = self.cache_dir / f"{cache_key}.pkl"
        if cache_file.exists():
            try:
                with gzip.open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                if self.verbose:
                    logger.info(f"âœ… Cache HIT: {cache_file.name}")
                return data
            except Exception as e:
                logger.warning(f"Failed to load cache {cache_file}: {e}")
        return None

    def _save_to_cache(self, cache_key: str, data: dict[str, Any]):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not self.cache_dir:
            return

        cache_file = self.cache_dir / f"{cache_key}.pkl"
        try:
            # Drop huge correlation matrix to reduce cache size
            data_to_save = dict(data)
            if 'correlation_matrix' in data_to_save:
                data_to_save.pop('correlation_matrix', None)

            # Compressed pickle (~3-10x smaller depending on payload)
            with gzip.open(cache_file, 'wb', compresslevel=3) as f:
                pickle.dump(data_to_save, f, protocol=pickle.HIGHEST_PROTOCOL)
            if self.verbose:
                logger.info(f"ğŸ’¾ Cache SAVE: {cache_file.name}")
        except Exception as e:
            logger.warning(f"Failed to save cache {cache_file}: {e}")

    def _prepare_return_matrix(
        self,
        data: pd.DataFrame | pl.DataFrame,
        codes: list[str],
        date_end: date,
        return_column: str = 'return_1d'
    ) -> tuple[np.ndarray, list[str]]:
        """
        ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—ã‚’æº–å‚™ï¼ˆNÃ—Tå½¢å¼ï¼‰
        
        Returns:
            (return_matrix, valid_codes): ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—ã¨æœ‰åŠ¹éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰
        """
        # Polarsã«å¤‰æ›
        if isinstance(data, pd.DataFrame):
            data = pl.from_pandas(data)

        # æ—¥ä»˜ç¯„å›²ã‚’è¨ˆç®—
        date_start = date_end - timedelta(days=self.correlation_window + 10)  # ãƒãƒƒãƒ•ã‚¡å«ã‚€

        # å‹æ­£è¦åŒ–ï¼ˆDate vs Datetimeã®ä¸ä¸€è‡´ã‚’é¿ã‘ã‚‹ï¼‰
        try:
            if data.schema.get('date', None) is not None and data['date'].dtype != pl.Date:
                data = data.with_columns(pl.col('date').cast(pl.Date, strict=False))
        except Exception:
            pass

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿
        filtered_data = data.filter(
            (pl.col('date') >= date_start) &
            (pl.col('date') <= date_end) &
            (pl.col('code').is_in(codes)) &
            (pl.col(return_column).is_not_null()) &
            (pl.col(return_column).is_finite())
        ).sort(['code', 'date'])

        if filtered_data.is_empty():
            logger.debug(f"No data available for graph building on {date_end}")
            return np.array([]), []

        # Early check: sufficient historical data available?
        unique_dates = filtered_data.select('date').unique().height
        if unique_dates < self.min_observations:
            if self.verbose:
                logger.debug(
                    f"Skipping {date_end}: only {unique_dates} days available "
                    f"(need {self.min_observations})"
                )
            return np.array([]), []

        # éŠ˜æŸ„Ã—æ—¥ä»˜ã®ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—ã‚’æ§‹ç¯‰
        return_matrix = []
        valid_codes = []

        for code in codes:
            code_data = filtered_data.filter(pl.col('code') == code)

            if len(code_data) < self.min_observations:
                continue

            # æœ€æ–°ã®correlation_windowæ—¥åˆ†ã‚’å–å¾—
            recent_returns = code_data.tail(self.correlation_window)[return_column].to_numpy()

            # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
            if len(recent_returns) < self.min_observations or not np.all(np.isfinite(recent_returns)):
                continue

            return_matrix.append(recent_returns)
            valid_codes.append(code)

        if not return_matrix:
            logger.debug(f"No valid codes for graph building on {date_end}")
            return np.array([]), []

        # è¡Œåˆ—ã‚’çµ±ä¸€é•·ã«èª¿æ•´ï¼ˆæœ€çŸ­é•·ã«åˆã‚ã›ã‚‹ï¼‰
        min_length = min(len(arr) for arr in return_matrix)
        return_matrix = np.array([arr[-min_length:] for arr in return_matrix])

        if self.verbose:
            logger.debug(
                f"Prepared return matrix: {len(valid_codes)} codes Ã— {min_length} days"
            )

        return return_matrix, valid_codes

    def _compute_correlation_matrix(
        self,
        return_matrix: np.ndarray
    ) -> np.ndarray:
        """ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—ï¼ˆGPUæœ€é©åŒ–ã‚ã‚Šï¼‰"""
        n_stocks = return_matrix.shape[0]

        if n_stocks < 2:
            return np.eye(1) if n_stocks == 1 else np.array([])

        try:
            if self.correlation_method == 'pearson':
                # GPUæœ€é©åŒ–: CuPyãŒåˆ©ç”¨å¯èƒ½ã§ã‚ã‚Œã°GPUã§ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—
                use_gpu = False
                try:
                    import cupy as cp  # type: ignore
                    if cp.cuda.runtime.getDeviceCount() > 0:
                        use_gpu = True
                except Exception:
                    use_gpu = False

                if use_gpu:
                    Xg = None
                    try:
                        Xg = cp.asarray(return_matrix, dtype=cp.float64)
                        corr_g = cp.corrcoef(Xg)
                        corr_matrix = cp.asnumpy(corr_g)
                    finally:
                        # æ˜ç¤ºçš„ã«GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
                        try:
                            del Xg, corr_g  # type: ignore[name-defined]
                        except Exception:
                            pass
                else:
                    # CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    corr_matrix = np.corrcoef(return_matrix)

            elif self.correlation_method == 'spearman':
                # Spearmanç›¸é–¢ï¼ˆãƒ©ãƒ³ã‚¯ç›¸é–¢ï¼‰
                from scipy.stats import spearmanr
                corr_matrix, _ = spearmanr(return_matrix, axis=1)
                if np.isscalar(corr_matrix):
                    corr_matrix = np.array([[corr_matrix]])

            elif self.correlation_method == 'ewm_demean':
                # Exponentially-weighted correlation with per-asset de-meaningï¼ˆCPUï¼‰
                X = return_matrix.astype(np.float64, copy=False)  # [N,T]
                N, T = X.shape
                lam = np.log(2.0) / float(self.ewm_halflife)
                t_idx = np.arange(T)
                w = np.exp(-lam * (T - 1 - t_idx))
                w_sum = w.sum() + 1e-12
                w = w / w_sum
                mu = X @ w  # [N]
                Xc = X - mu[:, None]
                WX = Xc * w  # broadcast weights
                cov = WX @ Xc.T  # weighted covariance (already divided by sum via normalized w)
                var = np.diag(cov).copy()
                var[var <= 1e-12] = 1e-12
                std = np.sqrt(var)
                denom = np.outer(std, std)
                corr_matrix = cov / (denom + 1e-12)
                corr_matrix = np.clip(corr_matrix, -1.0, 1.0)
                # Symmetrize + diag=1
                corr_matrix = 0.5 * (corr_matrix + corr_matrix.T)
                np.fill_diagonal(corr_matrix, 1.0)
                # Shrinkage toward zero on off-diagonals
                if self.shrinkage_gamma > 0:
                    off = ~np.eye(N, dtype=bool)
                    corr_matrix[off] = (1.0 - self.shrinkage_gamma) * corr_matrix[off]
            else:
                raise ValueError(f"Unsupported correlation method: {self.correlation_method}")

            # NaN/infå‡¦ç†
            corr_matrix = np.nan_to_num(corr_matrix, nan=0.0, posinf=1.0, neginf=-1.0)

            # å¯¾è§’æˆåˆ†ã‚’1.0ã«è¨­å®š
            np.fill_diagonal(corr_matrix, 1.0)

            return corr_matrix

        except Exception as e:
            logger.warning(f"Correlation computation failed: {e}")
            return np.eye(n_stocks)

    def _extract_edges(
        self,
        corr_matrix: np.ndarray,
        codes: list[str],
        market_map: dict[str, Any] | None = None,
        sector_map: dict[str, Any] | None = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        ç›¸é–¢è¡Œåˆ—ã‹ã‚‰ã‚¨ãƒƒã‚¸ã‚’æŠ½å‡º
        
        Returns:
            (edge_index, edge_attr): ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚¨ãƒƒã‚¸å±æ€§
        """
        n_stocks = len(codes)
        if n_stocks < 2:
            return torch.empty((2, 0), dtype=torch.long), torch.empty(0, dtype=torch.float)

        edges = []
        edge_attrs: list[list[float]] = []

        # å„ãƒãƒ¼ãƒ‰ã‹ã‚‰ä¸Šä½kå€‹ã®ã‚¨ãƒƒã‚¸ã‚’æŠ½å‡º
        for i in range(n_stocks):
            # è‡ªåˆ†ä»¥å¤–ã®ç›¸é–¢ã‚’å–å¾—
            correlations = corr_matrix[i, :]

            # é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿
            if self.include_negative_correlation:
                valid_indices = np.where(np.abs(correlations) >= self.correlation_threshold)[0]
                valid_correlations = correlations[valid_indices]
            else:
                valid_indices = np.where(correlations >= self.correlation_threshold)[0]
                valid_correlations = correlations[valid_indices]

            # è‡ªãƒ«ãƒ¼ãƒ—ã‚’é™¤å¤–
            mask = valid_indices != i
            valid_indices = valid_indices[mask]
            valid_correlations = valid_correlations[mask]

            if len(valid_indices) == 0:
                continue

            # ç›¸é–¢ã®çµ¶å¯¾å€¤ã§ä¸¦ã³æ›¿ãˆï¼ˆå¼·ã„ç›¸é–¢ã‚’å„ªå…ˆï¼‰
            abs_correlations = np.abs(valid_correlations)
            sorted_indices = np.argsort(abs_correlations)[::-1]

            # ä¸Šä½kå€‹ã‚’é¸æŠ
            top_k = min(self.max_edges_per_node, len(sorted_indices))
            selected_indices = sorted_indices[:top_k]

            for idx in selected_indices:
                j = valid_indices[idx]
                correlation = float(valid_correlations[idx])
                corr_norm = (correlation + 1.0) / 2.0
                # 2) market similarity (1 if same, else 0)
                m_sim = 0.0
                if market_map is not None:
                    mi = market_map.get(codes[i])
                    mj = market_map.get(codes[j])
                    if mi is not None and mj is not None and mi == mj:
                        m_sim = 1.0
                # 3) sector similarity (1 if same, else 0)
                s_sim = 0.0
                if sector_map is not None:
                    si = sector_map.get(codes[i])
                    sj = sector_map.get(codes[j])
                    if si is not None and sj is not None and si == sj:
                        s_sim = 1.0

                edges.append([i, j])
                edge_attrs.append([corr_norm, m_sim, s_sim])
                if self.symmetric:
                    edges.append([j, i])
                    edge_attrs.append([corr_norm, m_sim, s_sim])

        if not edges:
            return (
                torch.empty((2, 0), dtype=torch.long),
                torch.empty((0, 3), dtype=torch.float),
            )

        # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
        edge_attr = torch.tensor(edge_attrs, dtype=torch.float)

        if self.verbose:
            logger.debug(f"Extracted {len(edges)} edges from {n_stocks} nodes")

        return edge_index, edge_attr

    def _compute_peer_features(
        self,
        return_matrix: np.ndarray,
        corr_matrix: np.ndarray,
        codes: list[str]
    ) -> dict[str, dict[str, float]]:
        """peerç‰¹å¾´é‡ã‚’è¨ˆç®—"""
        peer_features = {}

        for i, code in enumerate(codes):
            # ç¾åœ¨ã®éŠ˜æŸ„ã®ç›¸é–¢ã‚’ãƒ™ãƒ¼ã‚¹ã«è¿‘å‚ã‚’æ±ºå®š
            correlations = corr_matrix[i, :]

            # é–¾å€¤ã‚’æº€ãŸã™peerã‚’ç‰¹å®š
            if self.include_negative_correlation:
                peer_mask = (np.abs(correlations) >= self.correlation_threshold) & (np.arange(len(correlations)) != i)
            else:
                peer_mask = (correlations >= self.correlation_threshold) & (np.arange(len(correlations)) != i)

            peer_indices = np.where(peer_mask)[0]

            if len(peer_indices) == 0:
                # peer ãŒã„ãªã„å ´åˆ
                peer_features[code] = {
                    'peer_mean_return': 0.0,
                    'peer_var_return': 1.0,
                    'peer_count': 0,
                    'peer_correlation_mean': 0.0
                }
                continue

            # peer ã®ãƒªã‚¿ãƒ¼ãƒ³çµ±è¨ˆ
            peer_returns = return_matrix[peer_indices, :]  # shape: (n_peers, T)
            peer_mean_return = np.mean(peer_returns.flatten())
            peer_var_return = np.var(peer_returns.flatten())

            # peer ã¨ã®å¹³å‡ç›¸é–¢
            peer_correlations = correlations[peer_indices]
            peer_correlation_mean = np.mean(np.abs(peer_correlations))

            peer_features[code] = {
                'peer_mean_return': float(peer_mean_return),
                'peer_var_return': float(peer_var_return),
                'peer_count': len(peer_indices),
                'peer_correlation_mean': float(peer_correlation_mean)
            }

        return peer_features

    def build_graph(
        self,
        data: pd.DataFrame | pl.DataFrame,
        codes: list[str],
        date_end: str | date | datetime,
        return_column: str = 'return_1d'
    ) -> dict[str, Any]:
        """
        ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
        
        Args:
            data: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
            codes: éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
            date_end: åŸºæº–æ—¥
            return_column: ãƒªã‚¿ãƒ¼ãƒ³åˆ—å
            
        Returns:
            ã‚°ãƒ©ãƒ•æƒ…å ±è¾æ›¸
        """
        # æ—¥ä»˜å¤‰æ›
        if isinstance(date_end, str):
            date_end = datetime.strptime(date_end, '%Y-%m-%d').date()
        elif isinstance(date_end, datetime):
            date_end = date_end.date()

        date_key = str(date_end)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = self._get_cache_key(date_end, codes)
        cached_result = self._load_from_cache(cache_key)
        if cached_result:
            if self.verbose:
                logger.debug(f"Loaded graph from cache for {date_end}")
            return cached_result

        # ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—ã‚’æº–å‚™
        return_matrix, valid_codes = self._prepare_return_matrix(
            data, codes, date_end, return_column
        )

        if len(valid_codes) < 2:
            logger.debug(f"Insufficient codes ({len(valid_codes)}) for graph building")
            empty_result = {
                'edge_index': torch.empty((2, 0), dtype=torch.long),
                'edge_attr': torch.empty(0, dtype=torch.float),
                'node_mapping': {},
                'correlation_matrix': np.array([]),
                'peer_features': {},
                'date': date_end,
                'n_nodes': 0,
                'n_edges': 0
            }
            return empty_result

        # ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—
        corr_matrix = self._compute_correlation_matrix(return_matrix)

        # å¸‚å ´/ã‚»ã‚¯ã‚¿ãƒãƒƒãƒ—ã‚’æº–å‚™ï¼ˆå¯èƒ½ãªã‚‰ï¼‰
        market_col = self.market_col
        sector_col = self.sector_col
        try:
            # Determine available columns (lower-case for robustness)
            candidate_market_cols = []
            if isinstance(market_col, str) and market_col:
                candidate_market_cols.append(market_col.lower())
            candidate_market_cols += ["marketcode", "market_code", "market", "section", "meta_section"]
            candidate_sector_cols = []
            if isinstance(sector_col, str) and sector_col:
                candidate_sector_cols.append(sector_col.lower())
            candidate_sector_cols += ["sector33", "sectorcode", "sector", "meta_section", "section"]
            # Convert to pandas only if needed (ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–)
            if isinstance(data, pd.DataFrame):
                _df = data
            else:
                # Polarsã®å ´åˆ: valid_codesã®ã¿ã‚’é¸æŠã—ã¦ã‹ã‚‰Pandasã«å¤‰æ›
                _df = data.filter(pl.col('code').is_in(valid_codes)).to_pandas()
            try:
                _df.columns = [str(c).lower() for c in _df.columns]
            except Exception:
                pass
            # Filter to valid codes only; prefer latest date rows within window buffer
            _subset = _df[_df["code"].isin(valid_codes)].copy()
            # Try to pick latest row per code
            if "date" in _subset.columns:
                _subset = _subset.sort_values(["code", "date"]).drop_duplicates("code", keep="last")
            market_map = None
            sector_map = None
            for col in candidate_market_cols:
                if col in _subset.columns:
                    market_map = {str(r.code): r[col] for r in _subset[["code", col]].itertuples(index=False)}
                    break
            for col in candidate_sector_cols:
                if col in _subset.columns:
                    sector_map = {str(r.code): r[col] for r in _subset[["code", col]].itertuples(index=False)}
                    break
        except Exception as _e:
            market_map = None
            sector_map = None
            if self.verbose:
                logger.warning(f"Failed to prepare market/sector maps: {_e}")

        # ã‚¨ãƒƒã‚¸ã‚’æŠ½å‡ºï¼ˆç›¸é–¢ + å¸‚å ´/ã‚»ã‚¯ã‚¿é¡ä¼¼ï¼‰
        edge_index, edge_attr = self._extract_edges(
            corr_matrix, valid_codes, market_map=market_map, sector_map=sector_map
        )

        # ãƒãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        node_mapping = {code: i for i, code in enumerate(valid_codes)}

        # peerç‰¹å¾´é‡ã‚’è¨ˆç®—
        peer_features = self._compute_peer_features(return_matrix, corr_matrix, valid_codes)

        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        result = {
            'edge_index': edge_index,
            'edge_attr': edge_attr,
            'node_mapping': node_mapping,
            'correlation_matrix': corr_matrix,
            'peer_features': peer_features,
            'date': date_end,
            'n_nodes': len(valid_codes),
            'n_edges': edge_index.size(1)
        }

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self._save_to_cache(cache_key, result)

        # å†…éƒ¨çŠ¶æ…‹ã‚’æ›´æ–°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        if self.keep_in_memory:
            self.correlation_matrices[date_key] = corr_matrix
            self.edge_indices[date_key] = edge_index
            self.edge_attributes[date_key] = edge_attr
            self.node_mappings[date_key] = node_mapping
            self.peer_features[date_key] = peer_features

        if self.verbose:
            logger.info(
                f"Built graph for {date_end}: {len(valid_codes)} nodes, "
                f"{edge_index.size(1)} edges"
            )

        return result

    def get_peer_features_for_codes(
        self,
        codes: list[str],
        date: str | date | datetime
    ) -> dict[str, dict[str, float]]:
        """æŒ‡å®šéŠ˜æŸ„ã®peerç‰¹å¾´é‡ã‚’å–å¾—"""
        date_key = str(date)

        if date_key not in self.peer_features:
            logger.warning(f"No peer features available for {date}")
            return {code: {'peer_mean_return': 0.0, 'peer_var_return': 1.0,
                          'peer_count': 0, 'peer_correlation_mean': 0.0} for code in codes}

        peer_data = self.peer_features[date_key]
        return {code: peer_data.get(code, {'peer_mean_return': 0.0, 'peer_var_return': 1.0,
                                          'peer_count': 0, 'peer_correlation_mean': 0.0})
                for code in codes}

    def analyze_graph_statistics(
        self,
        date: str | date | datetime
    ) -> dict[str, Any]:
        """ã‚°ãƒ©ãƒ•ã®çµ±è¨ˆæƒ…å ±ã‚’åˆ†æ"""
        date_key = str(date)

        if date_key not in self.edge_indices:
            return {}

        edge_index = self.edge_indices[date_key]
        edge_attr = self.edge_attributes[date_key]
        corr_matrix = self.correlation_matrices[date_key]

        # åŸºæœ¬çµ±è¨ˆ
        n_nodes = corr_matrix.shape[0] if corr_matrix.size > 0 else 0
        n_edges = edge_index.size(1)

        # ç›¸é–¢çµ±è¨ˆ
        if corr_matrix.size > 0:
            # ä¸Šä¸‰è§’ã®ã¿ï¼ˆå¯¾è§’é™¤å¤–ï¼‰
            triu_indices = np.triu_indices(n_nodes, k=1)
            correlations = corr_matrix[triu_indices]

            correlation_stats = {
                'mean_correlation': float(np.mean(correlations)),
                'std_correlation': float(np.std(correlations)),
                'min_correlation': float(np.min(correlations)),
                'max_correlation': float(np.max(correlations)),
                'median_correlation': float(np.median(correlations))
            }
        else:
            correlation_stats = {}

        # ã‚¨ãƒƒã‚¸çµ±è¨ˆ
        if n_edges > 0:
            edge_stats = {
                'mean_edge_weight': float(torch.mean(edge_attr)),
                'std_edge_weight': float(torch.std(edge_attr)),
                'min_edge_weight': float(torch.min(edge_attr)),
                'max_edge_weight': float(torch.max(edge_attr))
            }

            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆï¼ˆNetworkXä½¿ç”¨ï¼‰
            try:
                G = nx.Graph()
                G.add_nodes_from(range(n_nodes))
                edges_list = edge_index.t().numpy().tolist()
                weights = edge_attr.numpy().tolist()
                weighted_edges = [(e[0], e[1], {'weight': abs(w)}) for e, w in zip(edges_list, weights, strict=False)]
                G.add_edges_from(weighted_edges)

                network_stats = {
                    'density': nx.density(G),
                    'average_clustering': nx.average_clustering(G),
                    'n_connected_components': nx.number_connected_components(G)
                }
            except Exception as e:
                logger.warning(f"Network analysis failed: {e}")
                network_stats = {}
        else:
            edge_stats = {}
            network_stats = {}

        return {
            'date': date,
            'n_nodes': n_nodes,
            'n_edges': n_edges,
            'correlation_stats': correlation_stats,
            'edge_stats': edge_stats,
            'network_stats': network_stats
        }
