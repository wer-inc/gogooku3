#!/usr/bin/env python3
"""
Create ultra-clean dataset with correlation-based leakage detection
ç›®çš„: ç›¸é–¢ãƒ™ãƒ¼ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’å®Œå…¨ã«é™¤å»
"""

from pathlib import Path

import numpy as np
import pandas as pd


def create_ultra_clean_dataset():
    """ç›¸é–¢ãƒ™ãƒ¼ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’å®Œå…¨ã«é™¤å»ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""

    print("=" * 60)
    print("ğŸ§¹ CREATING ULTRA-CLEAN DATASET")
    print("=" * 60)

    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    input_path = Path("/home/ubuntu/gogooku3-standalone/output/ml_dataset_latest_full.parquet")
    output_path = Path("/home/ubuntu/gogooku3-standalone/output/ml_dataset_ultra_clean.parquet")

    print(f"\nğŸ“‚ Loading data from: {input_path}")
    df = pd.read_parquet(input_path)
    print(f"âœ… Original data shape: {df.shape}")

    # 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ—ã®ç‰¹å®š
    target_cols = ['returns_1d', 'returns_5d', 'returns_10d', 'returns_20d', 'returns_60d', 'returns_120d']
    meta_cols = ['Code', 'Date']

    # 3. åˆæœŸã®ç‰¹å¾´é‡åˆ—
    feature_cols = [col for col in df.columns if col not in meta_cols + target_cols]
    print(f"\nğŸ“Š Initial features: {len(feature_cols)}")

    # 4. æ˜ç¤ºçš„ã«é™¤å¤–ã™ã¹ãåˆ—ï¼ˆåå‰ãƒ™ãƒ¼ã‚¹ï¼‰
    print("\nğŸ” Phase 1: Name-based exclusion...")

    name_based_exclusions = []
    for col in feature_cols:
        col_lower = col.lower()
        # ãƒªã‚¿ãƒ¼ãƒ³é–¢é€£
        if any(x in col_lower for x in ['return', 'ret_', '_ret', 'log_ret', 'feat_ret']):
            name_based_exclusions.append(col)
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¢é€£
        elif any(x in col_lower for x in ['target', 'label', 'y_', '_y']):
            name_based_exclusions.append(col)
        # ã‚¢ãƒ«ãƒ•ã‚¡ï¼ˆè¶…éãƒªã‚¿ãƒ¼ãƒ³ï¼‰
        elif 'alpha' in col_lower and any(x in col_lower for x in ['1d', '5d', '10d', '20d']):
            name_based_exclusions.append(col)
        # ç›¸å¯¾å¼·åº¦ï¼ˆã“ã‚Œã‚‚ãƒªã‚¿ãƒ¼ãƒ³ã®å¤‰å½¢ï¼‰
        elif 'rel_strength' in col_lower and any(x in col_lower for x in ['1d', '5d', '10d', '20d']):
            name_based_exclusions.append(col)

    print(f"   Excluded by name: {len(name_based_exclusions)} features")

    # 5. ç›¸é–¢ãƒ™ãƒ¼ã‚¹ã®é™¤å¤–ï¼ˆå³æ ¼ãªåŸºæº–ï¼‰
    print("\nğŸ” Phase 2: Correlation-based exclusion...")

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ç›¸é–¢ã‚’è¨ˆç®—
    sample_size = min(100000, len(df))
    sample_df = df.sample(sample_size, random_state=42)

    correlation_exclusions = []
    CORR_THRESHOLD = 0.8  # 80%ä»¥ä¸Šã®ç›¸é–¢ã¯é™¤å¤–

    for target_col in target_cols:
        if target_col not in sample_df.columns:
            continue

        target_vals = sample_df[target_col].fillna(0).values

        for col in feature_cols:
            if col in name_based_exclusions:  # æ—¢ã«é™¤å¤–æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue

            if sample_df[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                try:
                    col_vals = sample_df[col].fillna(0).values
                    corr = np.corrcoef(target_vals, col_vals)[0, 1]

                    if abs(corr) > CORR_THRESHOLD:
                        if col not in correlation_exclusions:
                            correlation_exclusions.append(col)
                            print(f"   ğŸ”´ High correlation: {col} with {target_col} = {corr:.4f}")
                except:
                    pass

    print(f"   Excluded by correlation: {len(correlation_exclusions)} additional features")

    # 6. å…¨é™¤å¤–ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    all_exclusions = list(set(name_based_exclusions + correlation_exclusions))
    print(f"\nğŸ—‘ï¸ Total exclusions: {len(all_exclusions)} features")

    # 7. ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    print("\nğŸ§¹ Creating ultra-clean dataset...")

    keep_cols = meta_cols + target_cols
    for col in feature_cols:
        if col not in all_exclusions:
            keep_cols.append(col)

    clean_df = df[keep_cols].copy()

    print(f"\nâœ… Ultra-clean data shape: {clean_df.shape}")
    print(f"   Original features: {len(feature_cols)}")
    print(f"   Clean features: {len(keep_cols) - len(meta_cols) - len(target_cols)}")
    print(f"   Removed features: {len(all_exclusions)}")

    # 8. æœ€çµ‚æ¤œè¨¼ï¼ˆç›¸é–¢ãƒã‚§ãƒƒã‚¯ï¼‰
    print("\nğŸ” Final validation...")

    # ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã§å†åº¦ç›¸é–¢ã‚’ãƒã‚§ãƒƒã‚¯
    sample_clean = clean_df.sample(min(50000, len(clean_df)), random_state=42)
    clean_feature_cols = [col for col in clean_df.columns if col not in meta_cols + target_cols]

    max_corr_found = 0
    max_corr_feature = None
    max_corr_target = None

    for target_col in target_cols[:2]:  # æœ€åˆã®2ã¤ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§æ¤œè¨¼
        if target_col not in sample_clean.columns:
            continue

        target_vals = sample_clean[target_col].fillna(0).values

        for col in clean_feature_cols[:50]:  # æœ€åˆã®50ç‰¹å¾´é‡ã§æ¤œè¨¼
            if sample_clean[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                try:
                    col_vals = sample_clean[col].fillna(0).values
                    corr = np.corrcoef(target_vals, col_vals)[0, 1]

                    if abs(corr) > abs(max_corr_found):
                        max_corr_found = corr
                        max_corr_feature = col
                        max_corr_target = target_col
                except:
                    pass

    print("\nğŸ“Š Maximum correlation found:")
    print(f"   {max_corr_feature} with {max_corr_target}: {max_corr_found:.4f}")

    if abs(max_corr_found) > 0.8:
        print("   âš ï¸ WARNING: Still some high correlation remaining!")
    elif abs(max_corr_found) > 0.5:
        print("   ğŸŸ¡ Moderate correlation - acceptable for most ML models")
    else:
        print("   âœ… Low correlation - excellent for training!")

    # 9. ä¿å­˜
    print(f"\nğŸ’¾ Saving ultra-clean dataset to: {output_path}")
    clean_df.to_parquet(output_path, index=False)
    print("âœ… Ultra-clean dataset saved successfully!")

    # 10. ä½¿ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    print("\nğŸ“‹ Sample of remaining features:")
    remaining_features = clean_feature_cols[:20]
    for i, feat in enumerate(remaining_features, 1):
        print(f"   {i:2}. {feat}")

    print("\n" + "=" * 60)
    print("ğŸš€ Next Steps")
    print("=" * 60)
    print("\n1. Test with baseline model:")
    print("   python scripts/test_baseline_rankic.py --data output/ml_dataset_ultra_clean.parquet")
    print("\n2. Convert to ATFT format:")
    print("   python scripts/convert_to_atft_data.py --input output/ml_dataset_ultra_clean.parquet")
    print("\n3. Train ATFT with ultra-clean data:")
    print("   make train-ultra-clean")

    return output_path

if __name__ == "__main__":
    output_path = create_ultra_clean_dataset()
    print(f"\nâœ¨ Ultra-clean dataset ready at: {output_path}")
