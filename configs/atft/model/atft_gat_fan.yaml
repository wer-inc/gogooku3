# @package model
name: ATFT_GAT_FAN
architecture: atft_gat_fan

# モデル全体設定（グループ直下にフラットに定義）
hidden_size: 256  # Increased from 64 for better model capacity (~20M params)
ema_teacher:
  enabled: true
  decay: 0.999
input_projection:
  use_layer_norm: true
  dropout: 0.1

input_dims:
  basic_features: 182  # Align with auto-detected dynamic features in loader
  historical_features: 0
  total_features: 182

tft:
  lstm:
    layers: 1
    hidden_size: ${model.hidden_size}
    dropout: 0.1
    bidirectional: false
  attention:
    heads: 8
    dropout: 0.1
    use_scale: true
  variable_selection:
    hidden_size: ${model.hidden_size}
    dropout: 0.1
    use_sigmoid: true
    sparsity_coefficient: 0.01
  grn:
    hidden_size: ${model.hidden_size}
    dropout: 0.1
    use_layer_norm: true
  temporal:
    use_positional_encoding: true
    max_sequence_length: 20

adaptive_normalization:
  fan:
    enabled: true
    window_sizes: [3, 7, 14, 30]
    aggregation: weighted_mean
    learn_weights: true
  san:
    enabled: true
    num_slices: 4
    overlap: 0.5
    slice_aggregation: learned

gat:
  enabled: true
  alpha_min: 0.4
  architecture:
    num_layers: 2
    hidden_channels:
      - ${model.hidden_size}
      - ${model.hidden_size}
    heads: [8, 4]
    concat: [true, false]
  layer_config:
    dropout: 0.2
    edge_dropout: 0.1
    negative_slope: 0.2
    add_self_loops: false
    bias: true
  edge_features:
    use_edge_attr: true
    edge_dim: 3
    edge_projection: linear
  regularization:
    edge_weight_penalty: 0.01
    attention_entropy_penalty: 0.001

prediction_head:
  architecture:
    hidden_layers: [32]
    activation: relu
    dropout: 0.2
    use_batch_norm: false
  stochastic_depth:
    rate: 0.1
  output:
    point_prediction: true
    quantile_loss:
      enforce_convexity: true
      smoothing_temperature: 0.5
    quantile_prediction:
      enabled: true
      quantiles: [0.05, 0.25, 0.5, 0.75, 0.95]
    distribution_prediction:
      enabled: false
      type: normal
    student_t: true  # Enable Student-t distribution heads for heteroscedastic predictions

optimization:
  compression:
    gradient_checkpointing: false
    mixed_precision: true
    channels_last: true
  compile:
    enabled: true  # Enabled for torch.compile optimization
    mode: reduce-overhead  # FIX: max-autotune causes CUDA misaligned address errors on A100
    fullgraph: false  # Keep false for safety with dynamic graphs
    dynamic: false  # Static shapes for better optimization
  inference:
    use_torch_script: false
    use_onnx: false
    optimize_for_inference: true

initialization:
  method: xavier_uniform
  gain: 1.0

regularization:
  weight_decay: 1e-4
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
